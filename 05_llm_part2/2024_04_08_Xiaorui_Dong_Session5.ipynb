{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J1tOS7oWba4s"
   },
   "source": [
    "# Large language models (LLMs): Part II\n",
    "\n",
    "Author: Archit Vasan , including materials on LLMs by Varuni Sastri, and discussion/editorial work by Taylor Childers, Carlo Graziani, Bethany Lusch, and Venkat Vishwanath (Argonne)\n",
    "\n",
    "Inspiration from the blog posts \"The Illustrated Transformer\" and \"The Illustrated GPT2\" by Jay Alammar, highly recommended reading.\n",
    "\n",
    "Before you begin, make sure that you have your environment set up and your repo refreshed, as described in previous lessons, and reviewed in the accompanying 'Readme.md' file. Make sure that you select the kernel 'datascience/conda-2023-01-10' at the top-left of the Jupyter notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "1. Training and inference using Hugging Face\n",
    "2. Elements of an LLM\n",
    "3. Attention mechanisms\n",
    "4. Positional encoding\n",
    "5. Output layers\n",
    "6. Training loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HTTP_PROXY\"]=\"http://proxy-01.pub.alcf.anl.gov:3128\"\n",
    "os.environ[\"HTTPS_PROXY\"]=\"http://proxy-01.pub.alcf.anl.gov:3128\"\n",
    "os.environ[\"http_proxy\"]=\"http://proxy-01.pub.alcf.anl.gov:3128\"\n",
    "os.environ[\"https_proxy\"]=\"http://proxy-01.pub.alcf.anl.gov:3128\"\n",
    "os.environ[\"ftp_proxy\"]=\"http://proxy-01.pub.alcf.anl.gov:3128\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM training and inference using HuggingFace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/hf-logo-with-title.png\" alt=\"Drawing\" style=\"width: 300px;\"/>\n",
    "HuggingFace is a platform and community that provides open-source library tools and resources like pre-trained models and datasets.\n",
    "Refer to the following links for more information :\n",
    "\n",
    "https://huggingface.co/docs/hub/index\n",
    "\n",
    "https://huggingface.co/docs/transformers/en/index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Warning: _Large Language Models are only as good as their training data. They have no ethics, no judgement, or editing ability. We will be using some pretrained models from Hugging Face which used wide samples of internet hosted text. The datasets have not been strictly filtered to restrict all malign content so the generated text may be surprisingly dark or questionable. They do not reflect our core values and are only used for demonstration purposes._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference\n",
    "\n",
    "We can use the Huggingface pipeline with a pretrained GPT2 model to generate text given a prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-09 02:38:27.717789: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/soft/datascience/conda/2023-01-10/mconda3/lib/python3.10/site-packages/h5py/__init__.py:36: UserWarning: h5py is running against HDF5 1.12.2 when it was built against 1.12.1, this may cause problems\n",
      "  _warn((\"h5py is running against HDF5 {0} when it was built against {1}, \"\n",
      "/soft/datascience/conda/2023-01-10/mconda3/lib/python3.10/site-packages/transformers/generation/utils.py:1186: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"My dog really wanted to come right out and give me a hug, and I've never wanted his\"},\n",
       " {'generated_text': 'My dog really wanted to stay, so I was going to send him back home so I could watch'},\n",
       " {'generated_text': \"My dog really wanted to have some new things. She's a very bright puppy. She's my\"},\n",
       " {'generated_text': 'My dog really wanted to do so. She found out after we had gotten our car keys, and'},\n",
       " {'generated_text': 'My dog really wanted to get home so he started to pick it up,\" says Tom Rizzo'}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer,AutoModelForCausalLM, AutoConfig\n",
    "input_text = \"My dog really wanted to\"\n",
    "from transformers import pipeline\n",
    "generator = pipeline(\"text-generation\", model=\"openai-community/gpt2\")\n",
    "generator(input_text, max_length=20, num_return_sequences=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will cover  evaluation metrics,as well as safe and responsibilities practices when using LLMs in **Session 8**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also load in our own dataset and train a model with this data as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: accelerate in /home/dxr1234/.local/lib/python3.10/site-packages (0.29.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /soft/datascience/conda/2023-01-10/mconda3/lib/python3.10/site-packages (from accelerate) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /soft/datascience/conda/2023-01-10/mconda3/lib/python3.10/site-packages (from accelerate) (21.3)\n",
      "Requirement already satisfied: psutil in /soft/datascience/conda/2023-01-10/mconda3/lib/python3.10/site-packages (from accelerate) (5.9.4)\n",
      "Requirement already satisfied: pyyaml in /soft/datascience/conda/2023-01-10/mconda3/lib/python3.10/site-packages (from accelerate) (6.0)\n",
      "Requirement already satisfied: torch>=1.10.0 in /soft/datascience/conda/2023-01-10/mconda3/lib/python3.10/site-packages (from accelerate) (1.13.0a0+git49444c3)\n",
      "Requirement already satisfied: huggingface-hub in /soft/datascience/conda/2023-01-10/mconda3/lib/python3.10/site-packages (from accelerate) (0.12.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /home/dxr1234/.local/lib/python3.10/site-packages (from accelerate) (0.4.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /soft/datascience/conda/2023-01-10/mconda3/lib/python3.10/site-packages (from packaging>=20.0->accelerate) (3.0.9)\n",
      "Requirement already satisfied: typing-extensions in /soft/datascience/conda/2023-01-10/mconda3/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (4.4.0)\n",
      "Requirement already satisfied: filelock in /soft/datascience/conda/2023-01-10/mconda3/lib/python3.10/site-packages (from huggingface-hub->accelerate) (3.9.0)\n",
      "Requirement already satisfied: requests in /soft/datascience/conda/2023-01-10/mconda3/lib/python3.10/site-packages (from huggingface-hub->accelerate) (2.28.1)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /soft/datascience/conda/2023-01-10/mconda3/lib/python3.10/site-packages (from huggingface-hub->accelerate) (4.64.1)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /soft/datascience/conda/2023-01-10/mconda3/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /soft/datascience/conda/2023-01-10/mconda3/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /soft/datascience/conda/2023-01-10/mconda3/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /soft/datascience/conda/2023-01-10/mconda3/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (2022.12.7)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install accelerate -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TextDataset,DataCollatorForLanguageModeling\n",
    "\n",
    "def load_dataset(train_path,test_path,tokenizer):\n",
    "    train_dataset = TextDataset(\n",
    "          tokenizer=tokenizer,\n",
    "          file_path=train_path,\n",
    "          block_size=128) \n",
    "    test_dataset = TextDataset(\n",
    "          tokenizer=tokenizer,\n",
    "          file_path=test_path,\n",
    "          block_size=128)   \n",
    "    data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer, mlm=False,\n",
    "    )\n",
    "    return train_dataset,test_dataset,data_collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/soft/datascience/conda/2023-01-10/mconda3/lib/python3.10/site-packages/transformers/data/datasets/language_modeling.py:54: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ü§ó Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer,AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"openai-community/gpt2\")\n",
    "\n",
    "train_dataset,test_dataset,data_collator = load_dataset('dataset/train_input.txt','dataset/test_input.txt', tokenizer)\n",
    "\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gpt2\", #The output directory\n",
    "    overwrite_output_dir=True, #overwrite the content of the output directory\n",
    "    num_train_epochs=3, # number of training epochs\n",
    "    per_device_train_batch_size=32, # batch size for training\n",
    "    per_device_eval_batch_size=64,  # batch size for evaluation\n",
    "    eval_steps = 40, # Number of update steps between two evaluations.\n",
    "    save_steps=80, # after # steps model is saved \n",
    "    warmup_steps=50,# number of warmup steps for learning rate scheduler\n",
    "    prediction_loss_only=True,\n",
    "    )\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is going on below the hood?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two components that are \"black-boxes\" here:\n",
    "1. The method for tokenization\n",
    "2. The model that generates novel text.\n",
    "\n",
    "Carlo Graziani already gave a great explanation of tokenization last week and how this affects embeddings (https://github.com/argonne-lcf/ai-science-training-series/blob/main/04_intro_to_llms/Sequential_Data_Models.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Today we will take a closer look at how the model is designed to deal with language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look inside GPT2! GPT2 incorporates the `GPT2LMHeadModel` architecture so let's inspect this more closely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/dxr1234/.cache/huggingface/hub/models--openai-community--gpt2/snapshots/607a30d783dfa663caf39e06633721c8d4cfcd7e/config.json\n",
      "Model config GPT2Config {\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.26.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at /home/dxr1234/.cache/huggingface/hub/models--openai-community--gpt2/snapshots/607a30d783dfa663caf39e06633721c8d4cfcd7e/model.safetensors\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"transformers_version\": \"4.26.0\"\n",
      "}\n",
      "\n",
      "All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
      "\n",
      "All the weights of GPT2LMHeadModel were initialized from the model checkpoint at openai-community/gpt2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
      "loading configuration file generation_config.json from cache at /home/dxr1234/.cache/huggingface/hub/models--openai-community--gpt2/snapshots/607a30d783dfa663caf39e06633721c8d4cfcd7e/generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"transformers_version\": \"4.26.0\"\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2LMHeadModel(\n",
      "  (transformer): GPT2Model(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(1024, 768)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0): GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (1): GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (2): GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (3): GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (4): GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (5): GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (6): GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (7): GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (8): GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (9): GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (10): GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (11): GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "model = GPT2LMHeadModel.from_pretrained('openai-community/gpt2')\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General elements of an LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPT-2 is an example of the popular Transformer architecture. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cig2mvfguetQ"
   },
   "source": [
    "<img src=\"images/decoder_only_block.png\" alt=\"Drawing\" style=\"width: 200px;\"/>\n",
    "Image credit: https://arxiv.org/pdf/1706.03762.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gray section in this figure is the Transfomer Decoder and it is the main mechanism GPT2 uses to encode context of language into its predictions.\n",
    "\n",
    "<img src=\"images/transformer-decoder-intro.png\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "Image credit: https://jalammar.github.io/illustrated-gpt2/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Transformer-Decoder is composed of Decoder blocks stacked ontop of each other where each contains two types of layers: \n",
    "1. Masked Self-Attention and \n",
    "2. Feed Forward Neural Networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have already discussed Feed Forward Neural Networks in detail in the other lectures in this series. To review this, please look at https://github.com/argonne-lcf/ai-science-training-series/blob/main/02_intro_neural_networks/01_introduction_mnist.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lecture, we will \n",
    "* First, discuss attention mechanisms at length as this is arguably the greatest contribution by Transformers.\n",
    "* Second, extend the discussion from last week (https://github.com/argonne-lcf/ai-science-training-series/blob/main/04_intro_to_llms/Sequential_Data_Models.ipynb) on embedding input data while taking into account position.\n",
    "* Third, discuss outputting real text/sequences from the models.\n",
    "* Fourth, build a training loop for a mini-LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's set up all the imports we will need**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1474f31acbd0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## IMPORTS\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337)\n",
    "# hyperparameters\n",
    "batch_size = 16 # how many independent sequences will we process in parallel?\n",
    "block_size = 32 # what is the maximum context length for predictions?\n",
    "max_iters = 5000\n",
    "eval_interval = 100\n",
    "learning_rate = 1e-3\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = 64\n",
    "n_head = 4 ## so head_size = 16\n",
    "n_layer = 4\n",
    "dropout = 0.0\n",
    "# ------------\n",
    "\n",
    "torch.manual_seed(1337)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BowLYFlCrDrr"
   },
   "source": [
    "## Attention mechanisms\n",
    "\n",
    "Suppose the following sentence is an input sentence we want to translate using an LLM:\n",
    "\n",
    "`‚ÄùThe animal didn't cross the street because it was too tired‚Äù`\n",
    "\n",
    "Last week, Carlo mentioned that the Transformer learns an embedding of all words allowing interpretation of meanings of words.\n",
    "\n",
    "<img src=\"images/viz-bert-voc-verbs.png\" alt=\"Drawing\" style=\"width: 400px;\"/>\n",
    "\n",
    "So, if the model did a good job in token embedding, it will \"know\" what all the words in this sentence mean. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But to understand a full sentence, the model also need to understand what each word means in relation to other words.\n",
    "\n",
    "For example, when we read the sentence:\n",
    "`‚ÄùThe animal didn't cross the street because it was too tired‚Äù`\n",
    "we know intuitively that the word `\"it\"` refers to `\"animal\"`, the state for `\"it\"` is `\"tired\"`, and the associated action is `\"didn't cross\"`.\n",
    "\n",
    "However, the model needs a way to learn all of this information in a simple yet generalizable way.\n",
    "What makes Transformers particularly powerful compared to earlier sequential architectures is how it encodes context with the **self-attention mechanism**.\n",
    "\n",
    "As the model processes each word in the input sequence, attention looks at other positions in the input sequence for clues to a better understanding for this word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/transformer_self-attention_visualization.png\" alt=\"Drawing\" style=\"width: 300px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UGbAi0cJ7x3a"
   },
   "source": [
    "Image credit: https://jalammar.github.io/illustrated-transformer/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Self-attention mechanisms use 3 vectors to encode the context of a word in a sequence with another word:\n",
    "1. Query: the word representation we score other words against using the other word's keys\n",
    "2. Key: labels for the words in a sequence that we match against the query\n",
    "3. Value: actual word representation. We will use the queries and keys to score the word's relevance to the query, and multiply this by the value. \n",
    "\n",
    "An analogy provided by Jay Alammar is thinking about attention as choosing a file from a file cabinet according to information on a post-it note. You can use the post-it note (query) to identify the folder (key) that most matches the topic you are looking up. Then you access the contents of the file (value) according to its relevance to your query."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/self-attention-example-folders-3.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "Image credit: https://jalammar.github.io/illustrated-gpt2/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our models, we can encode queries, keys, and values using simple linear layers with the same size (`sequence length, head_size`). During the training process, these layers will be updated to best encode context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = 32 # channels\n",
    "head_size = 16\n",
    "\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jzf9VE_AqWeR"
   },
   "source": [
    "The algorithm for self-attention is as follows:\n",
    "\n",
    "1. Generate query, key and value vectors for each word\n",
    "2. Calculate a score for each word in the input sentence against each other.\n",
    "3. Divide the scores by the square root of the dimension of the key vectors to stabilize the gradients. This is then passed through a softmax operation.\n",
    "4. Multiply each value vector by the softmax score.\n",
    "5. Sum up the weighted value vectors to produce the output.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/self-attention-output.png\" alt=\"Drawing\" style=\"width: 450px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yOwm-NkXA8U3"
   },
   "source": [
    "Image credit: https://jalammar.github.io/illustrated-transformer/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how attention is performed in the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337)\n",
    "B,T,C = 4,8,32 # batch, time, channels\n",
    "x = torch.randn(B,T,C)\n",
    "\n",
    "# Here we want the wei to be data dependent - ie gather info from the past but in a data dependant way\n",
    "\n",
    "head_size = 16\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "k = key(x)   # (B, T, 16) # each token here (totally B*T) produce a key and query in parallel and independently\n",
    "q = query(x) # (B, T, 16)\n",
    "v = value(x)\n",
    "\n",
    "wei =  q @ k.transpose(-2, -1) * head_size**-0.5 # (B, T, 16) @ (B, 16, T) ---> (B, T, T). #\n",
    "wei = F.softmax(wei, dim=-1) # exponentiate and normalize giving a nice distibution that sums to 1 and\n",
    "                             # now it tells us that in a data dependent manner how much of info to aggregate from\n",
    "\n",
    "out = wei @ v # aggregate the attention scores and value vector.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0618, -0.0091, -0.3488,  0.3208,  0.2971, -0.1573, -0.0561,  0.1068,\n",
      "          0.0368,  0.0139, -0.0017,  0.3110,  0.1404, -0.0158,  0.1853,  0.4290],\n",
      "        [ 0.1578, -0.0971, -0.4256,  0.3538,  0.3621, -0.2392, -0.0536,  0.1759,\n",
      "          0.1115,  0.0282, -0.0649,  0.3641,  0.1928,  0.0261,  0.2162,  0.3758],\n",
      "        [ 0.1293,  0.0759, -0.2946,  0.2292,  0.2215, -0.0710, -0.0107,  0.1616,\n",
      "         -0.0930, -0.0877,  0.0567,  0.1899,  0.0311, -0.0894,  0.0309,  0.5471],\n",
      "        [ 0.1247,  0.1400, -0.2436,  0.1819,  0.1976,  0.0338, -0.0028,  0.1124,\n",
      "         -0.1477, -0.0748,  0.0650,  0.1392, -0.0314, -0.0989,  0.0613,  0.5433],\n",
      "        [ 0.0667,  0.1845, -0.2135,  0.2813,  0.2064,  0.0873,  0.0084,  0.2055,\n",
      "         -0.1130, -0.1466,  0.0459,  0.1923, -0.0275, -0.1107,  0.0065,  0.4674],\n",
      "        [ 0.1924,  0.1693, -0.1568,  0.2284,  0.1620,  0.0737,  0.0443,  0.2519,\n",
      "         -0.1912, -0.1979,  0.0832,  0.0713, -0.0826, -0.0848, -0.1047,  0.6089],\n",
      "        [ 0.1184,  0.0884, -0.2652,  0.2560,  0.1840,  0.0284, -0.0621,  0.1181,\n",
      "         -0.0880,  0.0104,  0.1123,  0.1850,  0.0369, -0.0730,  0.0663,  0.5242],\n",
      "        [ 0.1243,  0.0453, -0.3412,  0.2709,  0.2335, -0.0948, -0.0421,  0.2143,\n",
      "         -0.0330, -0.0313,  0.0520,  0.2378,  0.1084, -0.0959,  0.0300,  0.4707]],\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(out[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4lwyFlxKW6oA"
   },
   "source": [
    "### Multi-head attention\n",
    "\n",
    "In practice, multiple attention heads are used which\n",
    "1. Expands the model‚Äôs ability to focus on different positions and prevent the attention to be dominated by the word itself.\n",
    "2. Have multiple ‚Äúrepresentation subspaces‚Äù. Have multiple sets of Query/Key/Value weight matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/transformer_multi-headed_self-attention-recap.png\" alt=\"Drawing\" style=\"width: 700px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6oHsezdVBIaf"
   },
   "source": [
    "Image credit: https://jalammar.github.io/illustrated-transformer/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's see attention mechanisms in action!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use the powerful visualization tool bertviz, which allows an interactive experience of the attention mechanisms. Normally these mechanisms are abstracted away but this will allow us to inspect our model in more detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: bertviz in /home/dxr1234/.local/lib/python3.10/site-packages (1.4.0)\n",
      "Requirement already satisfied: transformers>=2.0 in /soft/datascience/conda/2023-01-10/mconda3/lib/python3.10/site-packages (from bertviz) (4.26.0)\n",
      "Requirement already satisfied: torch>=1.0 in /soft/datascience/conda/2023-01-10/mconda3/lib/python3.10/site-packages (from bertviz) (1.13.0a0+git49444c3)\n",
      "Requirement already satisfied: tqdm in /soft/datascience/conda/2023-01-10/mconda3/lib/python3.10/site-packages (from bertviz) (4.64.1)\n",
      "Requirement already satisfied: boto3 in /home/dxr1234/.local/lib/python3.10/site-packages (from bertviz) (1.34.80)\n",
      "Requirement already satisfied: requests in /soft/datascience/conda/2023-01-10/mconda3/lib/python3.10/site-packages (from bertviz) (2.28.1)\n",
      "Requirement already satisfied: regex in /soft/datascience/conda/2023-01-10/mconda3/lib/python3.10/site-packages (from bertviz) (2022.10.31)\n",
      "Requirement already satisfied: sentencepiece in /home/dxr1234/.local/lib/python3.10/site-packages (from bertviz) (0.2.0)\n",
      "Requirement already satisfied: typing-extensions in /soft/datascience/conda/2023-01-10/mconda3/lib/python3.10/site-packages (from torch>=1.0->bertviz) (4.4.0)\n",
      "Requirement already satisfied: filelock in /soft/datascience/conda/2023-01-10/mconda3/lib/python3.10/site-packages (from transformers>=2.0->bertviz) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /soft/datascience/conda/2023-01-10/mconda3/lib/python3.10/site-packages (from transformers>=2.0->bertviz) (0.12.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /soft/datascience/conda/2023-01-10/mconda3/lib/python3.10/site-packages (from transformers>=2.0->bertviz) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /soft/datascience/conda/2023-01-10/mconda3/lib/python3.10/site-packages (from transformers>=2.0->bertviz) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /soft/datascience/conda/2023-01-10/mconda3/lib/python3.10/site-packages (from transformers>=2.0->bertviz) (6.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /soft/datascience/conda/2023-01-10/mconda3/lib/python3.10/site-packages (from transformers>=2.0->bertviz) (0.13.2)\n",
      "Requirement already satisfied: botocore<1.35.0,>=1.34.80 in /home/dxr1234/.local/lib/python3.10/site-packages (from boto3->bertviz) (1.34.80)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /home/dxr1234/.local/lib/python3.10/site-packages (from boto3->bertviz) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in /home/dxr1234/.local/lib/python3.10/site-packages (from boto3->bertviz) (0.10.1)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /soft/datascience/conda/2023-01-10/mconda3/lib/python3.10/site-packages (from requests->bertviz) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /soft/datascience/conda/2023-01-10/mconda3/lib/python3.10/site-packages (from requests->bertviz) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /soft/datascience/conda/2023-01-10/mconda3/lib/python3.10/site-packages (from requests->bertviz) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /soft/datascience/conda/2023-01-10/mconda3/lib/python3.10/site-packages (from requests->bertviz) (2022.12.7)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /soft/datascience/conda/2023-01-10/mconda3/lib/python3.10/site-packages (from botocore<1.35.0,>=1.34.80->boto3->bertviz) (2.8.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /soft/datascience/conda/2023-01-10/mconda3/lib/python3.10/site-packages (from packaging>=20.0->transformers>=2.0->bertviz) (3.0.9)\n",
      "Requirement already satisfied: six>=1.5 in /soft/datascience/conda/2023-01-10/mconda3/lib/python3.10/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.35.0,>=1.34.80->boto3->bertviz) (1.16.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install bertviz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load in the model, GPT2 and look at the attention mechanisms. \n",
    "\n",
    "**Hint... click on the different blocks in the visualization to see the attention**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\"></script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "      \n",
       "        <div id=\"bertviz-8e96ccb089214e25aab782e3d2695e5e\" style=\"font-family:'Helvetica Neue', Helvetica, Arial, sans-serif;\">\n",
       "            <span style=\"user-select:none\">\n",
       "                \n",
       "            </span>\n",
       "            <div id='vis'></div>\n",
       "        </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "/**\n",
       " * @fileoverview Transformer Visualization D3 javascript code.\n",
       " *\n",
       " * Based on: https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/visualization/attention.js\n",
       " *\n",
       " * Change log:\n",
       " *\n",
       " * 02/01/19  Jesse Vig   Initial implementation\n",
       " * 12/31/20  Jesse Vig   Support multiple visualizations in single notebook.\n",
       " * 01/19/21  Jesse Vig   Support light/dark modes\n",
       " * 02/06/21  Jesse Vig   Move require config from separate jupyter notebook step\n",
       " * 05/03/21  Jesse Vig   Adjust visualization height dynamically\n",
       " * 03/23/22  Daniel SC   Update requirement URLs for d3 and jQuery (source of bug not allowing end result to be displayed on browsers)\n",
       " **/\n",
       "\n",
       "require.config({\n",
       "  paths: {\n",
       "      d3: 'https://cdnjs.cloudflare.com/ajax/libs/d3/5.7.0/d3.min',\n",
       "    jquery: 'https://cdnjs.cloudflare.com/ajax/libs/jquery/2.0.0/jquery.min',\n",
       "  }\n",
       "});\n",
       "\n",
       "requirejs(['jquery', 'd3'], function($, d3) {\n",
       "\n",
       "        const params = {\"attention\": [{\"name\": null, \"attn\": [[[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.961219847202301, 0.038780149072408676, 0.0, 0.0, 0.0, 0.0], [0.7466979026794434, 0.11987314373254776, 0.1334289014339447, 0.0, 0.0, 0.0], [0.5885030031204224, 0.13792067766189575, 0.212137371301651, 0.06143897399306297, 0.0, 0.0], [0.6570857763290405, 0.08996301889419556, 0.12751281261444092, 0.08361563086509705, 0.041822850704193115, 0.0], [0.2728874385356903, 0.11203353852033615, 0.1663985401391983, 0.08467111736536026, 0.16952736675739288, 0.19448210299015045]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.010616563260555267, 0.9893833994865417, 0.0, 0.0, 0.0, 0.0], [0.0024677535984665155, 0.008448007516562939, 0.9890841841697693, 0.0, 0.0, 0.0], [0.0001232847134815529, 0.0018733182223513722, 0.013126976788043976, 0.9848763942718506, 0.0, 0.0], [0.0010669564362615347, 0.001136627048254013, 0.003034998197108507, 0.0015735096530988812, 0.9931879043579102, 0.0], [0.00019791982776951045, 0.0010528112761676311, 0.0015437351539731026, 0.0009642760851420462, 3.4924432839034125e-05, 0.9962062835693359]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.47578439116477966, 0.524215579032898, 0.0, 0.0, 0.0, 0.0], [0.5906045436859131, 0.2486611008644104, 0.16073434054851532, 0.0, 0.0, 0.0], [0.5529289841651917, 0.18856702744960785, 0.14457571506500244, 0.11392831057310104, 0.0, 0.0], [0.45094072818756104, 0.16486799716949463, 0.17318038642406464, 0.11748014390468597, 0.09353074431419373, 0.0], [0.4257245659828186, 0.1732865273952484, 0.15651953220367432, 0.07022649794816971, 0.0808701142668724, 0.09337282180786133]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6133623123168945, 0.38663768768310547, 0.0, 0.0, 0.0, 0.0], [0.06098509579896927, 0.03253461793065071, 0.9064802527427673, 0.0, 0.0, 0.0], [0.006717085838317871, 0.0004012881254311651, 0.7572958469390869, 0.23558568954467773, 0.0, 0.0], [0.03722766041755676, 0.002948855282738805, 0.10081092268228531, 0.04142269119620323, 0.8175898790359497, 0.0], [0.04989781975746155, 0.00030758307548239827, 0.0024198265746235847, 0.0034334994852542877, 0.0006823898293077946, 0.9432588815689087]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9489555954933167, 0.051044441759586334, 0.0, 0.0, 0.0, 0.0], [0.6821408867835999, 0.1395241767168045, 0.17833495140075684, 0.0, 0.0, 0.0], [0.20366324484348297, 0.05641487240791321, 0.06399301439523697, 0.6759288311004639, 0.0, 0.0], [0.3419547975063324, 0.06725440919399261, 0.07926183938980103, 0.1783619523048401, 0.3331669867038727, 0.0], [0.09464015811681747, 0.0074282134883105755, 0.006983973551541567, 0.0071843694895505905, 0.018724264577031136, 0.865039050579071]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.33834606409072876, 0.6616539359092712, 0.0, 0.0, 0.0, 0.0], [0.07855993509292603, 0.006165449041873217, 0.9152746200561523, 0.0, 0.0, 0.0], [0.01677597686648369, 0.0004037705948576331, 0.003340460592880845, 0.9794798493385315, 0.0, 0.0], [0.027600426226854324, 0.00044415233423933387, 0.0006541680195368826, 0.0002266185765620321, 0.971074640750885, 0.0], [0.010248198173940182, 3.701553578139283e-05, 0.00016064041119534522, 2.7341819077264518e-05, 1.0187304724240676e-05, 0.98951655626297]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.982503354549408, 0.017496665939688683, 0.0, 0.0, 0.0, 0.0], [0.8874197006225586, 0.05467939004302025, 0.05790085718035698, 0.0, 0.0, 0.0], [0.6849910616874695, 0.1228068619966507, 0.04972026124596596, 0.14248186349868774, 0.0, 0.0], [0.6015856862068176, 0.09881888329982758, 0.07070108503103256, 0.16652540862560272, 0.06236903741955757, 0.0], [0.3232504427433014, 0.12567411363124847, 0.04432179778814316, 0.07076980918645859, 0.06606649607419968, 0.36991727352142334]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9191647171974182, 0.0808352455496788, 0.0, 0.0, 0.0, 0.0], [0.45986413955688477, 0.39703112840652466, 0.14310479164123535, 0.0, 0.0, 0.0], [0.3003872334957123, 0.22181738913059235, 0.38161516189575195, 0.09618020057678223, 0.0, 0.0], [0.18963925540447235, 0.1376371532678604, 0.20173484086990356, 0.23632164299488068, 0.23466713726520538, 0.0], [0.15410441160202026, 0.09489496797323227, 0.11902562528848648, 0.10277965664863586, 0.4317220449447632, 0.09747327119112015]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.364999920129776, 0.6350001096725464, 0.0, 0.0, 0.0, 0.0], [0.24595215916633606, 0.5519201755523682, 0.20212766528129578, 0.0, 0.0, 0.0], [0.2721358835697174, 0.40738627314567566, 0.25186213850975037, 0.06861574947834015, 0.0, 0.0], [0.10242555290460587, 0.16683615744113922, 0.524804949760437, 0.05445462837815285, 0.15147870779037476, 0.0], [0.25029507279396057, 0.22198128700256348, 0.18899968266487122, 0.10677118599414825, 0.1303267478942871, 0.10162602365016937]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6990506649017334, 0.3009493350982666, 0.0, 0.0, 0.0, 0.0], [0.5107942819595337, 0.2948642075061798, 0.1943415403366089, 0.0, 0.0, 0.0], [0.4604707360267639, 0.2805190980434418, 0.19174803793430328, 0.0672621801495552, 0.0, 0.0], [0.37648412585258484, 0.21120662987232208, 0.20214538276195526, 0.10207021236419678, 0.10809355974197388, 0.0], [0.30138441920280457, 0.20456179976463318, 0.18250338733196259, 0.11019382625818253, 0.1629127413034439, 0.03844383731484413]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7131582498550415, 0.2868417799472809, 0.0, 0.0, 0.0, 0.0], [0.4058799147605896, 0.18063297867774963, 0.41348710656166077, 0.0, 0.0, 0.0], [0.265546053647995, 0.1698586493730545, 0.3358593285083771, 0.228736013174057, 0.0, 0.0], [0.31385406851768494, 0.1831669807434082, 0.14928358793258667, 0.05377671495079994, 0.29991865158081055, 0.0], [0.20466560125350952, 0.18731118738651276, 0.15959151089191437, 0.06381776183843613, 0.03642302006483078, 0.34819093346595764]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6586242914199829, 0.3413757383823395, 0.0, 0.0, 0.0, 0.0], [0.5917776226997375, 0.3160035014152527, 0.0922188088297844, 0.0, 0.0, 0.0], [0.5477152466773987, 0.23586955666542053, 0.061456020921468735, 0.1549593061208725, 0.0, 0.0], [0.4587061107158661, 0.22439992427825928, 0.07887422293424606, 0.0992034301161766, 0.13881628215312958, 0.0], [0.32743722200393677, 0.19600819051265717, 0.068057119846344, 0.0892510637640953, 0.11618079245090485, 0.20306548476219177]]], [[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9961552023887634, 0.0038448425475507975, 0.0, 0.0, 0.0, 0.0], [0.8594854474067688, 0.06906110048294067, 0.07145342975854874, 0.0, 0.0, 0.0], [0.3800053000450134, 0.04127567633986473, 0.5496612787246704, 0.029057776555418968, 0.0, 0.0], [0.21445226669311523, 0.05088742449879646, 0.4317440092563629, 0.25869303941726685, 0.044223275035619736, 0.0], [0.11175256222486496, 0.017593080177903175, 0.027507441118359566, 0.04086771607398987, 0.7754669785499573, 0.026812179014086723]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9285967946052551, 0.07140326499938965, 0.0, 0.0, 0.0, 0.0], [0.6077286005020142, 0.3121427297592163, 0.08012867718935013, 0.0, 0.0, 0.0], [0.4942909777164459, 0.28503698110580444, 0.11849315464496613, 0.10217894613742828, 0.0, 0.0], [0.4183879494667053, 0.23117904365062714, 0.0834062322974205, 0.11365949362516403, 0.1533672958612442, 0.0], [0.42215850949287415, 0.12917140126228333, 0.08740927278995514, 0.1016375944018364, 0.21230268478393555, 0.04732053726911545]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9786475896835327, 0.02135237120091915, 0.0, 0.0, 0.0, 0.0], [0.7749121785163879, 0.06510371714830399, 0.15998409688472748, 0.0, 0.0, 0.0], [0.6484923362731934, 0.07483134418725967, 0.14751605689525604, 0.12916021049022675, 0.0, 0.0], [0.5224639773368835, 0.06921815127134323, 0.13823404908180237, 0.1110658198595047, 0.15901805460453033, 0.0], [0.3964517116546631, 0.07325823605060577, 0.12938153743743896, 0.1064242571592331, 0.14864002168178558, 0.1458442211151123]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5525906085968018, 0.44740936160087585, 0.0, 0.0, 0.0, 0.0], [0.5585009455680847, 0.2176259458065033, 0.22387312352657318, 0.0, 0.0, 0.0], [0.5143128633499146, 0.15964674949645996, 0.15491968393325806, 0.1711207628250122, 0.0, 0.0], [0.5039961338043213, 0.11401888728141785, 0.11974027007818222, 0.12552587687969208, 0.13671889901161194, 0.0], [0.5061842799186707, 0.08567393571138382, 0.08903021365404129, 0.09759818762540817, 0.1027572825551033, 0.11875619739294052]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9242545366287231, 0.07574543356895447, 0.0, 0.0, 0.0, 0.0], [0.8257425427436829, 0.07932533323764801, 0.09493216127157211, 0.0, 0.0, 0.0], [0.7306380271911621, 0.0857183039188385, 0.08043931424617767, 0.10320431739091873, 0.0, 0.0], [0.6383238434791565, 0.07886394113302231, 0.07815027981996536, 0.08758097141981125, 0.1170809343457222, 0.0], [0.5552157163619995, 0.07409121096134186, 0.06834889203310013, 0.07778600603342056, 0.09999319165945053, 0.12456497550010681]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8578913807868958, 0.14210854470729828, 0.0, 0.0, 0.0, 0.0], [0.6423038244247437, 0.166290283203125, 0.19140593707561493, 0.0, 0.0, 0.0], [0.5530979633331299, 0.10609274357557297, 0.07821257412433624, 0.26259663701057434, 0.0, 0.0], [0.40121692419052124, 0.12223611027002335, 0.1934729963541031, 0.14164622128009796, 0.14142780005931854, 0.0], [0.40212565660476685, 0.18450751900672913, 0.07516805827617645, 0.05849048122763634, 0.1444634348154068, 0.13524490594863892]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9791558980941772, 0.020844051614403725, 0.0, 0.0, 0.0, 0.0], [0.8829841613769531, 0.06233249977231026, 0.05468335747718811, 0.0, 0.0, 0.0], [0.8105455040931702, 0.08617085963487625, 0.07321777194738388, 0.03006584383547306, 0.0, 0.0], [0.6819812059402466, 0.04990820586681366, 0.08296552300453186, 0.08369525521993637, 0.10144983977079391, 0.0], [0.4056689441204071, 0.07337666302919388, 0.08601408451795578, 0.061709366738796234, 0.13226434588432312, 0.2409665435552597]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9670190811157227, 0.03298088163137436, 0.0, 0.0, 0.0, 0.0], [0.8449064493179321, 0.0851450264453888, 0.06994850933551788, 0.0, 0.0, 0.0], [0.7123572826385498, 0.07896047830581665, 0.055410757660865784, 0.15327158570289612, 0.0, 0.0], [0.6402613520622253, 0.0739755630493164, 0.044393062591552734, 0.14322125911712646, 0.09814881533384323, 0.0], [0.5073903799057007, 0.07523059099912643, 0.07754647731781006, 0.11362491548061371, 0.13947951793670654, 0.08672808855772018]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8487569093704224, 0.1512431502342224, 0.0, 0.0, 0.0, 0.0], [0.8415648937225342, 0.12107233703136444, 0.03736274689435959, 0.0, 0.0, 0.0], [0.7505517601966858, 0.11348944902420044, 0.06179959326982498, 0.07415912300348282, 0.0, 0.0], [0.6614719033241272, 0.10242646187543869, 0.052934251725673676, 0.07529708743095398, 0.10787025839090347, 0.0], [0.6014202237129211, 0.11340376734733582, 0.05631929263472557, 0.07096721231937408, 0.10906282067298889, 0.04882663115859032]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9445484280586243, 0.05545158311724663, 0.0, 0.0, 0.0, 0.0], [0.8874568939208984, 0.05474215745925903, 0.0578010231256485, 0.0, 0.0, 0.0], [0.8281888961791992, 0.06895001977682114, 0.059034693986177444, 0.0438263975083828, 0.0, 0.0], [0.6429892778396606, 0.0674755647778511, 0.11629703640937805, 0.05417950078845024, 0.11905858665704727, 0.0], [0.7367823719978333, 0.056119054555892944, 0.06857288628816605, 0.034219540655612946, 0.0787537544965744, 0.02555238828063011]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0002913394710049033, 0.9997085928916931, 0.0, 0.0, 0.0, 0.0], [0.0007981209782883525, 0.5288336873054504, 0.4703682065010071, 0.0, 0.0, 0.0], [0.0007648481405340135, 0.34519824385643005, 0.3085267245769501, 0.34551018476486206, 0.0, 0.0], [0.0010283143492415547, 0.241359144449234, 0.23320138454437256, 0.2555713355541229, 0.2688397467136383, 0.0], [0.0009746829164214432, 0.17789699137210846, 0.16743157804012299, 0.1858760118484497, 0.18734444677829742, 0.28047630190849304]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.824492871761322, 0.17550717294216156, 0.0, 0.0, 0.0, 0.0], [0.12386877834796906, 0.044499922543764114, 0.8316312432289124, 0.0, 0.0, 0.0], [0.07924355566501617, 0.01296587660908699, 0.0015277155907824636, 0.9062628149986267, 0.0, 0.0], [0.08806384354829788, 0.0213409923017025, 0.0028886159416288137, 0.002845379989594221, 0.884861171245575, 0.0], [0.09983218461275101, 0.03363388776779175, 0.0054999832063913345, 0.002433052286505699, 0.0015082412865012884, 0.8570926189422607]]], [[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9646892547607422, 0.03531072288751602, 0.0, 0.0, 0.0, 0.0], [0.7529157400131226, 0.08733473718166351, 0.15974950790405273, 0.0, 0.0, 0.0], [0.4202282726764679, 0.09195102006196976, 0.23549850285053253, 0.25232216715812683, 0.0, 0.0], [0.30848920345306396, 0.05908140912652016, 0.38391315937042236, 0.15659146010875702, 0.09192468225955963, 0.0], [0.44790443778038025, 0.04329312965273857, 0.0796918049454689, 0.11081931740045547, 0.22124572098255157, 0.09704558551311493]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.991096019744873, 0.008904009126126766, 0.0, 0.0, 0.0, 0.0], [0.9697675704956055, 0.026084503158926964, 0.004147922620177269, 0.0, 0.0, 0.0], [0.9082901477813721, 0.033206019550561905, 0.00942116230726242, 0.049082688987255096, 0.0, 0.0], [0.8949133157730103, 0.05544555187225342, 0.005577624775469303, 0.03150692582130432, 0.012556522153317928, 0.0], [0.8497740030288696, 0.028890123590826988, 0.0036647915840148926, 0.03751987963914871, 0.038427725434303284, 0.04172350466251373]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9984525442123413, 0.0015474462416023016, 0.0, 0.0, 0.0, 0.0], [0.48947831988334656, 0.4812193810939789, 0.029302269220352173, 0.0, 0.0, 0.0], [0.11772153526544571, 0.13121186196804047, 0.6702314615249634, 0.08083520829677582, 0.0, 0.0], [0.13043689727783203, 0.04068669304251671, 0.2652038037776947, 0.4114362895488739, 0.15223638713359833, 0.0], [0.12661904096603394, 0.03275119513273239, 0.03567872568964958, 0.06039190664887428, 0.6021825075149536, 0.1423766165971756]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9805176854133606, 0.019482342526316643, 0.0, 0.0, 0.0, 0.0], [0.7948849201202393, 0.12061909586191177, 0.08449601382017136, 0.0, 0.0, 0.0], [0.5612356066703796, 0.15743127465248108, 0.20339730381965637, 0.0779358446598053, 0.0, 0.0], [0.42583736777305603, 0.10742014646530151, 0.15123659372329712, 0.08755031228065491, 0.22795552015304565, 0.0], [0.24752654135227203, 0.024188270792365074, 0.03039524517953396, 0.08586956560611725, 0.5714336633682251, 0.040586672723293304]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9887767434120178, 0.011223225854337215, 0.0, 0.0, 0.0, 0.0], [0.7572693228721619, 0.22317346930503845, 0.019557112827897072, 0.0, 0.0, 0.0], [0.5341880321502686, 0.22107566893100739, 0.1762184202671051, 0.06851787120103836, 0.0, 0.0], [0.17095312476158142, 0.0822940468788147, 0.576022207736969, 0.11097585409879684, 0.059754710644483566, 0.0], [0.2487109899520874, 0.08880793303251266, 0.08980197459459305, 0.09729334712028503, 0.4413093626499176, 0.03407646715641022]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8422133326530457, 0.15778663754463196, 0.0, 0.0, 0.0, 0.0], [0.468412846326828, 0.46105360984802246, 0.07053359597921371, 0.0, 0.0, 0.0], [0.2588140666484833, 0.4635888636112213, 0.18503506481647491, 0.09256205707788467, 0.0, 0.0], [0.18399578332901, 0.29154160618782043, 0.17031098902225494, 0.27173006534576416, 0.08242159336805344, 0.0], [0.1646990180015564, 0.2472696155309677, 0.08770562708377838, 0.22575001418590546, 0.1774536371231079, 0.09712201356887817]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9919946193695068, 0.008005390875041485, 0.0, 0.0, 0.0, 0.0], [0.9068724513053894, 0.044065121561288834, 0.04906242713332176, 0.0, 0.0, 0.0], [0.8582221865653992, 0.055348269641399384, 0.040419407188892365, 0.046010036021471024, 0.0, 0.0], [0.7855252623558044, 0.041242364794015884, 0.08369296044111252, 0.04887620359659195, 0.040663279592990875, 0.0], [0.7856317162513733, 0.05014643445611, 0.04751267284154892, 0.027365952730178833, 0.05614755302667618, 0.03319567069411278]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9041035175323486, 0.09589648246765137, 0.0, 0.0, 0.0, 0.0], [0.5862312912940979, 0.07199832051992416, 0.34177035093307495, 0.0, 0.0, 0.0], [0.3878960907459259, 0.04660807177424431, 0.20278996229171753, 0.36270591616630554, 0.0, 0.0], [0.2665242552757263, 0.024533024057745934, 0.12211935967206955, 0.20041218400001526, 0.386411190032959, 0.0], [0.23357485234737396, 0.02053728699684143, 0.09610321372747421, 0.13062246143817902, 0.22990450263023376, 0.289257675409317]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9639912247657776, 0.036008793860673904, 0.0, 0.0, 0.0, 0.0], [0.7075552344322205, 0.2542775869369507, 0.038167137652635574, 0.0, 0.0, 0.0], [0.2566526234149933, 0.20589298009872437, 0.01665665954351425, 0.5207977294921875, 0.0, 0.0], [0.1037939190864563, 0.04639088362455368, 0.008698614314198494, 0.7866851687431335, 0.05443140119314194, 0.0], [0.2214341163635254, 0.03379744663834572, 0.029023902490735054, 0.541292130947113, 0.15286092460155487, 0.021591555327177048]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9891703724861145, 0.010829661041498184, 0.0, 0.0, 0.0, 0.0], [0.7913155555725098, 0.12309625744819641, 0.08558809012174606, 0.0, 0.0, 0.0], [0.2954600155353546, 0.15808308124542236, 0.4217240810394287, 0.1247328370809555, 0.0, 0.0], [0.23440983891487122, 0.09886523336172104, 0.33160170912742615, 0.1971396654844284, 0.1379835456609726, 0.0], [0.19728390872478485, 0.05741839483380318, 0.06909029185771942, 0.16469819843769073, 0.2797277867794037, 0.23178131878376007]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9359127879142761, 0.0640871673822403, 0.0, 0.0, 0.0, 0.0], [0.7888627648353577, 0.08673475682735443, 0.12440246343612671, 0.0, 0.0, 0.0], [0.6535118818283081, 0.07573551684617996, 0.09732568264007568, 0.17342689633369446, 0.0, 0.0], [0.522276759147644, 0.058278825134038925, 0.09920477122068405, 0.17020836472511292, 0.15003129839897156, 0.0], [0.4108840823173523, 0.047306034713983536, 0.07265672832727432, 0.10560744255781174, 0.10550004243850708, 0.25804558396339417]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9683833122253418, 0.03161672502756119, 0.0, 0.0, 0.0, 0.0], [0.8965396881103516, 0.038870569318532944, 0.06458976864814758, 0.0, 0.0, 0.0], [0.8264952898025513, 0.03213464096188545, 0.05196719989180565, 0.0894029513001442, 0.0, 0.0], [0.7718173265457153, 0.030402837321162224, 0.045827414840459824, 0.07118473201990128, 0.08076759427785873, 0.0], [0.7292331457138062, 0.021699821576476097, 0.033074747771024704, 0.04720093309879303, 0.06474557518959045, 0.10404567420482635]]], [[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9979567527770996, 0.0020432830788195133, 0.0, 0.0, 0.0, 0.0], [0.955294132232666, 0.00802531372755766, 0.03668047487735748, 0.0, 0.0, 0.0], [0.9254710078239441, 0.002755576279014349, 0.0020629852078855038, 0.06971040368080139, 0.0, 0.0], [0.8660576939582825, 0.0038883681409060955, 0.0006785982404835522, 0.0006981453043408692, 0.1286771297454834, 0.0], [0.8455929160118103, 0.0037804055027663708, 0.000253423087997362, 6.0270751419011503e-05, 0.00011820747749879956, 0.15019479393959045]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9262455105781555, 0.07375453412532806, 0.0, 0.0, 0.0, 0.0], [0.7717157006263733, 0.16241952776908875, 0.06586471945047379, 0.0, 0.0, 0.0], [0.8167637586593628, 0.07807160913944244, 0.06324034929275513, 0.041924238204956055, 0.0, 0.0], [0.6867184638977051, 0.07755157351493835, 0.10056912153959274, 0.05955080687999725, 0.07561002671718597, 0.0], [0.6421161890029907, 0.11014898866415024, 0.07688194513320923, 0.054033469408750534, 0.10333634912967682, 0.013483096845448017]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9395954608917236, 0.060404520481824875, 0.0, 0.0, 0.0, 0.0], [0.23004619777202606, 0.6617380380630493, 0.1082158014178276, 0.0, 0.0, 0.0], [0.2670227289199829, 0.3607950508594513, 0.3249626159667969, 0.047219593077898026, 0.0, 0.0], [0.595201313495636, 0.12269274890422821, 0.06302059441804886, 0.08916817605495453, 0.12991715967655182, 0.0], [0.10284596681594849, 0.02938011661171913, 0.013739082030951977, 0.045860596001148224, 0.7698501348495483, 0.03832406550645828]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9040980935096741, 0.09590194374322891, 0.0, 0.0, 0.0, 0.0], [0.357237845659256, 0.6274612545967102, 0.015300876460969448, 0.0, 0.0, 0.0], [0.5917996764183044, 0.2764042019844055, 0.10476048290729523, 0.027035649865865707, 0.0, 0.0], [0.7254403829574585, 0.04983152449131012, 0.014982940629124641, 0.1778142899274826, 0.031930916011333466, 0.0], [0.7612743973731995, 0.06158972904086113, 0.005942251533269882, 0.01642685756087303, 0.1267806589603424, 0.0279861893504858]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9947587847709656, 0.005241230130195618, 0.0, 0.0, 0.0, 0.0], [0.9632415771484375, 0.017816413193941116, 0.018942030146718025, 0.0, 0.0, 0.0], [0.9671078324317932, 0.008509586565196514, 0.00856222677975893, 0.015820473432540894, 0.0, 0.0], [0.9340996146202087, 0.011952387169003487, 0.02018021047115326, 0.02675083465874195, 0.0070168930105865, 0.0], [0.9587237238883972, 0.004657115787267685, 0.003326789475977421, 0.006545313633978367, 0.010182461701333523, 0.016564540565013885]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9769991040229797, 0.023000910878181458, 0.0, 0.0, 0.0, 0.0], [0.7917609214782715, 0.1753319948911667, 0.032907065004110336, 0.0, 0.0, 0.0], [0.7949192523956299, 0.10531841963529587, 0.040218502283096313, 0.05954383686184883, 0.0, 0.0], [0.7097718715667725, 0.10552527755498886, 0.06597573310136795, 0.05765606462955475, 0.061070989817380905, 0.0], [0.7506601214408875, 0.026514461264014244, 0.021576043218374252, 0.034296683967113495, 0.08494450151920319, 0.08200812339782715]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.983751654624939, 0.016248304396867752, 0.0, 0.0, 0.0, 0.0], [0.5615494847297668, 0.08956841379404068, 0.3488820493221283, 0.0, 0.0, 0.0], [0.32929039001464844, 0.024114903062582016, 0.5428059697151184, 0.10378880053758621, 0.0, 0.0], [0.34330207109451294, 0.01308644749224186, 0.5121983289718628, 0.11146228760480881, 0.019950881600379944, 0.0], [0.4792812764644623, 0.01733359508216381, 0.1180536150932312, 0.06130281835794449, 0.20071913301944733, 0.12330964207649231]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9908847212791443, 0.009115329943597317, 0.0, 0.0, 0.0, 0.0], [0.5282707214355469, 0.3292262554168701, 0.1425030380487442, 0.0, 0.0, 0.0], [0.48788541555404663, 0.23368670046329498, 0.17578084766864777, 0.10264702141284943, 0.0, 0.0], [0.31444698572158813, 0.18065163493156433, 0.168714240193367, 0.09506598114967346, 0.24112118780612946, 0.0], [0.5168765187263489, 0.035897161811590195, 0.026188155636191368, 0.04039734974503517, 0.18791745603084564, 0.1927233189344406]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8750308156013489, 0.12496919929981232, 0.0, 0.0, 0.0, 0.0], [0.4550614655017853, 0.4900427758693695, 0.05489582195878029, 0.0, 0.0, 0.0], [0.2933720052242279, 0.5449907183647156, 0.09444297850131989, 0.06719419360160828, 0.0, 0.0], [0.489708811044693, 0.2720997631549835, 0.06861965358257294, 0.14694802463054657, 0.022623788565397263, 0.0], [0.4729066491127014, 0.08103099465370178, 0.016052134335041046, 0.30672287940979004, 0.10120721161365509, 0.022080255672335625]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9630220532417297, 0.03697792813181877, 0.0, 0.0, 0.0, 0.0], [0.7557195425033569, 0.16436372697353363, 0.07991670072078705, 0.0, 0.0, 0.0], [0.6947705745697021, 0.08409853279590607, 0.0638260766863823, 0.15730486810207367, 0.0, 0.0], [0.5821147561073303, 0.03297805413603783, 0.07936596870422363, 0.19441406428813934, 0.11112712323665619, 0.0], [0.5974540710449219, 0.04261096194386482, 0.06919723749160767, 0.14563441276550293, 0.12481734901666641, 0.020285936072468758]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9957822561264038, 0.004217816516757011, 0.0, 0.0, 0.0, 0.0], [0.9312832951545715, 0.010560247115790844, 0.05815650522708893, 0.0, 0.0, 0.0], [0.8435326814651489, 0.015695005655288696, 0.045751139521598816, 0.09502115100622177, 0.0, 0.0], [0.772409975528717, 0.011981245130300522, 0.03504609689116478, 0.03876771405339241, 0.14179500937461853, 0.0], [0.7642908692359924, 0.009868789464235306, 0.00812275055795908, 0.013314393348991871, 0.04824395477771759, 0.15615922212600708]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9701177477836609, 0.02988232672214508, 0.0, 0.0, 0.0, 0.0], [0.6564007997512817, 0.22506150603294373, 0.11853761970996857, 0.0, 0.0, 0.0], [0.6958062648773193, 0.14701850712299347, 0.07145983725786209, 0.08571550250053406, 0.0, 0.0], [0.6353274583816528, 0.1346064656972885, 0.030994214117527008, 0.056916315108537674, 0.1421555131673813, 0.0], [0.6779401898384094, 0.053654152899980545, 0.01800631172955036, 0.06284520775079727, 0.1103820651769638, 0.07717210054397583]]], [[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9822334051132202, 0.017766647040843964, 0.0, 0.0, 0.0, 0.0], [0.9037663340568542, 0.06541544198989868, 0.03081829659640789, 0.0, 0.0, 0.0], [0.8119193911552429, 0.03679030388593674, 0.060560714453458786, 0.09072960168123245, 0.0, 0.0], [0.40546438097953796, 0.10383912175893784, 0.10211236774921417, 0.35434210300445557, 0.03424208238720894, 0.0], [0.22824221849441528, 0.017278727144002914, 0.05055465176701546, 0.6015752553939819, 0.09411764144897461, 0.008231506682932377]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9873148202896118, 0.012685136869549751, 0.0, 0.0, 0.0, 0.0], [0.35445743799209595, 0.5317603349685669, 0.11378221958875656, 0.0, 0.0, 0.0], [0.07823363691568375, 0.7221359014511108, 0.10936623811721802, 0.090264230966568, 0.0, 0.0], [0.21967869997024536, 0.4048435091972351, 0.12358088046312332, 0.20018866658210754, 0.051708199083805084, 0.0], [0.36089760065078735, 0.10459021478891373, 0.06983799487352371, 0.2976483404636383, 0.13869903981685638, 0.02832675166428089]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9732162356376648, 0.0267837755382061, 0.0, 0.0, 0.0, 0.0], [0.9167553782463074, 0.061452705413103104, 0.02179192565381527, 0.0, 0.0, 0.0], [0.8543081283569336, 0.08049600571393967, 0.030334919691085815, 0.03486092761158943, 0.0, 0.0], [0.8919214606285095, 0.04280779883265495, 0.022045055404305458, 0.023470671847462654, 0.01975487545132637, 0.0], [0.8116763234138489, 0.03413533419370651, 0.03567665070295334, 0.04748587682843208, 0.0253971628844738, 0.04562860727310181]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9502761960029602, 0.04972382262349129, 0.0, 0.0, 0.0, 0.0], [0.7637454271316528, 0.2007361352443695, 0.03551840782165527, 0.0, 0.0, 0.0], [0.6279097199440002, 0.03768139332532883, 0.1994536966085434, 0.13495522737503052, 0.0, 0.0], [0.6397060751914978, 0.027007432654500008, 0.09082036465406418, 0.20653828978538513, 0.03592785820364952, 0.0], [0.4559425115585327, 0.021641194820404053, 0.12939567863941193, 0.21800927817821503, 0.10379841923713684, 0.07121295481920242]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9498406648635864, 0.050159383565187454, 0.0, 0.0, 0.0, 0.0], [0.8688724637031555, 0.0872218981385231, 0.043905653059482574, 0.0, 0.0, 0.0], [0.6937950253486633, 0.06359200924634933, 0.091790571808815, 0.15082231163978577, 0.0, 0.0], [0.7266597151756287, 0.04389883577823639, 0.04683985933661461, 0.09851823002099991, 0.08408336341381073, 0.0], [0.7848998308181763, 0.037147827446460724, 0.012907838448882103, 0.01053939200937748, 0.12079165875911713, 0.03371351957321167]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9891054034233093, 0.01089458167552948, 0.0, 0.0, 0.0, 0.0], [0.8929519653320312, 0.08700055629014969, 0.02004752680659294, 0.0, 0.0, 0.0], [0.7891124486923218, 0.09797251224517822, 0.08633202314376831, 0.026582980528473854, 0.0, 0.0], [0.8850635886192322, 0.03645012155175209, 0.05395457148551941, 0.01237727515399456, 0.012154522351920605, 0.0], [0.6861329674720764, 0.05720378831028938, 0.011636304669082165, 0.021660611033439636, 0.1748800277709961, 0.048486363142728806]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9396191835403442, 0.06038080155849457, 0.0, 0.0, 0.0, 0.0], [0.7851794958114624, 0.19751444458961487, 0.017306052148342133, 0.0, 0.0, 0.0], [0.7660509943962097, 0.15444670617580414, 0.03188290074467659, 0.04761936888098717, 0.0, 0.0], [0.703522801399231, 0.05171430483460426, 0.07760990411043167, 0.1533905267715454, 0.013762423768639565, 0.0], [0.7121888399124146, 0.04994234815239906, 0.03772548958659172, 0.08649132400751114, 0.06541401147842407, 0.04823806509375572]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.974072277545929, 0.025927715003490448, 0.0, 0.0, 0.0, 0.0], [0.792539656162262, 0.01171559002250433, 0.19574476778507233, 0.0, 0.0, 0.0], [0.5106770992279053, 0.007296787109225988, 0.039619915187358856, 0.4424062669277191, 0.0, 0.0], [0.5862472057342529, 0.012099712155759335, 0.024585209786891937, 0.06737840175628662, 0.30968940258026123, 0.0], [0.30196306109428406, 0.007724012713879347, 0.011518122628331184, 0.046947259455919266, 0.22146707773208618, 0.41038045287132263]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9744554162025452, 0.02554464340209961, 0.0, 0.0, 0.0, 0.0], [0.9769195318222046, 0.015048524364829063, 0.008031901903450489, 0.0, 0.0, 0.0], [0.9060619473457336, 0.025875424966216087, 0.025954782962799072, 0.04210779070854187, 0.0, 0.0], [0.9400081038475037, 0.00555665697902441, 0.005828304681926966, 0.031757812947034836, 0.016849134117364883, 0.0], [0.9105738401412964, 0.0019752182997763157, 0.008646721951663494, 0.013360846787691116, 0.03543964773416519, 0.030003678053617477]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9791666865348816, 0.020833350718021393, 0.0, 0.0, 0.0, 0.0], [0.8444858193397522, 0.13507869839668274, 0.020435383543372154, 0.0, 0.0, 0.0], [0.7903086543083191, 0.14559169113636017, 0.037529975175857544, 0.026569725945591927, 0.0, 0.0], [0.7298924326896667, 0.056496407836675644, 0.032735615968704224, 0.10400459170341492, 0.07687094807624817, 0.0], [0.5684185028076172, 0.04388832300901413, 0.026293467730283737, 0.0811714455485344, 0.24314835667610168, 0.037079911679029465]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9499868154525757, 0.05001320689916611, 0.0, 0.0, 0.0, 0.0], [0.9336170554161072, 0.05848868936300278, 0.007894262671470642, 0.0, 0.0, 0.0], [0.7897834181785583, 0.11071821302175522, 0.05360178276896477, 0.04589657858014107, 0.0, 0.0], [0.885930061340332, 0.05752986669540405, 0.01374326553195715, 0.0033877466339617968, 0.03940902277827263, 0.0], [0.9337607622146606, 0.02647063508629799, 0.004523396957665682, 0.0061904797330498695, 0.014132906682789326, 0.014921708963811398]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [1.0, 1.8521224554035598e-09, 0.0, 0.0, 0.0, 0.0], [6.6758907451003324e-06, 0.9999804496765137, 1.2841281204600818e-05, 0.0, 0.0, 0.0], [2.2194194926328237e-08, 2.6684581211355862e-09, 0.9999971389770508, 2.8136880700913025e-06, 0.0, 0.0], [1.0145409987671883e-06, 4.464065739284706e-08, 0.00035356366424821317, 0.9993677735328674, 0.0002776293840724975, 0.0], [9.436550429953172e-10, 1.382057315812979e-11, 5.017835036369434e-10, 2.965183876213473e-09, 0.9999971389770508, 2.8644042231462663e-06]]], [[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9948632121086121, 0.005136783700436354, 0.0, 0.0, 0.0, 0.0], [0.9274215698242188, 0.01832387037575245, 0.05425456911325455, 0.0, 0.0, 0.0], [0.9678993225097656, 0.004143435508012772, 0.004314453341066837, 0.023642776533961296, 0.0, 0.0], [0.8999068737030029, 0.001467161695472896, 0.00029133574571460485, 0.002585014794021845, 0.09574954956769943, 0.0], [0.9386115670204163, 0.00022248300956562161, 0.0006146665546111763, 0.0015495637198910117, 0.030689461156725883, 0.028312424197793007]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9999959468841553, 4.042720775032649e-06, 0.0, 0.0, 0.0, 0.0], [0.9982761144638062, 3.2613831990602193e-06, 0.001720669330097735, 0.0, 0.0, 0.0], [0.9998809099197388, 5.328835683826583e-08, 6.376215537784446e-07, 0.00011847059795400128, 0.0, 0.0], [0.9996154308319092, 3.473169556400535e-07, 3.8920820344401363e-08, 4.468433303372876e-07, 0.00038369710091501474, 0.0], [0.9994840621948242, 1.655020476221125e-08, 2.8715557931491276e-08, 1.0638284493325045e-06, 0.0002126671897713095, 0.00030212008277885616]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9514135718345642, 0.048586405813694, 0.0, 0.0, 0.0, 0.0], [0.5749948024749756, 0.39028096199035645, 0.03472418338060379, 0.0, 0.0, 0.0], [0.7442318201065063, 0.1752411425113678, 0.0756477490067482, 0.004879283253103495, 0.0, 0.0], [0.5232070684432983, 0.09429339319467545, 0.1138191670179367, 0.19979268312454224, 0.06888769567012787, 0.0], [0.47472575306892395, 0.05636607110500336, 0.04530389606952667, 0.06967321783304214, 0.3098014295101166, 0.0441296212375164]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8734648823738098, 0.12653514742851257, 0.0, 0.0, 0.0, 0.0], [0.6097912788391113, 0.3541727066040039, 0.036036062985658646, 0.0, 0.0, 0.0], [0.45984190702438354, 0.38697871565818787, 0.0996011346578598, 0.05357823893427849, 0.0, 0.0], [0.572220504283905, 0.23636263608932495, 0.08344558626413345, 0.06921917200088501, 0.03875211998820305, 0.0], [0.5143564343452454, 0.16723087430000305, 0.09019406139850616, 0.0765448659658432, 0.10578085482120514, 0.04589281603693962]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.981228768825531, 0.018771231174468994, 0.0, 0.0, 0.0, 0.0], [0.6142941117286682, 0.3503977954387665, 0.0353081189095974, 0.0, 0.0, 0.0], [0.5770686268806458, 0.32858458161354065, 0.05508256331086159, 0.03926428034901619, 0.0, 0.0], [0.17188192903995514, 0.011042501777410507, 0.054578714072704315, 0.7326585650444031, 0.029838265851140022, 0.0], [0.3783015012741089, 0.017070062458515167, 0.021754134446382523, 0.4409688115119934, 0.06093813106417656, 0.08096737414598465]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9923112392425537, 0.007688735146075487, 0.0, 0.0, 0.0, 0.0], [0.9498787522315979, 0.016709784045815468, 0.03341152146458626, 0.0, 0.0, 0.0], [0.9961295127868652, 0.0008787295082584023, 0.0006868162308819592, 0.0023048371076583862, 0.0, 0.0], [0.9935757517814636, 0.0032634998206049204, 0.0009993825806304812, 0.00027932299417443573, 0.0018820574041455984, 0.0], [0.9907532930374146, 0.00021344318520277739, 0.0004595233185682446, 0.0007905619568191469, 0.004424723796546459, 0.003358350833877921]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9647740125656128, 0.03522596135735512, 0.0, 0.0, 0.0, 0.0], [0.8194130063056946, 0.1365436613559723, 0.04404333233833313, 0.0, 0.0, 0.0], [0.7584245800971985, 0.006878929678350687, 0.20653395354747772, 0.028162529692053795, 0.0, 0.0], [0.5298128128051758, 0.002678812015801668, 0.07857988774776459, 0.3598373234272003, 0.02909109927713871, 0.0], [0.7544413208961487, 0.00036782227107323706, 0.0019713479559868574, 0.00324004958383739, 0.1942344754934311, 0.04574500769376755]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9749131202697754, 0.02508680149912834, 0.0, 0.0, 0.0, 0.0], [0.9306471943855286, 0.05705660209059715, 0.012296222150325775, 0.0, 0.0, 0.0], [0.9305251836776733, 0.052770983427762985, 0.01111945416778326, 0.005584415514022112, 0.0, 0.0], [0.8863320350646973, 0.01292418036609888, 0.017724711447954178, 0.06150198355317116, 0.021517015993595123, 0.0], [0.791684627532959, 0.015036096796393394, 0.0317479707300663, 0.03392200171947479, 0.03707978501915932, 0.09052948653697968]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9608501195907593, 0.039149850606918335, 0.0, 0.0, 0.0, 0.0], [0.9121272563934326, 0.02257651649415493, 0.06529619544744492, 0.0, 0.0, 0.0], [0.9364108443260193, 0.015584447421133518, 0.024544963613152504, 0.02345985174179077, 0.0, 0.0], [0.9454620480537415, 0.006762288510799408, 0.022026237100362778, 0.009137796238064766, 0.016611700877547264, 0.0], [0.8346164226531982, 0.001881699077785015, 0.00560904573649168, 0.01887359470129013, 0.12449200451374054, 0.014527074061334133]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9964227080345154, 0.0035772807896137238, 0.0, 0.0, 0.0, 0.0], [0.9713928699493408, 0.024453025311231613, 0.004154058638960123, 0.0, 0.0, 0.0], [0.9735792279243469, 0.019003381952643394, 0.003664410673081875, 0.0037529165856540203, 0.0, 0.0], [0.9586312174797058, 0.007116180844604969, 0.009218388237059116, 0.022725583985447884, 0.0023084774147719145, 0.0], [0.973607063293457, 0.008490582928061485, 0.0032512471079826355, 0.003606445388868451, 0.004877461586147547, 0.006167212035506964]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.97598797082901, 0.024011990055441856, 0.0, 0.0, 0.0, 0.0], [0.9460638165473938, 0.04211375489830971, 0.011822436936199665, 0.0, 0.0, 0.0], [0.8446813225746155, 0.04293116182088852, 0.05218198522925377, 0.06020559370517731, 0.0, 0.0], [0.9378372430801392, 0.03354858607053757, 0.008826455101370811, 0.0028792242519557476, 0.016908427700400352, 0.0], [0.8124931454658508, 0.02696753479540348, 0.05999218672513962, 0.03445731848478317, 0.011011860333383083, 0.05507794767618179]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9001203775405884, 0.09987961500883102, 0.0, 0.0, 0.0, 0.0], [0.627193033695221, 0.07988718152046204, 0.29291975498199463, 0.0, 0.0, 0.0], [0.7624077796936035, 0.02734432928264141, 0.038679543882608414, 0.17156831920146942, 0.0, 0.0], [0.7995968461036682, 0.014336260966956615, 0.01437566988170147, 0.025438452139496803, 0.14625284075737, 0.0], [0.7851970791816711, 0.04204057529568672, 0.025253651663661003, 0.02908395044505596, 0.029306314885616302, 0.08911846578121185]]], [[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9954467415809631, 0.0045532057993113995, 0.0, 0.0, 0.0, 0.0], [0.9356001615524292, 0.04476744681596756, 0.019632352516055107, 0.0, 0.0, 0.0], [0.5605552792549133, 0.09861977398395538, 0.29983264207839966, 0.040992289781570435, 0.0, 0.0], [0.5893709659576416, 0.11000988632440567, 0.08033622056245804, 0.16754034161567688, 0.05274256691336632, 0.0], [0.22305884957313538, 0.05680817365646362, 0.05467984080314636, 0.24733951687812805, 0.3111244738101959, 0.1069890558719635]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9301451444625854, 0.06985488533973694, 0.0, 0.0, 0.0, 0.0], [0.8936478495597839, 0.08535721153020859, 0.020994966849684715, 0.0, 0.0, 0.0], [0.8404538035392761, 0.10619214922189713, 0.02363673783838749, 0.029717326164245605, 0.0, 0.0], [0.8927386403083801, 0.024784674867987633, 0.008319000713527203, 0.05165454372763634, 0.022503145039081573, 0.0], [0.8646610975265503, 0.009503193199634552, 0.0024329854641109705, 0.04796753078699112, 0.04273205250501633, 0.03270319849252701]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9859625697135925, 0.014037408865988255, 0.0, 0.0, 0.0, 0.0], [0.9702037572860718, 0.0168070700019598, 0.012989125214517117, 0.0, 0.0, 0.0], [0.9524770379066467, 0.016064459457993507, 0.013456220738589764, 0.018002323806285858, 0.0, 0.0], [0.9332928657531738, 0.01897200010716915, 0.02014683373272419, 0.017023753374814987, 0.010564540512859821, 0.0], [0.9113592505455017, 0.012528638355433941, 0.02209620550274849, 0.01751861348748207, 0.018517911434173584, 0.01797938533127308]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9681769013404846, 0.03182310611009598, 0.0, 0.0, 0.0, 0.0], [0.9096417427062988, 0.07916690409183502, 0.011191264726221561, 0.0, 0.0, 0.0], [0.8379932045936584, 0.13078266382217407, 0.012140989303588867, 0.019083037972450256, 0.0, 0.0], [0.9116525053977966, 0.05451957508921623, 0.009499342180788517, 0.00746585289016366, 0.01686275750398636, 0.0], [0.8510289192199707, 0.07338211685419083, 0.008022507652640343, 0.009083161130547523, 0.04261006414890289, 0.015873271971940994]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9799023866653442, 0.020097682252526283, 0.0, 0.0, 0.0, 0.0], [0.9558742642402649, 0.029063312336802483, 0.015062497928738594, 0.0, 0.0, 0.0], [0.7943133115768433, 0.06074100360274315, 0.06907659024000168, 0.07586916536092758, 0.0, 0.0], [0.5494324564933777, 0.03154711425304413, 0.05482015758752823, 0.05788077041506767, 0.3063195049762726, 0.0], [0.6453980803489685, 0.010770943015813828, 0.017528092488646507, 0.02157985046505928, 0.24958276748657227, 0.05514020845293999]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9506809115409851, 0.0493190623819828, 0.0, 0.0, 0.0, 0.0], [0.8553215265274048, 0.09256264567375183, 0.05211575701832771, 0.0, 0.0, 0.0], [0.850852370262146, 0.04734604433178902, 0.044177331030368805, 0.057624250650405884, 0.0, 0.0], [0.7697131633758545, 0.02788589708507061, 0.031017286702990532, 0.06842502951622009, 0.1029587835073471, 0.0], [0.7931903004646301, 0.04052198305726051, 0.029242033138871193, 0.04478124529123306, 0.04894689470529556, 0.04331749677658081]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9770310521125793, 0.02296893112361431, 0.0, 0.0, 0.0, 0.0], [0.9429817199707031, 0.017321482300758362, 0.03969680890440941, 0.0, 0.0, 0.0], [0.9144344925880432, 0.008583576418459415, 0.013035810552537441, 0.06394599378108978, 0.0, 0.0], [0.9222429990768433, 0.0036440351977944374, 0.003740275977179408, 0.010410364717245102, 0.05996239185333252, 0.0], [0.9198879599571228, 0.0030822583939880133, 0.0034827394410967827, 0.004206796642392874, 0.02125428058207035, 0.048085976392030716]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.977458119392395, 0.022541873157024384, 0.0, 0.0, 0.0, 0.0], [0.8929325342178345, 0.07475466281175613, 0.032312843948602676, 0.0, 0.0, 0.0], [0.8423511385917664, 0.05980278551578522, 0.03740081936120987, 0.06044524535536766, 0.0, 0.0], [0.7674624919891357, 0.03536349534988403, 0.042155250906944275, 0.06658654659986496, 0.08843226730823517, 0.0], [0.6182611584663391, 0.01611059531569481, 0.020167622715234756, 0.03868892416357994, 0.23147016763687134, 0.07530155777931213]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9634856581687927, 0.036514393985271454, 0.0, 0.0, 0.0, 0.0], [0.4363938570022583, 0.522637128829956, 0.04096902906894684, 0.0, 0.0, 0.0], [0.3608614206314087, 0.35129693150520325, 0.2655103802680969, 0.022331148386001587, 0.0, 0.0], [0.3942921757698059, 0.021704670041799545, 0.07794328778982162, 0.37168896198272705, 0.1343708038330078, 0.0], [0.6310713887214661, 0.01698400266468525, 0.025942081585526466, 0.08615949749946594, 0.2183200567960739, 0.021522950381040573]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9988250136375427, 0.0011750265257433057, 0.0, 0.0, 0.0, 0.0], [0.9944871068000793, 0.0004826401418540627, 0.0050302306190133095, 0.0, 0.0, 0.0], [0.9981209635734558, 2.705173392314464e-05, 0.0001130745149566792, 0.0017389442073181272, 0.0, 0.0], [0.9982239603996277, 6.83655816828832e-05, 0.00010199935059063137, 6.028370262356475e-05, 0.0015453165397047997, 0.0], [0.9982888102531433, 1.055222810464329e-06, 3.2781026675365865e-05, 0.00013038977340329438, 0.0006605894886888564, 0.0008863684488460422]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9936710596084595, 0.006328921765089035, 0.0, 0.0, 0.0, 0.0], [0.9727688431739807, 0.0018561368342489004, 0.025375060737133026, 0.0, 0.0, 0.0], [0.9724299907684326, 0.0019586149137467146, 0.011192461475729942, 0.014418890699744225, 0.0, 0.0], [0.9782041311264038, 0.0009589138207957149, 0.0018706483533605933, 0.006326568778604269, 0.012639678083360195, 0.0], [0.9592596888542175, 0.0024555064737796783, 0.00161241355817765, 0.005019655916839838, 0.006687097251415253, 0.024965662509202957]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9629000425338745, 0.03709998354315758, 0.0, 0.0, 0.0, 0.0], [0.36801934242248535, 0.6152258515357971, 0.016754813492298126, 0.0, 0.0, 0.0], [0.3173511326313019, 0.6140013337135315, 0.05375149846076965, 0.014896026812493801, 0.0, 0.0], [0.48987284302711487, 0.21071474254131317, 0.04693019017577171, 0.20700432360172272, 0.04547784850001335, 0.0], [0.48774227499961853, 0.1769528090953827, 0.06915216147899628, 0.09849268198013306, 0.12091436982154846, 0.046745721250772476]]], [[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9794419407844543, 0.020558049902319908, 0.0, 0.0, 0.0, 0.0], [0.6677903532981873, 0.31032365560531616, 0.021886007860302925, 0.0, 0.0, 0.0], [0.7118757367134094, 0.11108540743589401, 0.14187385141849518, 0.03516504913568497, 0.0, 0.0], [0.4501457214355469, 0.04036055505275726, 0.040458209812641144, 0.388570100069046, 0.08046531677246094, 0.0], [0.49346262216567993, 0.013696977868676186, 0.008126799948513508, 0.13074499368667603, 0.3086138069629669, 0.04535480588674545]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9846054315567017, 0.015394587069749832, 0.0, 0.0, 0.0, 0.0], [0.9806739091873169, 0.007713791914284229, 0.011612347327172756, 0.0, 0.0, 0.0], [0.932663083076477, 0.01957838423550129, 0.02410353161394596, 0.023654978722333908, 0.0, 0.0], [0.9422016739845276, 0.0009538981830701232, 0.0010898025939241052, 0.00319337984547019, 0.05256118252873421, 0.0], [0.9352930784225464, 0.0010279357666149735, 0.004444425459951162, 0.001637140172533691, 0.010590963996946812, 0.04700646549463272]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9985783100128174, 0.0014216724084690213, 0.0, 0.0, 0.0, 0.0], [0.9893348813056946, 0.0011178902350366116, 0.00954714696854353, 0.0, 0.0, 0.0], [0.9979978203773499, 7.997050124686211e-05, 0.00013218850654084235, 0.0017900333041325212, 0.0, 0.0], [0.9986976385116577, 4.1044117097044364e-05, 3.8683547245454974e-06, 2.3676282580709085e-05, 0.0012337174266576767, 0.0], [0.9971563816070557, 1.852225250331685e-05, 1.8826559653462027e-06, 2.7900125132873654e-05, 0.0006533482228405774, 0.0021419788245111704]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9768233299255371, 0.023176640272140503, 0.0, 0.0, 0.0, 0.0], [0.9194678068161011, 0.05088186264038086, 0.029650341719388962, 0.0, 0.0, 0.0], [0.8474554419517517, 0.06100169196724892, 0.04372376948595047, 0.04781914874911308, 0.0, 0.0], [0.8011623620986938, 0.041866958141326904, 0.04375807195901871, 0.041894737631082535, 0.07131782174110413, 0.0], [0.8031871914863586, 0.02450493723154068, 0.017323585227131844, 0.04744395986199379, 0.06109930947422981, 0.046441152691841125]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9829428195953369, 0.01705716922879219, 0.0, 0.0, 0.0, 0.0], [0.8863736987113953, 0.09492647647857666, 0.018699750304222107, 0.0, 0.0, 0.0], [0.9231085777282715, 0.03696346655488014, 0.032198335975408554, 0.007729663979262114, 0.0, 0.0], [0.9068527221679688, 0.016046639531850815, 0.014310522936284542, 0.04543786868453026, 0.017352323979139328, 0.0], [0.6555973887443542, 0.05091019719839096, 0.028384855017066002, 0.1256549060344696, 0.10546853393316269, 0.03398407623171806]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9502318501472473, 0.049768079072237015, 0.0, 0.0, 0.0, 0.0], [0.8829865455627441, 0.1000962108373642, 0.01691717840731144, 0.0, 0.0, 0.0], [0.8057457804679871, 0.14463546872138977, 0.03018922731280327, 0.019429458305239677, 0.0, 0.0], [0.8706230521202087, 0.032440632581710815, 0.026951627805829048, 0.04410304129123688, 0.025881657376885414, 0.0], [0.688364565372467, 0.009681451134383678, 0.016449343413114548, 0.0987110361456871, 0.08971209079027176, 0.09708156436681747]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9792683124542236, 0.02073168195784092, 0.0, 0.0, 0.0, 0.0], [0.9523284435272217, 0.025933818891644478, 0.021737735718488693, 0.0, 0.0, 0.0], [0.9144353270530701, 0.017671240493655205, 0.022358495742082596, 0.04553484544157982, 0.0, 0.0], [0.9448292851448059, 0.006467597559094429, 0.006386063527315855, 0.03263096138834953, 0.00968620739877224, 0.0], [0.9347906112670898, 0.007862505502998829, 0.007788175716996193, 0.021432818844914436, 0.008491144515573978, 0.01963483914732933]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.983370304107666, 0.016629677265882492, 0.0, 0.0, 0.0, 0.0], [0.963111400604248, 0.009229931980371475, 0.027658598497509956, 0.0, 0.0, 0.0], [0.9706628322601318, 0.0041494048200547695, 0.0068131014704704285, 0.018374638631939888, 0.0, 0.0], [0.987951934337616, 0.002165885642170906, 0.00034901127219200134, 0.001583816367201507, 0.00794942770153284, 0.0], [0.9457950592041016, 0.014583553187549114, 0.0003652951563708484, 0.0009569536778144538, 0.013621564954519272, 0.02467755414545536]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9878059029579163, 0.01219407469034195, 0.0, 0.0, 0.0, 0.0], [0.87103670835495, 0.09448163211345673, 0.03448161482810974, 0.0, 0.0, 0.0], [0.6309783458709717, 0.11090382188558578, 0.1923021823167801, 0.06581564992666245, 0.0, 0.0], [0.5360490083694458, 0.04618944972753525, 0.13605308532714844, 0.26455509662628174, 0.017153292894363403, 0.0], [0.8287520408630371, 0.023732755333185196, 0.02008037269115448, 0.07245264202356339, 0.030431220307946205, 0.024550989270210266]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8995685577392578, 0.10043150931596756, 0.0, 0.0, 0.0, 0.0], [0.270343542098999, 0.6504329442977905, 0.07922357320785522, 0.0, 0.0, 0.0], [0.20541730523109436, 0.5892508625984192, 0.18085837364196777, 0.024473490193486214, 0.0, 0.0], [0.5573861002922058, 0.1774134784936905, 0.08806808292865753, 0.09881848096847534, 0.07831384986639023, 0.0], [0.5922912359237671, 0.08700639009475708, 0.05643285810947418, 0.05685883015394211, 0.12181518226861954, 0.08559554070234299]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9316380620002747, 0.06836195290088654, 0.0, 0.0, 0.0, 0.0], [0.9572945833206177, 0.026243582367897034, 0.0164618119597435, 0.0, 0.0, 0.0], [0.9880544543266296, 0.00427332753315568, 0.002954584313556552, 0.004717645235359669, 0.0, 0.0], [0.99403977394104, 0.0009413420339114964, 0.0004739820142276585, 0.00011646930943243206, 0.004428447224199772, 0.0], [0.9806035161018372, 2.5468933017691597e-05, 0.00016239412070717663, 0.0001476418401580304, 0.0013442443450912833, 0.017716845497488976]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.993178129196167, 0.006821857299655676, 0.0, 0.0, 0.0, 0.0], [0.9756524562835693, 0.01318411435931921, 0.011163423769176006, 0.0, 0.0, 0.0], [0.9418966770172119, 0.004721744451671839, 0.0023818055633455515, 0.050999753177165985, 0.0, 0.0], [0.9905040860176086, 0.0022848136723041534, 6.198462506290525e-05, 0.0005984465242363513, 0.006550676189363003, 0.0], [0.9697660207748413, 0.0008878845837898552, 0.00023466735729016364, 0.0017040816601365805, 0.004128355998545885, 0.02327893301844597]]], [[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9716231822967529, 0.02837684564292431, 0.0, 0.0, 0.0, 0.0], [0.9223619699478149, 0.028907248750329018, 0.048730745911598206, 0.0, 0.0, 0.0], [0.8426317572593689, 0.023872116580605507, 0.04748132824897766, 0.08601479232311249, 0.0, 0.0], [0.8521121740341187, 0.020744236186146736, 0.04494619369506836, 0.05765002593398094, 0.02454746514558792, 0.0], [0.8800725936889648, 0.022448532283306122, 0.018235722556710243, 0.01925482600927353, 0.015854258090257645, 0.044134121388196945]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9412723779678345, 0.058727629482746124, 0.0, 0.0, 0.0, 0.0], [0.916313886642456, 0.05759201943874359, 0.02609400637447834, 0.0, 0.0, 0.0], [0.8392423391342163, 0.057690516114234924, 0.01382902916520834, 0.08923812955617905, 0.0, 0.0], [0.8987162113189697, 0.0134778693318367, 0.0003456450067460537, 0.003298751311376691, 0.08416149020195007, 0.0], [0.8701692223548889, 0.002700856188312173, 0.00143499206751585, 0.0056661744602024555, 0.08874300867319107, 0.031285665929317474]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9656725525856018, 0.03432750701904297, 0.0, 0.0, 0.0, 0.0], [0.9178615808486938, 0.062257930636405945, 0.019880469888448715, 0.0, 0.0, 0.0], [0.823314905166626, 0.06282395124435425, 0.03670429438352585, 0.07715693861246109, 0.0, 0.0], [0.8501748442649841, 0.03816927224397659, 0.03196492791175842, 0.0516013503074646, 0.02808968350291252, 0.0], [0.6572404503822327, 0.05877397954463959, 0.04336007311940193, 0.09013211727142334, 0.08146599680185318, 0.06902744621038437]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9162061810493469, 0.0837937667965889, 0.0, 0.0, 0.0, 0.0], [0.9451773762702942, 0.04099284112453461, 0.013829832896590233, 0.0, 0.0, 0.0], [0.8928355574607849, 0.05368670076131821, 0.017596954479813576, 0.03588071092963219, 0.0, 0.0], [0.8337052464485168, 0.04799601063132286, 0.033513229340314865, 0.04680858924984932, 0.03797686845064163, 0.0], [0.8167192339897156, 0.06337132304906845, 0.013286277651786804, 0.020469767972826958, 0.025292355567216873, 0.06086111441254616]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9525133371353149, 0.04748663306236267, 0.0, 0.0, 0.0, 0.0], [0.3019869327545166, 0.6520938873291016, 0.04591925069689751, 0.0, 0.0, 0.0], [0.285582959651947, 0.556952178478241, 0.1444743126630783, 0.012990524061024189, 0.0, 0.0], [0.843804121017456, 0.032251205295324326, 0.03954290598630905, 0.06848159432411194, 0.015920041128993034, 0.0], [0.6664940714836121, 0.06095913052558899, 0.04064354673027992, 0.06804485619068146, 0.09186329692602158, 0.07199501991271973]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9682655334472656, 0.031734466552734375, 0.0, 0.0, 0.0, 0.0], [0.738521933555603, 0.22856839001178741, 0.032909639179706573, 0.0, 0.0, 0.0], [0.5946676135063171, 0.2303314357995987, 0.14867636561393738, 0.02632458508014679, 0.0, 0.0], [0.6339254975318909, 0.05813034623861313, 0.09654320776462555, 0.14291946589946747, 0.06848153471946716, 0.0], [0.40375572443008423, 0.08945391327142715, 0.07635112851858139, 0.25587135553359985, 0.1433039754629135, 0.03126389905810356]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9869793653488159, 0.013020593672990799, 0.0, 0.0, 0.0, 0.0], [0.8631385564804077, 0.1105666309595108, 0.02629482001066208, 0.0, 0.0, 0.0], [0.9488080143928528, 0.028614996001124382, 0.006535546388477087, 0.016041526570916176, 0.0, 0.0], [0.9672170877456665, 0.006604980677366257, 0.00045171406236477196, 0.004844417329877615, 0.020881708711385727, 0.0], [0.9354621171951294, 0.02047806605696678, 0.0011700231116265059, 0.007056943140923977, 0.0163181871175766, 0.019514625892043114]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9846673011779785, 0.015332723967730999, 0.0, 0.0, 0.0, 0.0], [0.9052747488021851, 0.08373606950044632, 0.010989243164658546, 0.0, 0.0, 0.0], [0.8145939111709595, 0.04283742979168892, 0.10568301379680634, 0.03688570484519005, 0.0, 0.0], [0.23519809544086456, 0.012018457986414433, 0.05280117318034172, 0.6516180038452148, 0.04836418479681015, 0.0], [0.31818512082099915, 0.018632443621754646, 0.03948190063238144, 0.3755541741847992, 0.20787373185157776, 0.04027257487177849]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9811733365058899, 0.018826685845851898, 0.0, 0.0, 0.0, 0.0], [0.8618939518928528, 0.06479164958000183, 0.07331438362598419, 0.0, 0.0, 0.0], [0.7664540410041809, 0.07330425828695297, 0.10353513062000275, 0.056706514209508896, 0.0, 0.0], [0.8128499984741211, 0.03215480223298073, 0.059005625545978546, 0.05416511744260788, 0.04182446748018265, 0.0], [0.8687856197357178, 0.026987861841917038, 0.02047000452876091, 0.01629738137125969, 0.03218390792608261, 0.03527523949742317]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9264583587646484, 0.07354167848825455, 0.0, 0.0, 0.0, 0.0], [0.8403540849685669, 0.06373751163482666, 0.09590838104486465, 0.0, 0.0, 0.0], [0.7330995798110962, 0.06451118737459183, 0.10380073636770248, 0.09858842939138412, 0.0, 0.0], [0.9143612384796143, 0.008257776498794556, 0.007320381235331297, 0.017966248095035553, 0.05209439620375633, 0.0], [0.8971915245056152, 0.008555498905479908, 0.007019453682005405, 0.014860544353723526, 0.03399762138724327, 0.03837529569864273]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9180347919464111, 0.08196526020765305, 0.0, 0.0, 0.0, 0.0], [0.8328666687011719, 0.1219901517033577, 0.04514322429895401, 0.0, 0.0, 0.0], [0.7994157075881958, 0.0874413549900055, 0.03605784848332405, 0.07708510011434555, 0.0, 0.0], [0.880984902381897, 0.020749641582369804, 0.020554615184664726, 0.017120830714702606, 0.06058995798230171, 0.0], [0.745303213596344, 0.044334057718515396, 0.022549288347363472, 0.0331527441740036, 0.03357058763504028, 0.12109009176492691]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9867060780525208, 0.013293893076479435, 0.0, 0.0, 0.0, 0.0], [0.982177734375, 0.012414131313562393, 0.005408108700066805, 0.0, 0.0, 0.0], [0.9630486369132996, 0.015290752984583378, 0.010345698334276676, 0.0113149369135499, 0.0, 0.0], [0.9213568568229675, 0.014132463373243809, 0.017639216035604477, 0.016567690297961235, 0.030303770676255226, 0.0], [0.9373326301574707, 0.009064299054443836, 0.007548365276306868, 0.006576443091034889, 0.011827622540295124, 0.027650514617562294]]], [[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9951004385948181, 0.00489962799474597, 0.0, 0.0, 0.0, 0.0], [0.9476007223129272, 0.041407931596040726, 0.010991275310516357, 0.0, 0.0, 0.0], [0.9142175316810608, 0.023523783311247826, 0.039145033806562424, 0.023113621398806572, 0.0, 0.0], [0.9534738659858704, 0.008932933211326599, 0.015272765420377254, 0.007908251136541367, 0.014412266202270985, 0.0], [0.9427101016044617, 0.00823307130485773, 0.004650997929275036, 0.004178107250481844, 0.005463531706482172, 0.03476419299840927]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9543376564979553, 0.045662373304367065, 0.0, 0.0, 0.0, 0.0], [0.9696040749549866, 0.01954760029911995, 0.01084828469902277, 0.0, 0.0, 0.0], [0.9710449576377869, 0.012425386346876621, 0.008068876340985298, 0.008460716344416142, 0.0, 0.0], [0.9726192951202393, 0.002697656163945794, 0.00044831327977590263, 0.0013814778067171574, 0.022853154689073563, 0.0], [0.9675466418266296, 0.009613442234694958, 0.003203035332262516, 0.00424883933737874, 0.007442260626703501, 0.00794589426368475]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9887008666992188, 0.011299116536974907, 0.0, 0.0, 0.0, 0.0], [0.9382632374763489, 0.04204244911670685, 0.019694412127137184, 0.0, 0.0, 0.0], [0.8351995944976807, 0.03487853705883026, 0.05134471505880356, 0.07857715338468552, 0.0, 0.0], [0.9042676687240601, 0.010541575029492378, 0.016426723450422287, 0.025921987369656563, 0.04284200444817543, 0.0], [0.8913140892982483, 0.00891267228871584, 0.005010711494833231, 0.008175632916390896, 0.013514749705791473, 0.07307209819555283]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8693912029266357, 0.13060881197452545, 0.0, 0.0, 0.0, 0.0], [0.3507988452911377, 0.606351912021637, 0.04284917935729027, 0.0, 0.0, 0.0], [0.35475659370422363, 0.3502019941806793, 0.24722407758235931, 0.04781729355454445, 0.0, 0.0], [0.35370609164237976, 0.03527737781405449, 0.09567111730575562, 0.449796199798584, 0.06554921716451645, 0.0], [0.4132595360279083, 0.09055527299642563, 0.05286579951643944, 0.174679696559906, 0.173848956823349, 0.09479076415300369]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9629756212234497, 0.037024300545454025, 0.0, 0.0, 0.0, 0.0], [0.9756426811218262, 0.01965854875743389, 0.004698706325143576, 0.0, 0.0, 0.0], [0.9775736927986145, 0.013286248780786991, 0.0025590297300368547, 0.006581062916666269, 0.0, 0.0], [0.9870142936706543, 0.007388236932456493, 0.0009579154429957271, 0.0018318220973014832, 0.0028077505994588137, 0.0], [0.9409245848655701, 0.016633737832307816, 0.0022979143541306257, 0.0058906711637973785, 0.0055129327811300755, 0.02874022163450718]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.962827205657959, 0.037172831594944, 0.0, 0.0, 0.0, 0.0], [0.9582237601280212, 0.024641817435622215, 0.017134377732872963, 0.0, 0.0, 0.0], [0.9351300001144409, 0.015331573784351349, 0.014810982160270214, 0.034727465361356735, 0.0, 0.0], [0.9225171208381653, 0.010528750717639923, 0.011010154150426388, 0.01944003626704216, 0.036503832787275314, 0.0], [0.8420165777206421, 0.04357199743390083, 0.007488282397389412, 0.01496153138577938, 0.02385285682976246, 0.06810864061117172]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9926387071609497, 0.00736132962629199, 0.0, 0.0, 0.0, 0.0], [0.9957393407821655, 0.0033469819463789463, 0.000913690309971571, 0.0, 0.0, 0.0], [0.9869900345802307, 0.001974786864593625, 0.001524551771581173, 0.009510699659585953, 0.0, 0.0], [0.9933527708053589, 0.001020324882119894, 0.00034337223041802645, 0.0010291127255186439, 0.004254369530826807, 0.0], [0.9749016761779785, 0.00043480272870510817, 0.0004306558985263109, 0.0012364407302811742, 0.0015347707085311413, 0.021461669355630875]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9897475242614746, 0.010252462700009346, 0.0, 0.0, 0.0, 0.0], [0.9790639281272888, 0.01650906540453434, 0.0044270907528698444, 0.0, 0.0, 0.0], [0.9521436095237732, 0.029432358220219612, 0.008943161927163601, 0.009480923414230347, 0.0, 0.0], [0.939594030380249, 0.021510960534214973, 0.010278552770614624, 0.004555229097604752, 0.024061163887381554, 0.0], [0.9205074906349182, 0.016153652220964432, 0.010818594135344028, 0.01664440892636776, 0.014566398225724697, 0.021309375762939453]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9898501634597778, 0.010149780660867691, 0.0, 0.0, 0.0, 0.0], [0.9820910096168518, 0.006907520350068808, 0.011001535691320896, 0.0, 0.0, 0.0], [0.9684997200965881, 0.008987602777779102, 0.015342563390731812, 0.007170087192207575, 0.0, 0.0], [0.9274120330810547, 0.009485266171395779, 0.022066107019782066, 0.03222890570759773, 0.008807653561234474, 0.0], [0.900665819644928, 0.021623756736516953, 0.013808279298245907, 0.009843860752880573, 0.008521373383700848, 0.04553695768117905]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9954444169998169, 0.004555588588118553, 0.0, 0.0, 0.0, 0.0], [0.995254397392273, 0.002460238989442587, 0.002285485854372382, 0.0, 0.0, 0.0], [0.9862446188926697, 0.0015168144600465894, 0.004072288051247597, 0.008166354149580002, 0.0, 0.0], [0.9889963865280151, 0.001226040069013834, 0.0007996349013410509, 0.0006774227367714047, 0.008300574496388435, 0.0], [0.9865202903747559, 0.00039427157025784254, 0.0009571771952323616, 0.0004954367759637535, 0.0009604979422874749, 0.010672281496226788]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9821295142173767, 0.017870500683784485, 0.0, 0.0, 0.0, 0.0], [0.7489436268806458, 0.22002726793289185, 0.031029189005494118, 0.0, 0.0, 0.0], [0.28547799587249756, 0.21125678718090057, 0.47871601581573486, 0.024549242109060287, 0.0, 0.0], [0.8056644201278687, 0.026974644511938095, 0.04302806034684181, 0.06993705034255981, 0.05439583212137222, 0.0], [0.3307209014892578, 0.022326624020934105, 0.016627125442028046, 0.08019453287124634, 0.41574832797050476, 0.13438253104686737]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9697746634483337, 0.030225319787859917, 0.0, 0.0, 0.0, 0.0], [0.9800565838813782, 0.015018894337117672, 0.004924521781504154, 0.0, 0.0, 0.0], [0.9237861037254333, 0.052764780819416046, 0.00630240747705102, 0.017146753147244453, 0.0, 0.0], [0.9451844096183777, 0.03618047758936882, 0.001989208161830902, 0.003958724904805422, 0.012687299400568008, 0.0], [0.9633325934410095, 0.018662991002202034, 0.0030418417882174253, 0.007070912979543209, 0.0050094155594706535, 0.002882065251469612]]], [[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9873244762420654, 0.012675459496676922, 0.0, 0.0, 0.0, 0.0], [0.9904569983482361, 0.0055419523268938065, 0.004001122899353504, 0.0, 0.0, 0.0], [0.9814971685409546, 0.004653455223888159, 0.003725277027115226, 0.010124054737389088, 0.0, 0.0], [0.9744365811347961, 0.004632251337170601, 0.002379992976784706, 0.006518087349832058, 0.012033028528094292, 0.0], [0.9624497294425964, 0.0033743639942258596, 0.0013198587112128735, 0.0017275003483518958, 0.002944675739854574, 0.028183799237012863]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9807674288749695, 0.01923258602619171, 0.0, 0.0, 0.0, 0.0], [0.9664245843887329, 0.015413926914334297, 0.018161438405513763, 0.0, 0.0, 0.0], [0.9632682204246521, 0.004538117907941341, 0.002925391308963299, 0.029268190264701843, 0.0, 0.0], [0.9562349319458008, 0.0012223608791828156, 0.0005304080550558865, 0.00867149606347084, 0.03334089741110802, 0.0], [0.9657101035118103, 0.0009808284230530262, 0.0016686266753822565, 0.002634831238538027, 0.005866361316293478, 0.023139292374253273]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9639716148376465, 0.036028459668159485, 0.0, 0.0, 0.0, 0.0], [0.9562800526618958, 0.03373315557837486, 0.009986846707761288, 0.0, 0.0, 0.0], [0.8539998531341553, 0.08073022216558456, 0.03334445133805275, 0.031925540417432785, 0.0, 0.0], [0.9547491073608398, 0.009605025872588158, 0.004146162886172533, 0.0020133228972554207, 0.029486361891031265, 0.0], [0.9331137537956238, 0.028699662536382675, 0.005477475933730602, 0.006368075497448444, 0.012613046914339066, 0.013728085905313492]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9392993450164795, 0.06070063263177872, 0.0, 0.0, 0.0, 0.0], [0.9298391342163086, 0.061895377933979034, 0.008265496231615543, 0.0, 0.0, 0.0], [0.8471823334693909, 0.09035038203001022, 0.01763608679175377, 0.044831156730651855, 0.0, 0.0], [0.8857703804969788, 0.03918175399303436, 0.007867704145610332, 0.02276589721441269, 0.04441439360380173, 0.0], [0.8563280701637268, 0.10088995099067688, 0.006531452294439077, 0.008485927246510983, 0.007368441205471754, 0.020396249368786812]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8353264331817627, 0.1646735519170761, 0.0, 0.0, 0.0, 0.0], [0.6160858869552612, 0.3137648403644562, 0.07014927268028259, 0.0, 0.0, 0.0], [0.34316325187683105, 0.2758493721485138, 0.1196604073047638, 0.26132699847221375, 0.0, 0.0], [0.5908172130584717, 0.050290752202272415, 0.041665926575660706, 0.2199493646621704, 0.0972767099738121, 0.0], [0.8481413125991821, 0.06318090111017227, 0.014733693562448025, 0.055267371237277985, 0.00901501253247261, 0.009661628864705563]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9627319574356079, 0.03726799786090851, 0.0, 0.0, 0.0, 0.0], [0.7757522463798523, 0.1799626499414444, 0.044285036623477936, 0.0, 0.0, 0.0], [0.6317060589790344, 0.24380716681480408, 0.10925652086734772, 0.015230235643684864, 0.0, 0.0], [0.9539909958839417, 0.018182311207056046, 0.011601822450757027, 0.012299076654016972, 0.003925766795873642, 0.0], [0.40356943011283875, 0.14237558841705322, 0.05661217123270035, 0.1975736767053604, 0.0929921343922615, 0.10687707364559174]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9802619218826294, 0.019738124683499336, 0.0, 0.0, 0.0, 0.0], [0.9873908162117004, 0.007800452411174774, 0.004808681085705757, 0.0, 0.0, 0.0], [0.9283918738365173, 0.008301235735416412, 0.01330565195530653, 0.05000120773911476, 0.0, 0.0], [0.8981055021286011, 0.015591299161314964, 0.010177576914429665, 0.039987027645111084, 0.0361386202275753, 0.0], [0.9753499031066895, 0.00035433052107691765, 0.0005866039427928627, 0.0011877501383423805, 0.0010750899091362953, 0.021446440368890762]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9295330047607422, 0.07046692818403244, 0.0, 0.0, 0.0, 0.0], [0.9361506104469299, 0.04116682708263397, 0.022682538256049156, 0.0, 0.0, 0.0], [0.8486821055412292, 0.05802798643708229, 0.024856165051460266, 0.0684337466955185, 0.0, 0.0], [0.8661180734634399, 0.02232467755675316, 0.010369130410254002, 0.02600197121500969, 0.07518619298934937, 0.0], [0.8074421882629395, 0.044382549822330475, 0.01849711686372757, 0.03357789292931557, 0.018561245873570442, 0.07753907144069672]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9680535197257996, 0.03194643557071686, 0.0, 0.0, 0.0, 0.0], [0.9693689942359924, 0.02568492479622364, 0.004946070723235607, 0.0, 0.0, 0.0], [0.9620568156242371, 0.022552406415343285, 0.005471326876431704, 0.009919456206262112, 0.0, 0.0], [0.9727528095245361, 0.010137127712368965, 0.000757327419705689, 0.0028828983195126057, 0.013469807803630829, 0.0], [0.9624635577201843, 0.0031109037809073925, 0.0010007602395489812, 0.0019475930603221059, 0.008266227319836617, 0.02321087196469307]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8542501330375671, 0.14574992656707764, 0.0, 0.0, 0.0, 0.0], [0.9725967645645142, 0.014116315171122551, 0.01328685600310564, 0.0, 0.0, 0.0], [0.9257621765136719, 0.03257262706756592, 0.01461210660636425, 0.027053095400333405, 0.0, 0.0], [0.7923423051834106, 0.027305101975798607, 0.01880674995481968, 0.13854165375232697, 0.023004096001386642, 0.0], [0.6152060627937317, 0.02665526419878006, 0.029352931305766106, 0.05590886250138283, 0.11611279845237732, 0.15676409006118774]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9804654121398926, 0.019534552469849586, 0.0, 0.0, 0.0, 0.0], [0.9882452487945557, 0.007509466726332903, 0.004245325922966003, 0.0, 0.0, 0.0], [0.9584206938743591, 0.0109635591506958, 0.010456060990691185, 0.020159708335995674, 0.0, 0.0], [0.9604811668395996, 0.007182627450674772, 0.003072339342907071, 0.006898913532495499, 0.02236509881913662, 0.0], [0.966888964176178, 0.0032812939025461674, 0.00550054432824254, 0.004234083462506533, 0.005038043484091759, 0.015057181939482689]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9498194456100464, 0.05018055811524391, 0.0, 0.0, 0.0, 0.0], [0.9781363606452942, 0.016430046409368515, 0.0054335566237568855, 0.0, 0.0, 0.0], [0.8618696331977844, 0.036093585193157196, 0.07555554062128067, 0.026481209322810173, 0.0, 0.0], [0.5449837446212769, 0.015411133877933025, 0.023516526445746422, 0.25743600726127625, 0.15865260362625122, 0.0], [0.9571874737739563, 0.0030803855042904615, 0.0014446862041950226, 0.006861559115350246, 0.014818714000284672, 0.01660723052918911]]], [[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6156560778617859, 0.3843439519405365, 0.0, 0.0, 0.0, 0.0], [0.36760634183883667, 0.42816370725631714, 0.20423001050949097, 0.0, 0.0, 0.0], [0.16471554338932037, 0.4136792719364166, 0.2509237229824066, 0.17068152129650116, 0.0, 0.0], [0.4184456169605255, 0.1524762362241745, 0.10305401682853699, 0.11071498692035675, 0.21530911326408386, 0.0], [0.19686934351921082, 0.2014620453119278, 0.12827259302139282, 0.09203246980905533, 0.09167550504207611, 0.2896881103515625]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9027364253997803, 0.09726352989673615, 0.0, 0.0, 0.0, 0.0], [0.9736634492874146, 0.014004302211105824, 0.01233230996876955, 0.0, 0.0, 0.0], [0.8504456281661987, 0.05690572410821915, 0.032060906291007996, 0.06058764085173607, 0.0, 0.0], [0.7661210298538208, 0.03530392050743103, 0.03433045372366905, 0.09675204753875732, 0.06749245524406433, 0.0], [0.8650374412536621, 0.020085260272026062, 0.01149806659668684, 0.01855834573507309, 0.018430285155773163, 0.06639053672552109]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9653082489967346, 0.03469168767333031, 0.0, 0.0, 0.0, 0.0], [0.9816323518753052, 0.014176066033542156, 0.004191514104604721, 0.0, 0.0, 0.0], [0.9275256395339966, 0.04737218841910362, 0.01152826938778162, 0.013573966920375824, 0.0, 0.0], [0.9293117523193359, 0.025833239778876305, 0.007227106485515833, 0.014300585724413395, 0.02332727052271366, 0.0], [0.8895062804222107, 0.04689619690179825, 0.0047171092592179775, 0.006286581978201866, 0.00609014043584466, 0.04650374501943588]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8938026428222656, 0.10619727522134781, 0.0, 0.0, 0.0, 0.0], [0.8221707940101624, 0.06304481625556946, 0.11478441953659058, 0.0, 0.0, 0.0], [0.5047380924224854, 0.15375731885433197, 0.2277037501335144, 0.11380083113908768, 0.0, 0.0], [0.4082071781158447, 0.09066355973482132, 0.11696872115135193, 0.24553199112415314, 0.13862857222557068, 0.0], [0.7291035652160645, 0.06638889014720917, 0.023112818598747253, 0.031103096902370453, 0.057143256068229675, 0.09314827620983124]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9247531890869141, 0.07524678111076355, 0.0, 0.0, 0.0, 0.0], [0.8957376480102539, 0.06989553570747375, 0.03436679765582085, 0.0, 0.0, 0.0], [0.7924937605857849, 0.0960114598274231, 0.05509118735790253, 0.056403566151857376, 0.0, 0.0], [0.7891505360603333, 0.07880303263664246, 0.03840155899524689, 0.05396979674696922, 0.03967496380209923, 0.0], [0.7807856798171997, 0.0799354612827301, 0.042531758546829224, 0.03234211727976799, 0.0178169384598732, 0.046588052064180374]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9480886459350586, 0.05191127583384514, 0.0, 0.0, 0.0, 0.0], [0.863694965839386, 0.04756204038858414, 0.08874296396970749, 0.0, 0.0, 0.0], [0.9341371059417725, 0.022224076092243195, 0.022624483332037926, 0.021014342084527016, 0.0, 0.0], [0.9588143229484558, 0.008020909503102303, 0.004490078426897526, 0.005862293299287558, 0.022812429815530777, 0.0], [0.9385918378829956, 0.021227721124887466, 0.0048724692314863205, 0.010940189473330975, 0.009524582885205746, 0.014843451790511608]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9763734340667725, 0.023626558482646942, 0.0, 0.0, 0.0, 0.0], [0.9884802103042603, 0.005189393647015095, 0.0063303736969828606, 0.0, 0.0, 0.0], [0.9477092027664185, 0.0179851483553648, 0.010156610049307346, 0.024149026721715927, 0.0, 0.0], [0.967192530632019, 0.006552813574671745, 0.0033227826934307814, 0.00556332478299737, 0.017368387430906296, 0.0], [0.9584562182426453, 0.007502961438149214, 0.0051363310776650906, 0.008071648888289928, 0.005997124593704939, 0.014835843816399574]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.884070873260498, 0.11592914164066315, 0.0, 0.0, 0.0, 0.0], [0.9931254386901855, 0.005070806015282869, 0.0018038019770756364, 0.0, 0.0, 0.0], [0.9534159302711487, 0.02382904477417469, 0.007748977281153202, 0.015006075613200665, 0.0, 0.0], [0.9151289463043213, 0.010873105376958847, 0.013190957717597485, 0.011050421744585037, 0.04975655674934387, 0.0], [0.8769673109054565, 0.03385210782289505, 0.00848648976534605, 0.009969149716198444, 0.03468578681349754, 0.036039214581251144]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0003709519514814019, 0.999629020690918, 0.0, 0.0, 0.0, 0.0], [6.525027856696397e-05, 0.3737829029560089, 0.6261518597602844, 0.0, 0.0, 0.0], [4.606018774211407e-05, 0.210508793592453, 0.4115968942642212, 0.3778482675552368, 0.0, 0.0], [4.753069515572861e-05, 0.11616954207420349, 0.23264272511005402, 0.3985331058502197, 0.2526070475578308, 0.0], [1.247641534973809e-06, 0.14819711446762085, 0.15813173353672028, 0.30074331164360046, 0.11939018964767456, 0.27353641390800476]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.971555769443512, 0.028444187715649605, 0.0, 0.0, 0.0, 0.0], [0.9529065489768982, 0.03233075141906738, 0.014762768521904945, 0.0, 0.0, 0.0], [0.9343128204345703, 0.02351292595267296, 0.02049802988767624, 0.021676240488886833, 0.0, 0.0], [0.9529678225517273, 0.00855141133069992, 0.004359325394034386, 0.008064556866884232, 0.026056913658976555, 0.0], [0.9653593897819519, 0.008487647399306297, 0.003499280195683241, 0.002721576252952218, 0.0032828773837536573, 0.016649367287755013]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8630780577659607, 0.13692188262939453, 0.0, 0.0, 0.0, 0.0], [0.7696157097816467, 0.0851333811879158, 0.14525099098682404, 0.0, 0.0, 0.0], [0.7133337259292603, 0.10170899331569672, 0.11931268870830536, 0.06564456224441528, 0.0, 0.0], [0.7186222076416016, 0.05444284901022911, 0.01386815495789051, 0.07808027416467667, 0.13498654961585999, 0.0], [0.7990148663520813, 0.05805593729019165, 0.009447019547224045, 0.017770467326045036, 0.02113853208720684, 0.09457314014434814]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9518988728523254, 0.048101115971803665, 0.0, 0.0, 0.0, 0.0], [0.8580653071403503, 0.02944577857851982, 0.11248888075351715, 0.0, 0.0, 0.0], [0.6577738523483276, 0.08513449877500534, 0.1261308640241623, 0.1309608370065689, 0.0, 0.0], [0.8087368607521057, 0.0323016420006752, 0.01841817982494831, 0.06856140494346619, 0.07198194414377213, 0.0], [0.6683295965194702, 0.13281384110450745, 0.021880635991692543, 0.02787741646170616, 0.04923408478498459, 0.0998644009232521]]]], \"left_text\": [\"No\", \",\", \" I\", \" am\", \" your\", \" father\"], \"right_text\": [\"No\", \",\", \" I\", \" am\", \" your\", \" father\"]}], \"default_filter\": \"0\", \"display_mode\": \"dark\", \"root_div_id\": \"bertviz-8e96ccb089214e25aab782e3d2695e5e\", \"include_layers\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], \"include_heads\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], \"total_heads\": 12}; // HACK: {\"attention\": [{\"name\": null, \"attn\": [[[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.961219847202301, 0.038780149072408676, 0.0, 0.0, 0.0, 0.0], [0.7466979026794434, 0.11987314373254776, 0.1334289014339447, 0.0, 0.0, 0.0], [0.5885030031204224, 0.13792067766189575, 0.212137371301651, 0.06143897399306297, 0.0, 0.0], [0.6570857763290405, 0.08996301889419556, 0.12751281261444092, 0.08361563086509705, 0.041822850704193115, 0.0], [0.2728874385356903, 0.11203353852033615, 0.1663985401391983, 0.08467111736536026, 0.16952736675739288, 0.19448210299015045]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.010616563260555267, 0.9893833994865417, 0.0, 0.0, 0.0, 0.0], [0.0024677535984665155, 0.008448007516562939, 0.9890841841697693, 0.0, 0.0, 0.0], [0.0001232847134815529, 0.0018733182223513722, 0.013126976788043976, 0.9848763942718506, 0.0, 0.0], [0.0010669564362615347, 0.001136627048254013, 0.003034998197108507, 0.0015735096530988812, 0.9931879043579102, 0.0], [0.00019791982776951045, 0.0010528112761676311, 0.0015437351539731026, 0.0009642760851420462, 3.4924432839034125e-05, 0.9962062835693359]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.47578439116477966, 0.524215579032898, 0.0, 0.0, 0.0, 0.0], [0.5906045436859131, 0.2486611008644104, 0.16073434054851532, 0.0, 0.0, 0.0], [0.5529289841651917, 0.18856702744960785, 0.14457571506500244, 0.11392831057310104, 0.0, 0.0], [0.45094072818756104, 0.16486799716949463, 0.17318038642406464, 0.11748014390468597, 0.09353074431419373, 0.0], [0.4257245659828186, 0.1732865273952484, 0.15651953220367432, 0.07022649794816971, 0.0808701142668724, 0.09337282180786133]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6133623123168945, 0.38663768768310547, 0.0, 0.0, 0.0, 0.0], [0.06098509579896927, 0.03253461793065071, 0.9064802527427673, 0.0, 0.0, 0.0], [0.006717085838317871, 0.0004012881254311651, 0.7572958469390869, 0.23558568954467773, 0.0, 0.0], [0.03722766041755676, 0.002948855282738805, 0.10081092268228531, 0.04142269119620323, 0.8175898790359497, 0.0], [0.04989781975746155, 0.00030758307548239827, 0.0024198265746235847, 0.0034334994852542877, 0.0006823898293077946, 0.9432588815689087]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9489555954933167, 0.051044441759586334, 0.0, 0.0, 0.0, 0.0], [0.6821408867835999, 0.1395241767168045, 0.17833495140075684, 0.0, 0.0, 0.0], [0.20366324484348297, 0.05641487240791321, 0.06399301439523697, 0.6759288311004639, 0.0, 0.0], [0.3419547975063324, 0.06725440919399261, 0.07926183938980103, 0.1783619523048401, 0.3331669867038727, 0.0], [0.09464015811681747, 0.0074282134883105755, 0.006983973551541567, 0.0071843694895505905, 0.018724264577031136, 0.865039050579071]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.33834606409072876, 0.6616539359092712, 0.0, 0.0, 0.0, 0.0], [0.07855993509292603, 0.006165449041873217, 0.9152746200561523, 0.0, 0.0, 0.0], [0.01677597686648369, 0.0004037705948576331, 0.003340460592880845, 0.9794798493385315, 0.0, 0.0], [0.027600426226854324, 0.00044415233423933387, 0.0006541680195368826, 0.0002266185765620321, 0.971074640750885, 0.0], [0.010248198173940182, 3.701553578139283e-05, 0.00016064041119534522, 2.7341819077264518e-05, 1.0187304724240676e-05, 0.98951655626297]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.982503354549408, 0.017496665939688683, 0.0, 0.0, 0.0, 0.0], [0.8874197006225586, 0.05467939004302025, 0.05790085718035698, 0.0, 0.0, 0.0], [0.6849910616874695, 0.1228068619966507, 0.04972026124596596, 0.14248186349868774, 0.0, 0.0], [0.6015856862068176, 0.09881888329982758, 0.07070108503103256, 0.16652540862560272, 0.06236903741955757, 0.0], [0.3232504427433014, 0.12567411363124847, 0.04432179778814316, 0.07076980918645859, 0.06606649607419968, 0.36991727352142334]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9191647171974182, 0.0808352455496788, 0.0, 0.0, 0.0, 0.0], [0.45986413955688477, 0.39703112840652466, 0.14310479164123535, 0.0, 0.0, 0.0], [0.3003872334957123, 0.22181738913059235, 0.38161516189575195, 0.09618020057678223, 0.0, 0.0], [0.18963925540447235, 0.1376371532678604, 0.20173484086990356, 0.23632164299488068, 0.23466713726520538, 0.0], [0.15410441160202026, 0.09489496797323227, 0.11902562528848648, 0.10277965664863586, 0.4317220449447632, 0.09747327119112015]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.364999920129776, 0.6350001096725464, 0.0, 0.0, 0.0, 0.0], [0.24595215916633606, 0.5519201755523682, 0.20212766528129578, 0.0, 0.0, 0.0], [0.2721358835697174, 0.40738627314567566, 0.25186213850975037, 0.06861574947834015, 0.0, 0.0], [0.10242555290460587, 0.16683615744113922, 0.524804949760437, 0.05445462837815285, 0.15147870779037476, 0.0], [0.25029507279396057, 0.22198128700256348, 0.18899968266487122, 0.10677118599414825, 0.1303267478942871, 0.10162602365016937]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6990506649017334, 0.3009493350982666, 0.0, 0.0, 0.0, 0.0], [0.5107942819595337, 0.2948642075061798, 0.1943415403366089, 0.0, 0.0, 0.0], [0.4604707360267639, 0.2805190980434418, 0.19174803793430328, 0.0672621801495552, 0.0, 0.0], [0.37648412585258484, 0.21120662987232208, 0.20214538276195526, 0.10207021236419678, 0.10809355974197388, 0.0], [0.30138441920280457, 0.20456179976463318, 0.18250338733196259, 0.11019382625818253, 0.1629127413034439, 0.03844383731484413]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7131582498550415, 0.2868417799472809, 0.0, 0.0, 0.0, 0.0], [0.4058799147605896, 0.18063297867774963, 0.41348710656166077, 0.0, 0.0, 0.0], [0.265546053647995, 0.1698586493730545, 0.3358593285083771, 0.228736013174057, 0.0, 0.0], [0.31385406851768494, 0.1831669807434082, 0.14928358793258667, 0.05377671495079994, 0.29991865158081055, 0.0], [0.20466560125350952, 0.18731118738651276, 0.15959151089191437, 0.06381776183843613, 0.03642302006483078, 0.34819093346595764]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6586242914199829, 0.3413757383823395, 0.0, 0.0, 0.0, 0.0], [0.5917776226997375, 0.3160035014152527, 0.0922188088297844, 0.0, 0.0, 0.0], [0.5477152466773987, 0.23586955666542053, 0.061456020921468735, 0.1549593061208725, 0.0, 0.0], [0.4587061107158661, 0.22439992427825928, 0.07887422293424606, 0.0992034301161766, 0.13881628215312958, 0.0], [0.32743722200393677, 0.19600819051265717, 0.068057119846344, 0.0892510637640953, 0.11618079245090485, 0.20306548476219177]]], [[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9961552023887634, 0.0038448425475507975, 0.0, 0.0, 0.0, 0.0], [0.8594854474067688, 0.06906110048294067, 0.07145342975854874, 0.0, 0.0, 0.0], [0.3800053000450134, 0.04127567633986473, 0.5496612787246704, 0.029057776555418968, 0.0, 0.0], [0.21445226669311523, 0.05088742449879646, 0.4317440092563629, 0.25869303941726685, 0.044223275035619736, 0.0], [0.11175256222486496, 0.017593080177903175, 0.027507441118359566, 0.04086771607398987, 0.7754669785499573, 0.026812179014086723]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9285967946052551, 0.07140326499938965, 0.0, 0.0, 0.0, 0.0], [0.6077286005020142, 0.3121427297592163, 0.08012867718935013, 0.0, 0.0, 0.0], [0.4942909777164459, 0.28503698110580444, 0.11849315464496613, 0.10217894613742828, 0.0, 0.0], [0.4183879494667053, 0.23117904365062714, 0.0834062322974205, 0.11365949362516403, 0.1533672958612442, 0.0], [0.42215850949287415, 0.12917140126228333, 0.08740927278995514, 0.1016375944018364, 0.21230268478393555, 0.04732053726911545]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9786475896835327, 0.02135237120091915, 0.0, 0.0, 0.0, 0.0], [0.7749121785163879, 0.06510371714830399, 0.15998409688472748, 0.0, 0.0, 0.0], [0.6484923362731934, 0.07483134418725967, 0.14751605689525604, 0.12916021049022675, 0.0, 0.0], [0.5224639773368835, 0.06921815127134323, 0.13823404908180237, 0.1110658198595047, 0.15901805460453033, 0.0], [0.3964517116546631, 0.07325823605060577, 0.12938153743743896, 0.1064242571592331, 0.14864002168178558, 0.1458442211151123]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5525906085968018, 0.44740936160087585, 0.0, 0.0, 0.0, 0.0], [0.5585009455680847, 0.2176259458065033, 0.22387312352657318, 0.0, 0.0, 0.0], [0.5143128633499146, 0.15964674949645996, 0.15491968393325806, 0.1711207628250122, 0.0, 0.0], [0.5039961338043213, 0.11401888728141785, 0.11974027007818222, 0.12552587687969208, 0.13671889901161194, 0.0], [0.5061842799186707, 0.08567393571138382, 0.08903021365404129, 0.09759818762540817, 0.1027572825551033, 0.11875619739294052]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9242545366287231, 0.07574543356895447, 0.0, 0.0, 0.0, 0.0], [0.8257425427436829, 0.07932533323764801, 0.09493216127157211, 0.0, 0.0, 0.0], [0.7306380271911621, 0.0857183039188385, 0.08043931424617767, 0.10320431739091873, 0.0, 0.0], [0.6383238434791565, 0.07886394113302231, 0.07815027981996536, 0.08758097141981125, 0.1170809343457222, 0.0], [0.5552157163619995, 0.07409121096134186, 0.06834889203310013, 0.07778600603342056, 0.09999319165945053, 0.12456497550010681]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8578913807868958, 0.14210854470729828, 0.0, 0.0, 0.0, 0.0], [0.6423038244247437, 0.166290283203125, 0.19140593707561493, 0.0, 0.0, 0.0], [0.5530979633331299, 0.10609274357557297, 0.07821257412433624, 0.26259663701057434, 0.0, 0.0], [0.40121692419052124, 0.12223611027002335, 0.1934729963541031, 0.14164622128009796, 0.14142780005931854, 0.0], [0.40212565660476685, 0.18450751900672913, 0.07516805827617645, 0.05849048122763634, 0.1444634348154068, 0.13524490594863892]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9791558980941772, 0.020844051614403725, 0.0, 0.0, 0.0, 0.0], [0.8829841613769531, 0.06233249977231026, 0.05468335747718811, 0.0, 0.0, 0.0], [0.8105455040931702, 0.08617085963487625, 0.07321777194738388, 0.03006584383547306, 0.0, 0.0], [0.6819812059402466, 0.04990820586681366, 0.08296552300453186, 0.08369525521993637, 0.10144983977079391, 0.0], [0.4056689441204071, 0.07337666302919388, 0.08601408451795578, 0.061709366738796234, 0.13226434588432312, 0.2409665435552597]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9670190811157227, 0.03298088163137436, 0.0, 0.0, 0.0, 0.0], [0.8449064493179321, 0.0851450264453888, 0.06994850933551788, 0.0, 0.0, 0.0], [0.7123572826385498, 0.07896047830581665, 0.055410757660865784, 0.15327158570289612, 0.0, 0.0], [0.6402613520622253, 0.0739755630493164, 0.044393062591552734, 0.14322125911712646, 0.09814881533384323, 0.0], [0.5073903799057007, 0.07523059099912643, 0.07754647731781006, 0.11362491548061371, 0.13947951793670654, 0.08672808855772018]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8487569093704224, 0.1512431502342224, 0.0, 0.0, 0.0, 0.0], [0.8415648937225342, 0.12107233703136444, 0.03736274689435959, 0.0, 0.0, 0.0], [0.7505517601966858, 0.11348944902420044, 0.06179959326982498, 0.07415912300348282, 0.0, 0.0], [0.6614719033241272, 0.10242646187543869, 0.052934251725673676, 0.07529708743095398, 0.10787025839090347, 0.0], [0.6014202237129211, 0.11340376734733582, 0.05631929263472557, 0.07096721231937408, 0.10906282067298889, 0.04882663115859032]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9445484280586243, 0.05545158311724663, 0.0, 0.0, 0.0, 0.0], [0.8874568939208984, 0.05474215745925903, 0.0578010231256485, 0.0, 0.0, 0.0], [0.8281888961791992, 0.06895001977682114, 0.059034693986177444, 0.0438263975083828, 0.0, 0.0], [0.6429892778396606, 0.0674755647778511, 0.11629703640937805, 0.05417950078845024, 0.11905858665704727, 0.0], [0.7367823719978333, 0.056119054555892944, 0.06857288628816605, 0.034219540655612946, 0.0787537544965744, 0.02555238828063011]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0002913394710049033, 0.9997085928916931, 0.0, 0.0, 0.0, 0.0], [0.0007981209782883525, 0.5288336873054504, 0.4703682065010071, 0.0, 0.0, 0.0], [0.0007648481405340135, 0.34519824385643005, 0.3085267245769501, 0.34551018476486206, 0.0, 0.0], [0.0010283143492415547, 0.241359144449234, 0.23320138454437256, 0.2555713355541229, 0.2688397467136383, 0.0], [0.0009746829164214432, 0.17789699137210846, 0.16743157804012299, 0.1858760118484497, 0.18734444677829742, 0.28047630190849304]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.824492871761322, 0.17550717294216156, 0.0, 0.0, 0.0, 0.0], [0.12386877834796906, 0.044499922543764114, 0.8316312432289124, 0.0, 0.0, 0.0], [0.07924355566501617, 0.01296587660908699, 0.0015277155907824636, 0.9062628149986267, 0.0, 0.0], [0.08806384354829788, 0.0213409923017025, 0.0028886159416288137, 0.002845379989594221, 0.884861171245575, 0.0], [0.09983218461275101, 0.03363388776779175, 0.0054999832063913345, 0.002433052286505699, 0.0015082412865012884, 0.8570926189422607]]], [[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9646892547607422, 0.03531072288751602, 0.0, 0.0, 0.0, 0.0], [0.7529157400131226, 0.08733473718166351, 0.15974950790405273, 0.0, 0.0, 0.0], [0.4202282726764679, 0.09195102006196976, 0.23549850285053253, 0.25232216715812683, 0.0, 0.0], [0.30848920345306396, 0.05908140912652016, 0.38391315937042236, 0.15659146010875702, 0.09192468225955963, 0.0], [0.44790443778038025, 0.04329312965273857, 0.0796918049454689, 0.11081931740045547, 0.22124572098255157, 0.09704558551311493]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.991096019744873, 0.008904009126126766, 0.0, 0.0, 0.0, 0.0], [0.9697675704956055, 0.026084503158926964, 0.004147922620177269, 0.0, 0.0, 0.0], [0.9082901477813721, 0.033206019550561905, 0.00942116230726242, 0.049082688987255096, 0.0, 0.0], [0.8949133157730103, 0.05544555187225342, 0.005577624775469303, 0.03150692582130432, 0.012556522153317928, 0.0], [0.8497740030288696, 0.028890123590826988, 0.0036647915840148926, 0.03751987963914871, 0.038427725434303284, 0.04172350466251373]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9984525442123413, 0.0015474462416023016, 0.0, 0.0, 0.0, 0.0], [0.48947831988334656, 0.4812193810939789, 0.029302269220352173, 0.0, 0.0, 0.0], [0.11772153526544571, 0.13121186196804047, 0.6702314615249634, 0.08083520829677582, 0.0, 0.0], [0.13043689727783203, 0.04068669304251671, 0.2652038037776947, 0.4114362895488739, 0.15223638713359833, 0.0], [0.12661904096603394, 0.03275119513273239, 0.03567872568964958, 0.06039190664887428, 0.6021825075149536, 0.1423766165971756]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9805176854133606, 0.019482342526316643, 0.0, 0.0, 0.0, 0.0], [0.7948849201202393, 0.12061909586191177, 0.08449601382017136, 0.0, 0.0, 0.0], [0.5612356066703796, 0.15743127465248108, 0.20339730381965637, 0.0779358446598053, 0.0, 0.0], [0.42583736777305603, 0.10742014646530151, 0.15123659372329712, 0.08755031228065491, 0.22795552015304565, 0.0], [0.24752654135227203, 0.024188270792365074, 0.03039524517953396, 0.08586956560611725, 0.5714336633682251, 0.040586672723293304]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9887767434120178, 0.011223225854337215, 0.0, 0.0, 0.0, 0.0], [0.7572693228721619, 0.22317346930503845, 0.019557112827897072, 0.0, 0.0, 0.0], [0.5341880321502686, 0.22107566893100739, 0.1762184202671051, 0.06851787120103836, 0.0, 0.0], [0.17095312476158142, 0.0822940468788147, 0.576022207736969, 0.11097585409879684, 0.059754710644483566, 0.0], [0.2487109899520874, 0.08880793303251266, 0.08980197459459305, 0.09729334712028503, 0.4413093626499176, 0.03407646715641022]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8422133326530457, 0.15778663754463196, 0.0, 0.0, 0.0, 0.0], [0.468412846326828, 0.46105360984802246, 0.07053359597921371, 0.0, 0.0, 0.0], [0.2588140666484833, 0.4635888636112213, 0.18503506481647491, 0.09256205707788467, 0.0, 0.0], [0.18399578332901, 0.29154160618782043, 0.17031098902225494, 0.27173006534576416, 0.08242159336805344, 0.0], [0.1646990180015564, 0.2472696155309677, 0.08770562708377838, 0.22575001418590546, 0.1774536371231079, 0.09712201356887817]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9919946193695068, 0.008005390875041485, 0.0, 0.0, 0.0, 0.0], [0.9068724513053894, 0.044065121561288834, 0.04906242713332176, 0.0, 0.0, 0.0], [0.8582221865653992, 0.055348269641399384, 0.040419407188892365, 0.046010036021471024, 0.0, 0.0], [0.7855252623558044, 0.041242364794015884, 0.08369296044111252, 0.04887620359659195, 0.040663279592990875, 0.0], [0.7856317162513733, 0.05014643445611, 0.04751267284154892, 0.027365952730178833, 0.05614755302667618, 0.03319567069411278]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9041035175323486, 0.09589648246765137, 0.0, 0.0, 0.0, 0.0], [0.5862312912940979, 0.07199832051992416, 0.34177035093307495, 0.0, 0.0, 0.0], [0.3878960907459259, 0.04660807177424431, 0.20278996229171753, 0.36270591616630554, 0.0, 0.0], [0.2665242552757263, 0.024533024057745934, 0.12211935967206955, 0.20041218400001526, 0.386411190032959, 0.0], [0.23357485234737396, 0.02053728699684143, 0.09610321372747421, 0.13062246143817902, 0.22990450263023376, 0.289257675409317]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9639912247657776, 0.036008793860673904, 0.0, 0.0, 0.0, 0.0], [0.7075552344322205, 0.2542775869369507, 0.038167137652635574, 0.0, 0.0, 0.0], [0.2566526234149933, 0.20589298009872437, 0.01665665954351425, 0.5207977294921875, 0.0, 0.0], [0.1037939190864563, 0.04639088362455368, 0.008698614314198494, 0.7866851687431335, 0.05443140119314194, 0.0], [0.2214341163635254, 0.03379744663834572, 0.029023902490735054, 0.541292130947113, 0.15286092460155487, 0.021591555327177048]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9891703724861145, 0.010829661041498184, 0.0, 0.0, 0.0, 0.0], [0.7913155555725098, 0.12309625744819641, 0.08558809012174606, 0.0, 0.0, 0.0], [0.2954600155353546, 0.15808308124542236, 0.4217240810394287, 0.1247328370809555, 0.0, 0.0], [0.23440983891487122, 0.09886523336172104, 0.33160170912742615, 0.1971396654844284, 0.1379835456609726, 0.0], [0.19728390872478485, 0.05741839483380318, 0.06909029185771942, 0.16469819843769073, 0.2797277867794037, 0.23178131878376007]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9359127879142761, 0.0640871673822403, 0.0, 0.0, 0.0, 0.0], [0.7888627648353577, 0.08673475682735443, 0.12440246343612671, 0.0, 0.0, 0.0], [0.6535118818283081, 0.07573551684617996, 0.09732568264007568, 0.17342689633369446, 0.0, 0.0], [0.522276759147644, 0.058278825134038925, 0.09920477122068405, 0.17020836472511292, 0.15003129839897156, 0.0], [0.4108840823173523, 0.047306034713983536, 0.07265672832727432, 0.10560744255781174, 0.10550004243850708, 0.25804558396339417]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9683833122253418, 0.03161672502756119, 0.0, 0.0, 0.0, 0.0], [0.8965396881103516, 0.038870569318532944, 0.06458976864814758, 0.0, 0.0, 0.0], [0.8264952898025513, 0.03213464096188545, 0.05196719989180565, 0.0894029513001442, 0.0, 0.0], [0.7718173265457153, 0.030402837321162224, 0.045827414840459824, 0.07118473201990128, 0.08076759427785873, 0.0], [0.7292331457138062, 0.021699821576476097, 0.033074747771024704, 0.04720093309879303, 0.06474557518959045, 0.10404567420482635]]], [[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9979567527770996, 0.0020432830788195133, 0.0, 0.0, 0.0, 0.0], [0.955294132232666, 0.00802531372755766, 0.03668047487735748, 0.0, 0.0, 0.0], [0.9254710078239441, 0.002755576279014349, 0.0020629852078855038, 0.06971040368080139, 0.0, 0.0], [0.8660576939582825, 0.0038883681409060955, 0.0006785982404835522, 0.0006981453043408692, 0.1286771297454834, 0.0], [0.8455929160118103, 0.0037804055027663708, 0.000253423087997362, 6.0270751419011503e-05, 0.00011820747749879956, 0.15019479393959045]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9262455105781555, 0.07375453412532806, 0.0, 0.0, 0.0, 0.0], [0.7717157006263733, 0.16241952776908875, 0.06586471945047379, 0.0, 0.0, 0.0], [0.8167637586593628, 0.07807160913944244, 0.06324034929275513, 0.041924238204956055, 0.0, 0.0], [0.6867184638977051, 0.07755157351493835, 0.10056912153959274, 0.05955080687999725, 0.07561002671718597, 0.0], [0.6421161890029907, 0.11014898866415024, 0.07688194513320923, 0.054033469408750534, 0.10333634912967682, 0.013483096845448017]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9395954608917236, 0.060404520481824875, 0.0, 0.0, 0.0, 0.0], [0.23004619777202606, 0.6617380380630493, 0.1082158014178276, 0.0, 0.0, 0.0], [0.2670227289199829, 0.3607950508594513, 0.3249626159667969, 0.047219593077898026, 0.0, 0.0], [0.595201313495636, 0.12269274890422821, 0.06302059441804886, 0.08916817605495453, 0.12991715967655182, 0.0], [0.10284596681594849, 0.02938011661171913, 0.013739082030951977, 0.045860596001148224, 0.7698501348495483, 0.03832406550645828]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9040980935096741, 0.09590194374322891, 0.0, 0.0, 0.0, 0.0], [0.357237845659256, 0.6274612545967102, 0.015300876460969448, 0.0, 0.0, 0.0], [0.5917996764183044, 0.2764042019844055, 0.10476048290729523, 0.027035649865865707, 0.0, 0.0], [0.7254403829574585, 0.04983152449131012, 0.014982940629124641, 0.1778142899274826, 0.031930916011333466, 0.0], [0.7612743973731995, 0.06158972904086113, 0.005942251533269882, 0.01642685756087303, 0.1267806589603424, 0.0279861893504858]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9947587847709656, 0.005241230130195618, 0.0, 0.0, 0.0, 0.0], [0.9632415771484375, 0.017816413193941116, 0.018942030146718025, 0.0, 0.0, 0.0], [0.9671078324317932, 0.008509586565196514, 0.00856222677975893, 0.015820473432540894, 0.0, 0.0], [0.9340996146202087, 0.011952387169003487, 0.02018021047115326, 0.02675083465874195, 0.0070168930105865, 0.0], [0.9587237238883972, 0.004657115787267685, 0.003326789475977421, 0.006545313633978367, 0.010182461701333523, 0.016564540565013885]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9769991040229797, 0.023000910878181458, 0.0, 0.0, 0.0, 0.0], [0.7917609214782715, 0.1753319948911667, 0.032907065004110336, 0.0, 0.0, 0.0], [0.7949192523956299, 0.10531841963529587, 0.040218502283096313, 0.05954383686184883, 0.0, 0.0], [0.7097718715667725, 0.10552527755498886, 0.06597573310136795, 0.05765606462955475, 0.061070989817380905, 0.0], [0.7506601214408875, 0.026514461264014244, 0.021576043218374252, 0.034296683967113495, 0.08494450151920319, 0.08200812339782715]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.983751654624939, 0.016248304396867752, 0.0, 0.0, 0.0, 0.0], [0.5615494847297668, 0.08956841379404068, 0.3488820493221283, 0.0, 0.0, 0.0], [0.32929039001464844, 0.024114903062582016, 0.5428059697151184, 0.10378880053758621, 0.0, 0.0], [0.34330207109451294, 0.01308644749224186, 0.5121983289718628, 0.11146228760480881, 0.019950881600379944, 0.0], [0.4792812764644623, 0.01733359508216381, 0.1180536150932312, 0.06130281835794449, 0.20071913301944733, 0.12330964207649231]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9908847212791443, 0.009115329943597317, 0.0, 0.0, 0.0, 0.0], [0.5282707214355469, 0.3292262554168701, 0.1425030380487442, 0.0, 0.0, 0.0], [0.48788541555404663, 0.23368670046329498, 0.17578084766864777, 0.10264702141284943, 0.0, 0.0], [0.31444698572158813, 0.18065163493156433, 0.168714240193367, 0.09506598114967346, 0.24112118780612946, 0.0], [0.5168765187263489, 0.035897161811590195, 0.026188155636191368, 0.04039734974503517, 0.18791745603084564, 0.1927233189344406]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8750308156013489, 0.12496919929981232, 0.0, 0.0, 0.0, 0.0], [0.4550614655017853, 0.4900427758693695, 0.05489582195878029, 0.0, 0.0, 0.0], [0.2933720052242279, 0.5449907183647156, 0.09444297850131989, 0.06719419360160828, 0.0, 0.0], [0.489708811044693, 0.2720997631549835, 0.06861965358257294, 0.14694802463054657, 0.022623788565397263, 0.0], [0.4729066491127014, 0.08103099465370178, 0.016052134335041046, 0.30672287940979004, 0.10120721161365509, 0.022080255672335625]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9630220532417297, 0.03697792813181877, 0.0, 0.0, 0.0, 0.0], [0.7557195425033569, 0.16436372697353363, 0.07991670072078705, 0.0, 0.0, 0.0], [0.6947705745697021, 0.08409853279590607, 0.0638260766863823, 0.15730486810207367, 0.0, 0.0], [0.5821147561073303, 0.03297805413603783, 0.07936596870422363, 0.19441406428813934, 0.11112712323665619, 0.0], [0.5974540710449219, 0.04261096194386482, 0.06919723749160767, 0.14563441276550293, 0.12481734901666641, 0.020285936072468758]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9957822561264038, 0.004217816516757011, 0.0, 0.0, 0.0, 0.0], [0.9312832951545715, 0.010560247115790844, 0.05815650522708893, 0.0, 0.0, 0.0], [0.8435326814651489, 0.015695005655288696, 0.045751139521598816, 0.09502115100622177, 0.0, 0.0], [0.772409975528717, 0.011981245130300522, 0.03504609689116478, 0.03876771405339241, 0.14179500937461853, 0.0], [0.7642908692359924, 0.009868789464235306, 0.00812275055795908, 0.013314393348991871, 0.04824395477771759, 0.15615922212600708]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9701177477836609, 0.02988232672214508, 0.0, 0.0, 0.0, 0.0], [0.6564007997512817, 0.22506150603294373, 0.11853761970996857, 0.0, 0.0, 0.0], [0.6958062648773193, 0.14701850712299347, 0.07145983725786209, 0.08571550250053406, 0.0, 0.0], [0.6353274583816528, 0.1346064656972885, 0.030994214117527008, 0.056916315108537674, 0.1421555131673813, 0.0], [0.6779401898384094, 0.053654152899980545, 0.01800631172955036, 0.06284520775079727, 0.1103820651769638, 0.07717210054397583]]], [[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9822334051132202, 0.017766647040843964, 0.0, 0.0, 0.0, 0.0], [0.9037663340568542, 0.06541544198989868, 0.03081829659640789, 0.0, 0.0, 0.0], [0.8119193911552429, 0.03679030388593674, 0.060560714453458786, 0.09072960168123245, 0.0, 0.0], [0.40546438097953796, 0.10383912175893784, 0.10211236774921417, 0.35434210300445557, 0.03424208238720894, 0.0], [0.22824221849441528, 0.017278727144002914, 0.05055465176701546, 0.6015752553939819, 0.09411764144897461, 0.008231506682932377]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9873148202896118, 0.012685136869549751, 0.0, 0.0, 0.0, 0.0], [0.35445743799209595, 0.5317603349685669, 0.11378221958875656, 0.0, 0.0, 0.0], [0.07823363691568375, 0.7221359014511108, 0.10936623811721802, 0.090264230966568, 0.0, 0.0], [0.21967869997024536, 0.4048435091972351, 0.12358088046312332, 0.20018866658210754, 0.051708199083805084, 0.0], [0.36089760065078735, 0.10459021478891373, 0.06983799487352371, 0.2976483404636383, 0.13869903981685638, 0.02832675166428089]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9732162356376648, 0.0267837755382061, 0.0, 0.0, 0.0, 0.0], [0.9167553782463074, 0.061452705413103104, 0.02179192565381527, 0.0, 0.0, 0.0], [0.8543081283569336, 0.08049600571393967, 0.030334919691085815, 0.03486092761158943, 0.0, 0.0], [0.8919214606285095, 0.04280779883265495, 0.022045055404305458, 0.023470671847462654, 0.01975487545132637, 0.0], [0.8116763234138489, 0.03413533419370651, 0.03567665070295334, 0.04748587682843208, 0.0253971628844738, 0.04562860727310181]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9502761960029602, 0.04972382262349129, 0.0, 0.0, 0.0, 0.0], [0.7637454271316528, 0.2007361352443695, 0.03551840782165527, 0.0, 0.0, 0.0], [0.6279097199440002, 0.03768139332532883, 0.1994536966085434, 0.13495522737503052, 0.0, 0.0], [0.6397060751914978, 0.027007432654500008, 0.09082036465406418, 0.20653828978538513, 0.03592785820364952, 0.0], [0.4559425115585327, 0.021641194820404053, 0.12939567863941193, 0.21800927817821503, 0.10379841923713684, 0.07121295481920242]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9498406648635864, 0.050159383565187454, 0.0, 0.0, 0.0, 0.0], [0.8688724637031555, 0.0872218981385231, 0.043905653059482574, 0.0, 0.0, 0.0], [0.6937950253486633, 0.06359200924634933, 0.091790571808815, 0.15082231163978577, 0.0, 0.0], [0.7266597151756287, 0.04389883577823639, 0.04683985933661461, 0.09851823002099991, 0.08408336341381073, 0.0], [0.7848998308181763, 0.037147827446460724, 0.012907838448882103, 0.01053939200937748, 0.12079165875911713, 0.03371351957321167]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9891054034233093, 0.01089458167552948, 0.0, 0.0, 0.0, 0.0], [0.8929519653320312, 0.08700055629014969, 0.02004752680659294, 0.0, 0.0, 0.0], [0.7891124486923218, 0.09797251224517822, 0.08633202314376831, 0.026582980528473854, 0.0, 0.0], [0.8850635886192322, 0.03645012155175209, 0.05395457148551941, 0.01237727515399456, 0.012154522351920605, 0.0], [0.6861329674720764, 0.05720378831028938, 0.011636304669082165, 0.021660611033439636, 0.1748800277709961, 0.048486363142728806]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9396191835403442, 0.06038080155849457, 0.0, 0.0, 0.0, 0.0], [0.7851794958114624, 0.19751444458961487, 0.017306052148342133, 0.0, 0.0, 0.0], [0.7660509943962097, 0.15444670617580414, 0.03188290074467659, 0.04761936888098717, 0.0, 0.0], [0.703522801399231, 0.05171430483460426, 0.07760990411043167, 0.1533905267715454, 0.013762423768639565, 0.0], [0.7121888399124146, 0.04994234815239906, 0.03772548958659172, 0.08649132400751114, 0.06541401147842407, 0.04823806509375572]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.974072277545929, 0.025927715003490448, 0.0, 0.0, 0.0, 0.0], [0.792539656162262, 0.01171559002250433, 0.19574476778507233, 0.0, 0.0, 0.0], [0.5106770992279053, 0.007296787109225988, 0.039619915187358856, 0.4424062669277191, 0.0, 0.0], [0.5862472057342529, 0.012099712155759335, 0.024585209786891937, 0.06737840175628662, 0.30968940258026123, 0.0], [0.30196306109428406, 0.007724012713879347, 0.011518122628331184, 0.046947259455919266, 0.22146707773208618, 0.41038045287132263]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9744554162025452, 0.02554464340209961, 0.0, 0.0, 0.0, 0.0], [0.9769195318222046, 0.015048524364829063, 0.008031901903450489, 0.0, 0.0, 0.0], [0.9060619473457336, 0.025875424966216087, 0.025954782962799072, 0.04210779070854187, 0.0, 0.0], [0.9400081038475037, 0.00555665697902441, 0.005828304681926966, 0.031757812947034836, 0.016849134117364883, 0.0], [0.9105738401412964, 0.0019752182997763157, 0.008646721951663494, 0.013360846787691116, 0.03543964773416519, 0.030003678053617477]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9791666865348816, 0.020833350718021393, 0.0, 0.0, 0.0, 0.0], [0.8444858193397522, 0.13507869839668274, 0.020435383543372154, 0.0, 0.0, 0.0], [0.7903086543083191, 0.14559169113636017, 0.037529975175857544, 0.026569725945591927, 0.0, 0.0], [0.7298924326896667, 0.056496407836675644, 0.032735615968704224, 0.10400459170341492, 0.07687094807624817, 0.0], [0.5684185028076172, 0.04388832300901413, 0.026293467730283737, 0.0811714455485344, 0.24314835667610168, 0.037079911679029465]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9499868154525757, 0.05001320689916611, 0.0, 0.0, 0.0, 0.0], [0.9336170554161072, 0.05848868936300278, 0.007894262671470642, 0.0, 0.0, 0.0], [0.7897834181785583, 0.11071821302175522, 0.05360178276896477, 0.04589657858014107, 0.0, 0.0], [0.885930061340332, 0.05752986669540405, 0.01374326553195715, 0.0033877466339617968, 0.03940902277827263, 0.0], [0.9337607622146606, 0.02647063508629799, 0.004523396957665682, 0.0061904797330498695, 0.014132906682789326, 0.014921708963811398]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [1.0, 1.8521224554035598e-09, 0.0, 0.0, 0.0, 0.0], [6.6758907451003324e-06, 0.9999804496765137, 1.2841281204600818e-05, 0.0, 0.0, 0.0], [2.2194194926328237e-08, 2.6684581211355862e-09, 0.9999971389770508, 2.8136880700913025e-06, 0.0, 0.0], [1.0145409987671883e-06, 4.464065739284706e-08, 0.00035356366424821317, 0.9993677735328674, 0.0002776293840724975, 0.0], [9.436550429953172e-10, 1.382057315812979e-11, 5.017835036369434e-10, 2.965183876213473e-09, 0.9999971389770508, 2.8644042231462663e-06]]], [[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9948632121086121, 0.005136783700436354, 0.0, 0.0, 0.0, 0.0], [0.9274215698242188, 0.01832387037575245, 0.05425456911325455, 0.0, 0.0, 0.0], [0.9678993225097656, 0.004143435508012772, 0.004314453341066837, 0.023642776533961296, 0.0, 0.0], [0.8999068737030029, 0.001467161695472896, 0.00029133574571460485, 0.002585014794021845, 0.09574954956769943, 0.0], [0.9386115670204163, 0.00022248300956562161, 0.0006146665546111763, 0.0015495637198910117, 0.030689461156725883, 0.028312424197793007]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9999959468841553, 4.042720775032649e-06, 0.0, 0.0, 0.0, 0.0], [0.9982761144638062, 3.2613831990602193e-06, 0.001720669330097735, 0.0, 0.0, 0.0], [0.9998809099197388, 5.328835683826583e-08, 6.376215537784446e-07, 0.00011847059795400128, 0.0, 0.0], [0.9996154308319092, 3.473169556400535e-07, 3.8920820344401363e-08, 4.468433303372876e-07, 0.00038369710091501474, 0.0], [0.9994840621948242, 1.655020476221125e-08, 2.8715557931491276e-08, 1.0638284493325045e-06, 0.0002126671897713095, 0.00030212008277885616]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9514135718345642, 0.048586405813694, 0.0, 0.0, 0.0, 0.0], [0.5749948024749756, 0.39028096199035645, 0.03472418338060379, 0.0, 0.0, 0.0], [0.7442318201065063, 0.1752411425113678, 0.0756477490067482, 0.004879283253103495, 0.0, 0.0], [0.5232070684432983, 0.09429339319467545, 0.1138191670179367, 0.19979268312454224, 0.06888769567012787, 0.0], [0.47472575306892395, 0.05636607110500336, 0.04530389606952667, 0.06967321783304214, 0.3098014295101166, 0.0441296212375164]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8734648823738098, 0.12653514742851257, 0.0, 0.0, 0.0, 0.0], [0.6097912788391113, 0.3541727066040039, 0.036036062985658646, 0.0, 0.0, 0.0], [0.45984190702438354, 0.38697871565818787, 0.0996011346578598, 0.05357823893427849, 0.0, 0.0], [0.572220504283905, 0.23636263608932495, 0.08344558626413345, 0.06921917200088501, 0.03875211998820305, 0.0], [0.5143564343452454, 0.16723087430000305, 0.09019406139850616, 0.0765448659658432, 0.10578085482120514, 0.04589281603693962]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.981228768825531, 0.018771231174468994, 0.0, 0.0, 0.0, 0.0], [0.6142941117286682, 0.3503977954387665, 0.0353081189095974, 0.0, 0.0, 0.0], [0.5770686268806458, 0.32858458161354065, 0.05508256331086159, 0.03926428034901619, 0.0, 0.0], [0.17188192903995514, 0.011042501777410507, 0.054578714072704315, 0.7326585650444031, 0.029838265851140022, 0.0], [0.3783015012741089, 0.017070062458515167, 0.021754134446382523, 0.4409688115119934, 0.06093813106417656, 0.08096737414598465]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9923112392425537, 0.007688735146075487, 0.0, 0.0, 0.0, 0.0], [0.9498787522315979, 0.016709784045815468, 0.03341152146458626, 0.0, 0.0, 0.0], [0.9961295127868652, 0.0008787295082584023, 0.0006868162308819592, 0.0023048371076583862, 0.0, 0.0], [0.9935757517814636, 0.0032634998206049204, 0.0009993825806304812, 0.00027932299417443573, 0.0018820574041455984, 0.0], [0.9907532930374146, 0.00021344318520277739, 0.0004595233185682446, 0.0007905619568191469, 0.004424723796546459, 0.003358350833877921]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9647740125656128, 0.03522596135735512, 0.0, 0.0, 0.0, 0.0], [0.8194130063056946, 0.1365436613559723, 0.04404333233833313, 0.0, 0.0, 0.0], [0.7584245800971985, 0.006878929678350687, 0.20653395354747772, 0.028162529692053795, 0.0, 0.0], [0.5298128128051758, 0.002678812015801668, 0.07857988774776459, 0.3598373234272003, 0.02909109927713871, 0.0], [0.7544413208961487, 0.00036782227107323706, 0.0019713479559868574, 0.00324004958383739, 0.1942344754934311, 0.04574500769376755]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9749131202697754, 0.02508680149912834, 0.0, 0.0, 0.0, 0.0], [0.9306471943855286, 0.05705660209059715, 0.012296222150325775, 0.0, 0.0, 0.0], [0.9305251836776733, 0.052770983427762985, 0.01111945416778326, 0.005584415514022112, 0.0, 0.0], [0.8863320350646973, 0.01292418036609888, 0.017724711447954178, 0.06150198355317116, 0.021517015993595123, 0.0], [0.791684627532959, 0.015036096796393394, 0.0317479707300663, 0.03392200171947479, 0.03707978501915932, 0.09052948653697968]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9608501195907593, 0.039149850606918335, 0.0, 0.0, 0.0, 0.0], [0.9121272563934326, 0.02257651649415493, 0.06529619544744492, 0.0, 0.0, 0.0], [0.9364108443260193, 0.015584447421133518, 0.024544963613152504, 0.02345985174179077, 0.0, 0.0], [0.9454620480537415, 0.006762288510799408, 0.022026237100362778, 0.009137796238064766, 0.016611700877547264, 0.0], [0.8346164226531982, 0.001881699077785015, 0.00560904573649168, 0.01887359470129013, 0.12449200451374054, 0.014527074061334133]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9964227080345154, 0.0035772807896137238, 0.0, 0.0, 0.0, 0.0], [0.9713928699493408, 0.024453025311231613, 0.004154058638960123, 0.0, 0.0, 0.0], [0.9735792279243469, 0.019003381952643394, 0.003664410673081875, 0.0037529165856540203, 0.0, 0.0], [0.9586312174797058, 0.007116180844604969, 0.009218388237059116, 0.022725583985447884, 0.0023084774147719145, 0.0], [0.973607063293457, 0.008490582928061485, 0.0032512471079826355, 0.003606445388868451, 0.004877461586147547, 0.006167212035506964]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.97598797082901, 0.024011990055441856, 0.0, 0.0, 0.0, 0.0], [0.9460638165473938, 0.04211375489830971, 0.011822436936199665, 0.0, 0.0, 0.0], [0.8446813225746155, 0.04293116182088852, 0.05218198522925377, 0.06020559370517731, 0.0, 0.0], [0.9378372430801392, 0.03354858607053757, 0.008826455101370811, 0.0028792242519557476, 0.016908427700400352, 0.0], [0.8124931454658508, 0.02696753479540348, 0.05999218672513962, 0.03445731848478317, 0.011011860333383083, 0.05507794767618179]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9001203775405884, 0.09987961500883102, 0.0, 0.0, 0.0, 0.0], [0.627193033695221, 0.07988718152046204, 0.29291975498199463, 0.0, 0.0, 0.0], [0.7624077796936035, 0.02734432928264141, 0.038679543882608414, 0.17156831920146942, 0.0, 0.0], [0.7995968461036682, 0.014336260966956615, 0.01437566988170147, 0.025438452139496803, 0.14625284075737, 0.0], [0.7851970791816711, 0.04204057529568672, 0.025253651663661003, 0.02908395044505596, 0.029306314885616302, 0.08911846578121185]]], [[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9954467415809631, 0.0045532057993113995, 0.0, 0.0, 0.0, 0.0], [0.9356001615524292, 0.04476744681596756, 0.019632352516055107, 0.0, 0.0, 0.0], [0.5605552792549133, 0.09861977398395538, 0.29983264207839966, 0.040992289781570435, 0.0, 0.0], [0.5893709659576416, 0.11000988632440567, 0.08033622056245804, 0.16754034161567688, 0.05274256691336632, 0.0], [0.22305884957313538, 0.05680817365646362, 0.05467984080314636, 0.24733951687812805, 0.3111244738101959, 0.1069890558719635]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9301451444625854, 0.06985488533973694, 0.0, 0.0, 0.0, 0.0], [0.8936478495597839, 0.08535721153020859, 0.020994966849684715, 0.0, 0.0, 0.0], [0.8404538035392761, 0.10619214922189713, 0.02363673783838749, 0.029717326164245605, 0.0, 0.0], [0.8927386403083801, 0.024784674867987633, 0.008319000713527203, 0.05165454372763634, 0.022503145039081573, 0.0], [0.8646610975265503, 0.009503193199634552, 0.0024329854641109705, 0.04796753078699112, 0.04273205250501633, 0.03270319849252701]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9859625697135925, 0.014037408865988255, 0.0, 0.0, 0.0, 0.0], [0.9702037572860718, 0.0168070700019598, 0.012989125214517117, 0.0, 0.0, 0.0], [0.9524770379066467, 0.016064459457993507, 0.013456220738589764, 0.018002323806285858, 0.0, 0.0], [0.9332928657531738, 0.01897200010716915, 0.02014683373272419, 0.017023753374814987, 0.010564540512859821, 0.0], [0.9113592505455017, 0.012528638355433941, 0.02209620550274849, 0.01751861348748207, 0.018517911434173584, 0.01797938533127308]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9681769013404846, 0.03182310611009598, 0.0, 0.0, 0.0, 0.0], [0.9096417427062988, 0.07916690409183502, 0.011191264726221561, 0.0, 0.0, 0.0], [0.8379932045936584, 0.13078266382217407, 0.012140989303588867, 0.019083037972450256, 0.0, 0.0], [0.9116525053977966, 0.05451957508921623, 0.009499342180788517, 0.00746585289016366, 0.01686275750398636, 0.0], [0.8510289192199707, 0.07338211685419083, 0.008022507652640343, 0.009083161130547523, 0.04261006414890289, 0.015873271971940994]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9799023866653442, 0.020097682252526283, 0.0, 0.0, 0.0, 0.0], [0.9558742642402649, 0.029063312336802483, 0.015062497928738594, 0.0, 0.0, 0.0], [0.7943133115768433, 0.06074100360274315, 0.06907659024000168, 0.07586916536092758, 0.0, 0.0], [0.5494324564933777, 0.03154711425304413, 0.05482015758752823, 0.05788077041506767, 0.3063195049762726, 0.0], [0.6453980803489685, 0.010770943015813828, 0.017528092488646507, 0.02157985046505928, 0.24958276748657227, 0.05514020845293999]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9506809115409851, 0.0493190623819828, 0.0, 0.0, 0.0, 0.0], [0.8553215265274048, 0.09256264567375183, 0.05211575701832771, 0.0, 0.0, 0.0], [0.850852370262146, 0.04734604433178902, 0.044177331030368805, 0.057624250650405884, 0.0, 0.0], [0.7697131633758545, 0.02788589708507061, 0.031017286702990532, 0.06842502951622009, 0.1029587835073471, 0.0], [0.7931903004646301, 0.04052198305726051, 0.029242033138871193, 0.04478124529123306, 0.04894689470529556, 0.04331749677658081]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9770310521125793, 0.02296893112361431, 0.0, 0.0, 0.0, 0.0], [0.9429817199707031, 0.017321482300758362, 0.03969680890440941, 0.0, 0.0, 0.0], [0.9144344925880432, 0.008583576418459415, 0.013035810552537441, 0.06394599378108978, 0.0, 0.0], [0.9222429990768433, 0.0036440351977944374, 0.003740275977179408, 0.010410364717245102, 0.05996239185333252, 0.0], [0.9198879599571228, 0.0030822583939880133, 0.0034827394410967827, 0.004206796642392874, 0.02125428058207035, 0.048085976392030716]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.977458119392395, 0.022541873157024384, 0.0, 0.0, 0.0, 0.0], [0.8929325342178345, 0.07475466281175613, 0.032312843948602676, 0.0, 0.0, 0.0], [0.8423511385917664, 0.05980278551578522, 0.03740081936120987, 0.06044524535536766, 0.0, 0.0], [0.7674624919891357, 0.03536349534988403, 0.042155250906944275, 0.06658654659986496, 0.08843226730823517, 0.0], [0.6182611584663391, 0.01611059531569481, 0.020167622715234756, 0.03868892416357994, 0.23147016763687134, 0.07530155777931213]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9634856581687927, 0.036514393985271454, 0.0, 0.0, 0.0, 0.0], [0.4363938570022583, 0.522637128829956, 0.04096902906894684, 0.0, 0.0, 0.0], [0.3608614206314087, 0.35129693150520325, 0.2655103802680969, 0.022331148386001587, 0.0, 0.0], [0.3942921757698059, 0.021704670041799545, 0.07794328778982162, 0.37168896198272705, 0.1343708038330078, 0.0], [0.6310713887214661, 0.01698400266468525, 0.025942081585526466, 0.08615949749946594, 0.2183200567960739, 0.021522950381040573]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9988250136375427, 0.0011750265257433057, 0.0, 0.0, 0.0, 0.0], [0.9944871068000793, 0.0004826401418540627, 0.0050302306190133095, 0.0, 0.0, 0.0], [0.9981209635734558, 2.705173392314464e-05, 0.0001130745149566792, 0.0017389442073181272, 0.0, 0.0], [0.9982239603996277, 6.83655816828832e-05, 0.00010199935059063137, 6.028370262356475e-05, 0.0015453165397047997, 0.0], [0.9982888102531433, 1.055222810464329e-06, 3.2781026675365865e-05, 0.00013038977340329438, 0.0006605894886888564, 0.0008863684488460422]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9936710596084595, 0.006328921765089035, 0.0, 0.0, 0.0, 0.0], [0.9727688431739807, 0.0018561368342489004, 0.025375060737133026, 0.0, 0.0, 0.0], [0.9724299907684326, 0.0019586149137467146, 0.011192461475729942, 0.014418890699744225, 0.0, 0.0], [0.9782041311264038, 0.0009589138207957149, 0.0018706483533605933, 0.006326568778604269, 0.012639678083360195, 0.0], [0.9592596888542175, 0.0024555064737796783, 0.00161241355817765, 0.005019655916839838, 0.006687097251415253, 0.024965662509202957]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9629000425338745, 0.03709998354315758, 0.0, 0.0, 0.0, 0.0], [0.36801934242248535, 0.6152258515357971, 0.016754813492298126, 0.0, 0.0, 0.0], [0.3173511326313019, 0.6140013337135315, 0.05375149846076965, 0.014896026812493801, 0.0, 0.0], [0.48987284302711487, 0.21071474254131317, 0.04693019017577171, 0.20700432360172272, 0.04547784850001335, 0.0], [0.48774227499961853, 0.1769528090953827, 0.06915216147899628, 0.09849268198013306, 0.12091436982154846, 0.046745721250772476]]], [[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9794419407844543, 0.020558049902319908, 0.0, 0.0, 0.0, 0.0], [0.6677903532981873, 0.31032365560531616, 0.021886007860302925, 0.0, 0.0, 0.0], [0.7118757367134094, 0.11108540743589401, 0.14187385141849518, 0.03516504913568497, 0.0, 0.0], [0.4501457214355469, 0.04036055505275726, 0.040458209812641144, 0.388570100069046, 0.08046531677246094, 0.0], [0.49346262216567993, 0.013696977868676186, 0.008126799948513508, 0.13074499368667603, 0.3086138069629669, 0.04535480588674545]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9846054315567017, 0.015394587069749832, 0.0, 0.0, 0.0, 0.0], [0.9806739091873169, 0.007713791914284229, 0.011612347327172756, 0.0, 0.0, 0.0], [0.932663083076477, 0.01957838423550129, 0.02410353161394596, 0.023654978722333908, 0.0, 0.0], [0.9422016739845276, 0.0009538981830701232, 0.0010898025939241052, 0.00319337984547019, 0.05256118252873421, 0.0], [0.9352930784225464, 0.0010279357666149735, 0.004444425459951162, 0.001637140172533691, 0.010590963996946812, 0.04700646549463272]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9985783100128174, 0.0014216724084690213, 0.0, 0.0, 0.0, 0.0], [0.9893348813056946, 0.0011178902350366116, 0.00954714696854353, 0.0, 0.0, 0.0], [0.9979978203773499, 7.997050124686211e-05, 0.00013218850654084235, 0.0017900333041325212, 0.0, 0.0], [0.9986976385116577, 4.1044117097044364e-05, 3.8683547245454974e-06, 2.3676282580709085e-05, 0.0012337174266576767, 0.0], [0.9971563816070557, 1.852225250331685e-05, 1.8826559653462027e-06, 2.7900125132873654e-05, 0.0006533482228405774, 0.0021419788245111704]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9768233299255371, 0.023176640272140503, 0.0, 0.0, 0.0, 0.0], [0.9194678068161011, 0.05088186264038086, 0.029650341719388962, 0.0, 0.0, 0.0], [0.8474554419517517, 0.06100169196724892, 0.04372376948595047, 0.04781914874911308, 0.0, 0.0], [0.8011623620986938, 0.041866958141326904, 0.04375807195901871, 0.041894737631082535, 0.07131782174110413, 0.0], [0.8031871914863586, 0.02450493723154068, 0.017323585227131844, 0.04744395986199379, 0.06109930947422981, 0.046441152691841125]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9829428195953369, 0.01705716922879219, 0.0, 0.0, 0.0, 0.0], [0.8863736987113953, 0.09492647647857666, 0.018699750304222107, 0.0, 0.0, 0.0], [0.9231085777282715, 0.03696346655488014, 0.032198335975408554, 0.007729663979262114, 0.0, 0.0], [0.9068527221679688, 0.016046639531850815, 0.014310522936284542, 0.04543786868453026, 0.017352323979139328, 0.0], [0.6555973887443542, 0.05091019719839096, 0.028384855017066002, 0.1256549060344696, 0.10546853393316269, 0.03398407623171806]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9502318501472473, 0.049768079072237015, 0.0, 0.0, 0.0, 0.0], [0.8829865455627441, 0.1000962108373642, 0.01691717840731144, 0.0, 0.0, 0.0], [0.8057457804679871, 0.14463546872138977, 0.03018922731280327, 0.019429458305239677, 0.0, 0.0], [0.8706230521202087, 0.032440632581710815, 0.026951627805829048, 0.04410304129123688, 0.025881657376885414, 0.0], [0.688364565372467, 0.009681451134383678, 0.016449343413114548, 0.0987110361456871, 0.08971209079027176, 0.09708156436681747]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9792683124542236, 0.02073168195784092, 0.0, 0.0, 0.0, 0.0], [0.9523284435272217, 0.025933818891644478, 0.021737735718488693, 0.0, 0.0, 0.0], [0.9144353270530701, 0.017671240493655205, 0.022358495742082596, 0.04553484544157982, 0.0, 0.0], [0.9448292851448059, 0.006467597559094429, 0.006386063527315855, 0.03263096138834953, 0.00968620739877224, 0.0], [0.9347906112670898, 0.007862505502998829, 0.007788175716996193, 0.021432818844914436, 0.008491144515573978, 0.01963483914732933]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.983370304107666, 0.016629677265882492, 0.0, 0.0, 0.0, 0.0], [0.963111400604248, 0.009229931980371475, 0.027658598497509956, 0.0, 0.0, 0.0], [0.9706628322601318, 0.0041494048200547695, 0.0068131014704704285, 0.018374638631939888, 0.0, 0.0], [0.987951934337616, 0.002165885642170906, 0.00034901127219200134, 0.001583816367201507, 0.00794942770153284, 0.0], [0.9457950592041016, 0.014583553187549114, 0.0003652951563708484, 0.0009569536778144538, 0.013621564954519272, 0.02467755414545536]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9878059029579163, 0.01219407469034195, 0.0, 0.0, 0.0, 0.0], [0.87103670835495, 0.09448163211345673, 0.03448161482810974, 0.0, 0.0, 0.0], [0.6309783458709717, 0.11090382188558578, 0.1923021823167801, 0.06581564992666245, 0.0, 0.0], [0.5360490083694458, 0.04618944972753525, 0.13605308532714844, 0.26455509662628174, 0.017153292894363403, 0.0], [0.8287520408630371, 0.023732755333185196, 0.02008037269115448, 0.07245264202356339, 0.030431220307946205, 0.024550989270210266]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8995685577392578, 0.10043150931596756, 0.0, 0.0, 0.0, 0.0], [0.270343542098999, 0.6504329442977905, 0.07922357320785522, 0.0, 0.0, 0.0], [0.20541730523109436, 0.5892508625984192, 0.18085837364196777, 0.024473490193486214, 0.0, 0.0], [0.5573861002922058, 0.1774134784936905, 0.08806808292865753, 0.09881848096847534, 0.07831384986639023, 0.0], [0.5922912359237671, 0.08700639009475708, 0.05643285810947418, 0.05685883015394211, 0.12181518226861954, 0.08559554070234299]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9316380620002747, 0.06836195290088654, 0.0, 0.0, 0.0, 0.0], [0.9572945833206177, 0.026243582367897034, 0.0164618119597435, 0.0, 0.0, 0.0], [0.9880544543266296, 0.00427332753315568, 0.002954584313556552, 0.004717645235359669, 0.0, 0.0], [0.99403977394104, 0.0009413420339114964, 0.0004739820142276585, 0.00011646930943243206, 0.004428447224199772, 0.0], [0.9806035161018372, 2.5468933017691597e-05, 0.00016239412070717663, 0.0001476418401580304, 0.0013442443450912833, 0.017716845497488976]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.993178129196167, 0.006821857299655676, 0.0, 0.0, 0.0, 0.0], [0.9756524562835693, 0.01318411435931921, 0.011163423769176006, 0.0, 0.0, 0.0], [0.9418966770172119, 0.004721744451671839, 0.0023818055633455515, 0.050999753177165985, 0.0, 0.0], [0.9905040860176086, 0.0022848136723041534, 6.198462506290525e-05, 0.0005984465242363513, 0.006550676189363003, 0.0], [0.9697660207748413, 0.0008878845837898552, 0.00023466735729016364, 0.0017040816601365805, 0.004128355998545885, 0.02327893301844597]]], [[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9716231822967529, 0.02837684564292431, 0.0, 0.0, 0.0, 0.0], [0.9223619699478149, 0.028907248750329018, 0.048730745911598206, 0.0, 0.0, 0.0], [0.8426317572593689, 0.023872116580605507, 0.04748132824897766, 0.08601479232311249, 0.0, 0.0], [0.8521121740341187, 0.020744236186146736, 0.04494619369506836, 0.05765002593398094, 0.02454746514558792, 0.0], [0.8800725936889648, 0.022448532283306122, 0.018235722556710243, 0.01925482600927353, 0.015854258090257645, 0.044134121388196945]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9412723779678345, 0.058727629482746124, 0.0, 0.0, 0.0, 0.0], [0.916313886642456, 0.05759201943874359, 0.02609400637447834, 0.0, 0.0, 0.0], [0.8392423391342163, 0.057690516114234924, 0.01382902916520834, 0.08923812955617905, 0.0, 0.0], [0.8987162113189697, 0.0134778693318367, 0.0003456450067460537, 0.003298751311376691, 0.08416149020195007, 0.0], [0.8701692223548889, 0.002700856188312173, 0.00143499206751585, 0.0056661744602024555, 0.08874300867319107, 0.031285665929317474]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9656725525856018, 0.03432750701904297, 0.0, 0.0, 0.0, 0.0], [0.9178615808486938, 0.062257930636405945, 0.019880469888448715, 0.0, 0.0, 0.0], [0.823314905166626, 0.06282395124435425, 0.03670429438352585, 0.07715693861246109, 0.0, 0.0], [0.8501748442649841, 0.03816927224397659, 0.03196492791175842, 0.0516013503074646, 0.02808968350291252, 0.0], [0.6572404503822327, 0.05877397954463959, 0.04336007311940193, 0.09013211727142334, 0.08146599680185318, 0.06902744621038437]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9162061810493469, 0.0837937667965889, 0.0, 0.0, 0.0, 0.0], [0.9451773762702942, 0.04099284112453461, 0.013829832896590233, 0.0, 0.0, 0.0], [0.8928355574607849, 0.05368670076131821, 0.017596954479813576, 0.03588071092963219, 0.0, 0.0], [0.8337052464485168, 0.04799601063132286, 0.033513229340314865, 0.04680858924984932, 0.03797686845064163, 0.0], [0.8167192339897156, 0.06337132304906845, 0.013286277651786804, 0.020469767972826958, 0.025292355567216873, 0.06086111441254616]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9525133371353149, 0.04748663306236267, 0.0, 0.0, 0.0, 0.0], [0.3019869327545166, 0.6520938873291016, 0.04591925069689751, 0.0, 0.0, 0.0], [0.285582959651947, 0.556952178478241, 0.1444743126630783, 0.012990524061024189, 0.0, 0.0], [0.843804121017456, 0.032251205295324326, 0.03954290598630905, 0.06848159432411194, 0.015920041128993034, 0.0], [0.6664940714836121, 0.06095913052558899, 0.04064354673027992, 0.06804485619068146, 0.09186329692602158, 0.07199501991271973]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9682655334472656, 0.031734466552734375, 0.0, 0.0, 0.0, 0.0], [0.738521933555603, 0.22856839001178741, 0.032909639179706573, 0.0, 0.0, 0.0], [0.5946676135063171, 0.2303314357995987, 0.14867636561393738, 0.02632458508014679, 0.0, 0.0], [0.6339254975318909, 0.05813034623861313, 0.09654320776462555, 0.14291946589946747, 0.06848153471946716, 0.0], [0.40375572443008423, 0.08945391327142715, 0.07635112851858139, 0.25587135553359985, 0.1433039754629135, 0.03126389905810356]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9869793653488159, 0.013020593672990799, 0.0, 0.0, 0.0, 0.0], [0.8631385564804077, 0.1105666309595108, 0.02629482001066208, 0.0, 0.0, 0.0], [0.9488080143928528, 0.028614996001124382, 0.006535546388477087, 0.016041526570916176, 0.0, 0.0], [0.9672170877456665, 0.006604980677366257, 0.00045171406236477196, 0.004844417329877615, 0.020881708711385727, 0.0], [0.9354621171951294, 0.02047806605696678, 0.0011700231116265059, 0.007056943140923977, 0.0163181871175766, 0.019514625892043114]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9846673011779785, 0.015332723967730999, 0.0, 0.0, 0.0, 0.0], [0.9052747488021851, 0.08373606950044632, 0.010989243164658546, 0.0, 0.0, 0.0], [0.8145939111709595, 0.04283742979168892, 0.10568301379680634, 0.03688570484519005, 0.0, 0.0], [0.23519809544086456, 0.012018457986414433, 0.05280117318034172, 0.6516180038452148, 0.04836418479681015, 0.0], [0.31818512082099915, 0.018632443621754646, 0.03948190063238144, 0.3755541741847992, 0.20787373185157776, 0.04027257487177849]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9811733365058899, 0.018826685845851898, 0.0, 0.0, 0.0, 0.0], [0.8618939518928528, 0.06479164958000183, 0.07331438362598419, 0.0, 0.0, 0.0], [0.7664540410041809, 0.07330425828695297, 0.10353513062000275, 0.056706514209508896, 0.0, 0.0], [0.8128499984741211, 0.03215480223298073, 0.059005625545978546, 0.05416511744260788, 0.04182446748018265, 0.0], [0.8687856197357178, 0.026987861841917038, 0.02047000452876091, 0.01629738137125969, 0.03218390792608261, 0.03527523949742317]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9264583587646484, 0.07354167848825455, 0.0, 0.0, 0.0, 0.0], [0.8403540849685669, 0.06373751163482666, 0.09590838104486465, 0.0, 0.0, 0.0], [0.7330995798110962, 0.06451118737459183, 0.10380073636770248, 0.09858842939138412, 0.0, 0.0], [0.9143612384796143, 0.008257776498794556, 0.007320381235331297, 0.017966248095035553, 0.05209439620375633, 0.0], [0.8971915245056152, 0.008555498905479908, 0.007019453682005405, 0.014860544353723526, 0.03399762138724327, 0.03837529569864273]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9180347919464111, 0.08196526020765305, 0.0, 0.0, 0.0, 0.0], [0.8328666687011719, 0.1219901517033577, 0.04514322429895401, 0.0, 0.0, 0.0], [0.7994157075881958, 0.0874413549900055, 0.03605784848332405, 0.07708510011434555, 0.0, 0.0], [0.880984902381897, 0.020749641582369804, 0.020554615184664726, 0.017120830714702606, 0.06058995798230171, 0.0], [0.745303213596344, 0.044334057718515396, 0.022549288347363472, 0.0331527441740036, 0.03357058763504028, 0.12109009176492691]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9867060780525208, 0.013293893076479435, 0.0, 0.0, 0.0, 0.0], [0.982177734375, 0.012414131313562393, 0.005408108700066805, 0.0, 0.0, 0.0], [0.9630486369132996, 0.015290752984583378, 0.010345698334276676, 0.0113149369135499, 0.0, 0.0], [0.9213568568229675, 0.014132463373243809, 0.017639216035604477, 0.016567690297961235, 0.030303770676255226, 0.0], [0.9373326301574707, 0.009064299054443836, 0.007548365276306868, 0.006576443091034889, 0.011827622540295124, 0.027650514617562294]]], [[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9951004385948181, 0.00489962799474597, 0.0, 0.0, 0.0, 0.0], [0.9476007223129272, 0.041407931596040726, 0.010991275310516357, 0.0, 0.0, 0.0], [0.9142175316810608, 0.023523783311247826, 0.039145033806562424, 0.023113621398806572, 0.0, 0.0], [0.9534738659858704, 0.008932933211326599, 0.015272765420377254, 0.007908251136541367, 0.014412266202270985, 0.0], [0.9427101016044617, 0.00823307130485773, 0.004650997929275036, 0.004178107250481844, 0.005463531706482172, 0.03476419299840927]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9543376564979553, 0.045662373304367065, 0.0, 0.0, 0.0, 0.0], [0.9696040749549866, 0.01954760029911995, 0.01084828469902277, 0.0, 0.0, 0.0], [0.9710449576377869, 0.012425386346876621, 0.008068876340985298, 0.008460716344416142, 0.0, 0.0], [0.9726192951202393, 0.002697656163945794, 0.00044831327977590263, 0.0013814778067171574, 0.022853154689073563, 0.0], [0.9675466418266296, 0.009613442234694958, 0.003203035332262516, 0.00424883933737874, 0.007442260626703501, 0.00794589426368475]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9887008666992188, 0.011299116536974907, 0.0, 0.0, 0.0, 0.0], [0.9382632374763489, 0.04204244911670685, 0.019694412127137184, 0.0, 0.0, 0.0], [0.8351995944976807, 0.03487853705883026, 0.05134471505880356, 0.07857715338468552, 0.0, 0.0], [0.9042676687240601, 0.010541575029492378, 0.016426723450422287, 0.025921987369656563, 0.04284200444817543, 0.0], [0.8913140892982483, 0.00891267228871584, 0.005010711494833231, 0.008175632916390896, 0.013514749705791473, 0.07307209819555283]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8693912029266357, 0.13060881197452545, 0.0, 0.0, 0.0, 0.0], [0.3507988452911377, 0.606351912021637, 0.04284917935729027, 0.0, 0.0, 0.0], [0.35475659370422363, 0.3502019941806793, 0.24722407758235931, 0.04781729355454445, 0.0, 0.0], [0.35370609164237976, 0.03527737781405449, 0.09567111730575562, 0.449796199798584, 0.06554921716451645, 0.0], [0.4132595360279083, 0.09055527299642563, 0.05286579951643944, 0.174679696559906, 0.173848956823349, 0.09479076415300369]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9629756212234497, 0.037024300545454025, 0.0, 0.0, 0.0, 0.0], [0.9756426811218262, 0.01965854875743389, 0.004698706325143576, 0.0, 0.0, 0.0], [0.9775736927986145, 0.013286248780786991, 0.0025590297300368547, 0.006581062916666269, 0.0, 0.0], [0.9870142936706543, 0.007388236932456493, 0.0009579154429957271, 0.0018318220973014832, 0.0028077505994588137, 0.0], [0.9409245848655701, 0.016633737832307816, 0.0022979143541306257, 0.0058906711637973785, 0.0055129327811300755, 0.02874022163450718]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.962827205657959, 0.037172831594944, 0.0, 0.0, 0.0, 0.0], [0.9582237601280212, 0.024641817435622215, 0.017134377732872963, 0.0, 0.0, 0.0], [0.9351300001144409, 0.015331573784351349, 0.014810982160270214, 0.034727465361356735, 0.0, 0.0], [0.9225171208381653, 0.010528750717639923, 0.011010154150426388, 0.01944003626704216, 0.036503832787275314, 0.0], [0.8420165777206421, 0.04357199743390083, 0.007488282397389412, 0.01496153138577938, 0.02385285682976246, 0.06810864061117172]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9926387071609497, 0.00736132962629199, 0.0, 0.0, 0.0, 0.0], [0.9957393407821655, 0.0033469819463789463, 0.000913690309971571, 0.0, 0.0, 0.0], [0.9869900345802307, 0.001974786864593625, 0.001524551771581173, 0.009510699659585953, 0.0, 0.0], [0.9933527708053589, 0.001020324882119894, 0.00034337223041802645, 0.0010291127255186439, 0.004254369530826807, 0.0], [0.9749016761779785, 0.00043480272870510817, 0.0004306558985263109, 0.0012364407302811742, 0.0015347707085311413, 0.021461669355630875]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9897475242614746, 0.010252462700009346, 0.0, 0.0, 0.0, 0.0], [0.9790639281272888, 0.01650906540453434, 0.0044270907528698444, 0.0, 0.0, 0.0], [0.9521436095237732, 0.029432358220219612, 0.008943161927163601, 0.009480923414230347, 0.0, 0.0], [0.939594030380249, 0.021510960534214973, 0.010278552770614624, 0.004555229097604752, 0.024061163887381554, 0.0], [0.9205074906349182, 0.016153652220964432, 0.010818594135344028, 0.01664440892636776, 0.014566398225724697, 0.021309375762939453]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9898501634597778, 0.010149780660867691, 0.0, 0.0, 0.0, 0.0], [0.9820910096168518, 0.006907520350068808, 0.011001535691320896, 0.0, 0.0, 0.0], [0.9684997200965881, 0.008987602777779102, 0.015342563390731812, 0.007170087192207575, 0.0, 0.0], [0.9274120330810547, 0.009485266171395779, 0.022066107019782066, 0.03222890570759773, 0.008807653561234474, 0.0], [0.900665819644928, 0.021623756736516953, 0.013808279298245907, 0.009843860752880573, 0.008521373383700848, 0.04553695768117905]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9954444169998169, 0.004555588588118553, 0.0, 0.0, 0.0, 0.0], [0.995254397392273, 0.002460238989442587, 0.002285485854372382, 0.0, 0.0, 0.0], [0.9862446188926697, 0.0015168144600465894, 0.004072288051247597, 0.008166354149580002, 0.0, 0.0], [0.9889963865280151, 0.001226040069013834, 0.0007996349013410509, 0.0006774227367714047, 0.008300574496388435, 0.0], [0.9865202903747559, 0.00039427157025784254, 0.0009571771952323616, 0.0004954367759637535, 0.0009604979422874749, 0.010672281496226788]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9821295142173767, 0.017870500683784485, 0.0, 0.0, 0.0, 0.0], [0.7489436268806458, 0.22002726793289185, 0.031029189005494118, 0.0, 0.0, 0.0], [0.28547799587249756, 0.21125678718090057, 0.47871601581573486, 0.024549242109060287, 0.0, 0.0], [0.8056644201278687, 0.026974644511938095, 0.04302806034684181, 0.06993705034255981, 0.05439583212137222, 0.0], [0.3307209014892578, 0.022326624020934105, 0.016627125442028046, 0.08019453287124634, 0.41574832797050476, 0.13438253104686737]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9697746634483337, 0.030225319787859917, 0.0, 0.0, 0.0, 0.0], [0.9800565838813782, 0.015018894337117672, 0.004924521781504154, 0.0, 0.0, 0.0], [0.9237861037254333, 0.052764780819416046, 0.00630240747705102, 0.017146753147244453, 0.0, 0.0], [0.9451844096183777, 0.03618047758936882, 0.001989208161830902, 0.003958724904805422, 0.012687299400568008, 0.0], [0.9633325934410095, 0.018662991002202034, 0.0030418417882174253, 0.007070912979543209, 0.0050094155594706535, 0.002882065251469612]]], [[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9873244762420654, 0.012675459496676922, 0.0, 0.0, 0.0, 0.0], [0.9904569983482361, 0.0055419523268938065, 0.004001122899353504, 0.0, 0.0, 0.0], [0.9814971685409546, 0.004653455223888159, 0.003725277027115226, 0.010124054737389088, 0.0, 0.0], [0.9744365811347961, 0.004632251337170601, 0.002379992976784706, 0.006518087349832058, 0.012033028528094292, 0.0], [0.9624497294425964, 0.0033743639942258596, 0.0013198587112128735, 0.0017275003483518958, 0.002944675739854574, 0.028183799237012863]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9807674288749695, 0.01923258602619171, 0.0, 0.0, 0.0, 0.0], [0.9664245843887329, 0.015413926914334297, 0.018161438405513763, 0.0, 0.0, 0.0], [0.9632682204246521, 0.004538117907941341, 0.002925391308963299, 0.029268190264701843, 0.0, 0.0], [0.9562349319458008, 0.0012223608791828156, 0.0005304080550558865, 0.00867149606347084, 0.03334089741110802, 0.0], [0.9657101035118103, 0.0009808284230530262, 0.0016686266753822565, 0.002634831238538027, 0.005866361316293478, 0.023139292374253273]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9639716148376465, 0.036028459668159485, 0.0, 0.0, 0.0, 0.0], [0.9562800526618958, 0.03373315557837486, 0.009986846707761288, 0.0, 0.0, 0.0], [0.8539998531341553, 0.08073022216558456, 0.03334445133805275, 0.031925540417432785, 0.0, 0.0], [0.9547491073608398, 0.009605025872588158, 0.004146162886172533, 0.0020133228972554207, 0.029486361891031265, 0.0], [0.9331137537956238, 0.028699662536382675, 0.005477475933730602, 0.006368075497448444, 0.012613046914339066, 0.013728085905313492]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9392993450164795, 0.06070063263177872, 0.0, 0.0, 0.0, 0.0], [0.9298391342163086, 0.061895377933979034, 0.008265496231615543, 0.0, 0.0, 0.0], [0.8471823334693909, 0.09035038203001022, 0.01763608679175377, 0.044831156730651855, 0.0, 0.0], [0.8857703804969788, 0.03918175399303436, 0.007867704145610332, 0.02276589721441269, 0.04441439360380173, 0.0], [0.8563280701637268, 0.10088995099067688, 0.006531452294439077, 0.008485927246510983, 0.007368441205471754, 0.020396249368786812]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8353264331817627, 0.1646735519170761, 0.0, 0.0, 0.0, 0.0], [0.6160858869552612, 0.3137648403644562, 0.07014927268028259, 0.0, 0.0, 0.0], [0.34316325187683105, 0.2758493721485138, 0.1196604073047638, 0.26132699847221375, 0.0, 0.0], [0.5908172130584717, 0.050290752202272415, 0.041665926575660706, 0.2199493646621704, 0.0972767099738121, 0.0], [0.8481413125991821, 0.06318090111017227, 0.014733693562448025, 0.055267371237277985, 0.00901501253247261, 0.009661628864705563]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9627319574356079, 0.03726799786090851, 0.0, 0.0, 0.0, 0.0], [0.7757522463798523, 0.1799626499414444, 0.044285036623477936, 0.0, 0.0, 0.0], [0.6317060589790344, 0.24380716681480408, 0.10925652086734772, 0.015230235643684864, 0.0, 0.0], [0.9539909958839417, 0.018182311207056046, 0.011601822450757027, 0.012299076654016972, 0.003925766795873642, 0.0], [0.40356943011283875, 0.14237558841705322, 0.05661217123270035, 0.1975736767053604, 0.0929921343922615, 0.10687707364559174]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9802619218826294, 0.019738124683499336, 0.0, 0.0, 0.0, 0.0], [0.9873908162117004, 0.007800452411174774, 0.004808681085705757, 0.0, 0.0, 0.0], [0.9283918738365173, 0.008301235735416412, 0.01330565195530653, 0.05000120773911476, 0.0, 0.0], [0.8981055021286011, 0.015591299161314964, 0.010177576914429665, 0.039987027645111084, 0.0361386202275753, 0.0], [0.9753499031066895, 0.00035433052107691765, 0.0005866039427928627, 0.0011877501383423805, 0.0010750899091362953, 0.021446440368890762]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9295330047607422, 0.07046692818403244, 0.0, 0.0, 0.0, 0.0], [0.9361506104469299, 0.04116682708263397, 0.022682538256049156, 0.0, 0.0, 0.0], [0.8486821055412292, 0.05802798643708229, 0.024856165051460266, 0.0684337466955185, 0.0, 0.0], [0.8661180734634399, 0.02232467755675316, 0.010369130410254002, 0.02600197121500969, 0.07518619298934937, 0.0], [0.8074421882629395, 0.044382549822330475, 0.01849711686372757, 0.03357789292931557, 0.018561245873570442, 0.07753907144069672]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9680535197257996, 0.03194643557071686, 0.0, 0.0, 0.0, 0.0], [0.9693689942359924, 0.02568492479622364, 0.004946070723235607, 0.0, 0.0, 0.0], [0.9620568156242371, 0.022552406415343285, 0.005471326876431704, 0.009919456206262112, 0.0, 0.0], [0.9727528095245361, 0.010137127712368965, 0.000757327419705689, 0.0028828983195126057, 0.013469807803630829, 0.0], [0.9624635577201843, 0.0031109037809073925, 0.0010007602395489812, 0.0019475930603221059, 0.008266227319836617, 0.02321087196469307]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8542501330375671, 0.14574992656707764, 0.0, 0.0, 0.0, 0.0], [0.9725967645645142, 0.014116315171122551, 0.01328685600310564, 0.0, 0.0, 0.0], [0.9257621765136719, 0.03257262706756592, 0.01461210660636425, 0.027053095400333405, 0.0, 0.0], [0.7923423051834106, 0.027305101975798607, 0.01880674995481968, 0.13854165375232697, 0.023004096001386642, 0.0], [0.6152060627937317, 0.02665526419878006, 0.029352931305766106, 0.05590886250138283, 0.11611279845237732, 0.15676409006118774]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9804654121398926, 0.019534552469849586, 0.0, 0.0, 0.0, 0.0], [0.9882452487945557, 0.007509466726332903, 0.004245325922966003, 0.0, 0.0, 0.0], [0.9584206938743591, 0.0109635591506958, 0.010456060990691185, 0.020159708335995674, 0.0, 0.0], [0.9604811668395996, 0.007182627450674772, 0.003072339342907071, 0.006898913532495499, 0.02236509881913662, 0.0], [0.966888964176178, 0.0032812939025461674, 0.00550054432824254, 0.004234083462506533, 0.005038043484091759, 0.015057181939482689]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9498194456100464, 0.05018055811524391, 0.0, 0.0, 0.0, 0.0], [0.9781363606452942, 0.016430046409368515, 0.0054335566237568855, 0.0, 0.0, 0.0], [0.8618696331977844, 0.036093585193157196, 0.07555554062128067, 0.026481209322810173, 0.0, 0.0], [0.5449837446212769, 0.015411133877933025, 0.023516526445746422, 0.25743600726127625, 0.15865260362625122, 0.0], [0.9571874737739563, 0.0030803855042904615, 0.0014446862041950226, 0.006861559115350246, 0.014818714000284672, 0.01660723052918911]]], [[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6156560778617859, 0.3843439519405365, 0.0, 0.0, 0.0, 0.0], [0.36760634183883667, 0.42816370725631714, 0.20423001050949097, 0.0, 0.0, 0.0], [0.16471554338932037, 0.4136792719364166, 0.2509237229824066, 0.17068152129650116, 0.0, 0.0], [0.4184456169605255, 0.1524762362241745, 0.10305401682853699, 0.11071498692035675, 0.21530911326408386, 0.0], [0.19686934351921082, 0.2014620453119278, 0.12827259302139282, 0.09203246980905533, 0.09167550504207611, 0.2896881103515625]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9027364253997803, 0.09726352989673615, 0.0, 0.0, 0.0, 0.0], [0.9736634492874146, 0.014004302211105824, 0.01233230996876955, 0.0, 0.0, 0.0], [0.8504456281661987, 0.05690572410821915, 0.032060906291007996, 0.06058764085173607, 0.0, 0.0], [0.7661210298538208, 0.03530392050743103, 0.03433045372366905, 0.09675204753875732, 0.06749245524406433, 0.0], [0.8650374412536621, 0.020085260272026062, 0.01149806659668684, 0.01855834573507309, 0.018430285155773163, 0.06639053672552109]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9653082489967346, 0.03469168767333031, 0.0, 0.0, 0.0, 0.0], [0.9816323518753052, 0.014176066033542156, 0.004191514104604721, 0.0, 0.0, 0.0], [0.9275256395339966, 0.04737218841910362, 0.01152826938778162, 0.013573966920375824, 0.0, 0.0], [0.9293117523193359, 0.025833239778876305, 0.007227106485515833, 0.014300585724413395, 0.02332727052271366, 0.0], [0.8895062804222107, 0.04689619690179825, 0.0047171092592179775, 0.006286581978201866, 0.00609014043584466, 0.04650374501943588]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8938026428222656, 0.10619727522134781, 0.0, 0.0, 0.0, 0.0], [0.8221707940101624, 0.06304481625556946, 0.11478441953659058, 0.0, 0.0, 0.0], [0.5047380924224854, 0.15375731885433197, 0.2277037501335144, 0.11380083113908768, 0.0, 0.0], [0.4082071781158447, 0.09066355973482132, 0.11696872115135193, 0.24553199112415314, 0.13862857222557068, 0.0], [0.7291035652160645, 0.06638889014720917, 0.023112818598747253, 0.031103096902370453, 0.057143256068229675, 0.09314827620983124]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9247531890869141, 0.07524678111076355, 0.0, 0.0, 0.0, 0.0], [0.8957376480102539, 0.06989553570747375, 0.03436679765582085, 0.0, 0.0, 0.0], [0.7924937605857849, 0.0960114598274231, 0.05509118735790253, 0.056403566151857376, 0.0, 0.0], [0.7891505360603333, 0.07880303263664246, 0.03840155899524689, 0.05396979674696922, 0.03967496380209923, 0.0], [0.7807856798171997, 0.0799354612827301, 0.042531758546829224, 0.03234211727976799, 0.0178169384598732, 0.046588052064180374]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9480886459350586, 0.05191127583384514, 0.0, 0.0, 0.0, 0.0], [0.863694965839386, 0.04756204038858414, 0.08874296396970749, 0.0, 0.0, 0.0], [0.9341371059417725, 0.022224076092243195, 0.022624483332037926, 0.021014342084527016, 0.0, 0.0], [0.9588143229484558, 0.008020909503102303, 0.004490078426897526, 0.005862293299287558, 0.022812429815530777, 0.0], [0.9385918378829956, 0.021227721124887466, 0.0048724692314863205, 0.010940189473330975, 0.009524582885205746, 0.014843451790511608]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9763734340667725, 0.023626558482646942, 0.0, 0.0, 0.0, 0.0], [0.9884802103042603, 0.005189393647015095, 0.0063303736969828606, 0.0, 0.0, 0.0], [0.9477092027664185, 0.0179851483553648, 0.010156610049307346, 0.024149026721715927, 0.0, 0.0], [0.967192530632019, 0.006552813574671745, 0.0033227826934307814, 0.00556332478299737, 0.017368387430906296, 0.0], [0.9584562182426453, 0.007502961438149214, 0.0051363310776650906, 0.008071648888289928, 0.005997124593704939, 0.014835843816399574]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.884070873260498, 0.11592914164066315, 0.0, 0.0, 0.0, 0.0], [0.9931254386901855, 0.005070806015282869, 0.0018038019770756364, 0.0, 0.0, 0.0], [0.9534159302711487, 0.02382904477417469, 0.007748977281153202, 0.015006075613200665, 0.0, 0.0], [0.9151289463043213, 0.010873105376958847, 0.013190957717597485, 0.011050421744585037, 0.04975655674934387, 0.0], [0.8769673109054565, 0.03385210782289505, 0.00848648976534605, 0.009969149716198444, 0.03468578681349754, 0.036039214581251144]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0003709519514814019, 0.999629020690918, 0.0, 0.0, 0.0, 0.0], [6.525027856696397e-05, 0.3737829029560089, 0.6261518597602844, 0.0, 0.0, 0.0], [4.606018774211407e-05, 0.210508793592453, 0.4115968942642212, 0.3778482675552368, 0.0, 0.0], [4.753069515572861e-05, 0.11616954207420349, 0.23264272511005402, 0.3985331058502197, 0.2526070475578308, 0.0], [1.247641534973809e-06, 0.14819711446762085, 0.15813173353672028, 0.30074331164360046, 0.11939018964767456, 0.27353641390800476]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.971555769443512, 0.028444187715649605, 0.0, 0.0, 0.0, 0.0], [0.9529065489768982, 0.03233075141906738, 0.014762768521904945, 0.0, 0.0, 0.0], [0.9343128204345703, 0.02351292595267296, 0.02049802988767624, 0.021676240488886833, 0.0, 0.0], [0.9529678225517273, 0.00855141133069992, 0.004359325394034386, 0.008064556866884232, 0.026056913658976555, 0.0], [0.9653593897819519, 0.008487647399306297, 0.003499280195683241, 0.002721576252952218, 0.0032828773837536573, 0.016649367287755013]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8630780577659607, 0.13692188262939453, 0.0, 0.0, 0.0, 0.0], [0.7696157097816467, 0.0851333811879158, 0.14525099098682404, 0.0, 0.0, 0.0], [0.7133337259292603, 0.10170899331569672, 0.11931268870830536, 0.06564456224441528, 0.0, 0.0], [0.7186222076416016, 0.05444284901022911, 0.01386815495789051, 0.07808027416467667, 0.13498654961585999, 0.0], [0.7990148663520813, 0.05805593729019165, 0.009447019547224045, 0.017770467326045036, 0.02113853208720684, 0.09457314014434814]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9518988728523254, 0.048101115971803665, 0.0, 0.0, 0.0, 0.0], [0.8580653071403503, 0.02944577857851982, 0.11248888075351715, 0.0, 0.0, 0.0], [0.6577738523483276, 0.08513449877500534, 0.1261308640241623, 0.1309608370065689, 0.0, 0.0], [0.8087368607521057, 0.0323016420006752, 0.01841817982494831, 0.06856140494346619, 0.07198194414377213, 0.0], [0.6683295965194702, 0.13281384110450745, 0.021880635991692543, 0.02787741646170616, 0.04923408478498459, 0.0998644009232521]]]], \"left_text\": [\"No\", \",\", \" I\", \" am\", \" your\", \" father\"], \"right_text\": [\"No\", \",\", \" I\", \" am\", \" your\", \" father\"]}], \"default_filter\": \"0\", \"display_mode\": \"dark\", \"root_div_id\": \"bertviz-8e96ccb089214e25aab782e3d2695e5e\", \"include_layers\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], \"include_heads\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], \"total_heads\": 12} is a template marker that is replaced by actual params.\n",
       "        const config = {};\n",
       "\n",
       "        const MIN_X = 0;\n",
       "        const MIN_Y = 0;\n",
       "        const DIV_WIDTH = 970;\n",
       "        const THUMBNAIL_PADDING = 5;\n",
       "        const DETAIL_WIDTH = 300;\n",
       "        const DETAIL_ATTENTION_WIDTH = 140;\n",
       "        const DETAIL_BOX_WIDTH = 80;\n",
       "        const DETAIL_BOX_HEIGHT = 18;\n",
       "        const DETAIL_PADDING = 15;\n",
       "        const ATTN_PADDING = 0;\n",
       "        const DETAIL_HEADING_HEIGHT = 25;\n",
       "        const HEADING_TEXT_SIZE = 15;\n",
       "        const HEADING_PADDING = 5;\n",
       "        const TEXT_SIZE = 13;\n",
       "        const TEXT_PADDING = 5;\n",
       "        const LAYER_COLORS = d3.schemeCategory10;\n",
       "        const PALETTE = {\n",
       "            'light': {\n",
       "                'text': 'black',\n",
       "                'background': 'white',\n",
       "                'highlight': '#F5F5F5'\n",
       "            },\n",
       "            'dark': {\n",
       "                'text': '#ccc',\n",
       "                'background': 'black',\n",
       "                'highlight': '#222'\n",
       "            }\n",
       "        }\n",
       "\n",
       "        function render() {\n",
       "\n",
       "            // Set global state variables\n",
       "\n",
       "            var attData = config.attention[config.filter];\n",
       "            config.leftText = attData.left_text;\n",
       "            config.rightText = attData.right_text;\n",
       "            config.attn = attData.attn;\n",
       "            config.numLayers = config.attn.length;\n",
       "            config.numHeads = config.attn[0].length;\n",
       "            config.thumbnailBoxHeight = 7 * (12 / config.totalHeads);\n",
       "            const axisSize = HEADING_TEXT_SIZE + HEADING_PADDING + TEXT_SIZE + TEXT_PADDING;\n",
       "            config.thumbnailHeight = Math.max(config.leftText.length, config.rightText.length) * config.thumbnailBoxHeight + 2 * THUMBNAIL_PADDING;\n",
       "            config.thumbnailWidth = (DIV_WIDTH - axisSize) / config.totalHeads;\n",
       "            config.detailHeight = Math.max(config.leftText.length, config.rightText.length) * DETAIL_BOX_HEIGHT + 2 * DETAIL_PADDING + DETAIL_HEADING_HEIGHT;\n",
       "            config.divHeight = Math.max(config.numLayers * config.thumbnailHeight + axisSize, config.detailHeight);\n",
       "\n",
       "            const vis = $(`#${config.rootDivId} #vis`)\n",
       "            vis.empty();\n",
       "            vis.attr(\"height\", config.divHeight);\n",
       "            config.svg = d3.select(`#${config.rootDivId} #vis`)\n",
       "                .append('svg')\n",
       "                .attr(\"width\", DIV_WIDTH)\n",
       "                .attr(\"height\", config.divHeight)\n",
       "                .attr(\"fill\", getBackgroundColor());\n",
       "\n",
       "            renderAxisLabels();\n",
       "\n",
       "            var i;\n",
       "            var j;\n",
       "            for (i = 0; i < config.numLayers; i++) {\n",
       "                for (j = 0; j < config.numHeads; j++) {\n",
       "                    renderThumbnail(i, j);\n",
       "                }\n",
       "            }\n",
       "        }\n",
       "\n",
       "        function renderAxisLabels() {\n",
       "            const axisSize = HEADING_TEXT_SIZE + HEADING_PADDING + TEXT_SIZE + TEXT_PADDING;\n",
       "            const tableWidth = config.thumbnailWidth * config.heads.length;\n",
       "            config.svg.append(\"text\")\n",
       "                .text(\"Heads\")\n",
       "                .attr(\"fill\", \"black\")\n",
       "                .attr(\"font-weight\", \"bold\")\n",
       "                .attr(\"font-size\", HEADING_TEXT_SIZE + \"px\")\n",
       "                .attr(\"x\", axisSize + tableWidth / 2)\n",
       "                .attr(\"text-anchor\", \"middle\")\n",
       "                .attr(\"y\", 0)\n",
       "                .attr(\"dy\", HEADING_TEXT_SIZE);\n",
       "            for (let i = 0; i < config.numHeads; i++) {\n",
       "                config.svg.append(\"text\")\n",
       "                    .text(config.heads[i])\n",
       "                    .attr(\"fill\", \"black\")\n",
       "                    .attr(\"font-size\", TEXT_SIZE + \"px\")\n",
       "                    .attr(\"x\", axisSize + (i + .5) * config.thumbnailWidth)\n",
       "                    .attr(\"text-anchor\", \"middle\")\n",
       "                    .attr(\"y\", HEADING_TEXT_SIZE + HEADING_PADDING)\n",
       "                    .attr(\"dy\", TEXT_SIZE);\n",
       "            }\n",
       "            let x = 0;\n",
       "            let y = axisSize + config.thumbnailHeight * config.layers.length / 2;\n",
       "            console.log(\"x\", x, y)\n",
       "            config.svg.append(\"text\")\n",
       "                .text(\"Layers\")\n",
       "                .attr(\"fill\", \"black\")\n",
       "                .attr(\"font-weight\", \"bold\")\n",
       "                .attr(\"transform\", \"rotate(270, \" + x  + \", \" + y + \")\")\n",
       "                .attr(\"font-size\", HEADING_TEXT_SIZE + \"px\")\n",
       "                .attr(\"x\", x)\n",
       "                .attr(\"text-anchor\", \"middle\")\n",
       "                .attr(\"y\", y)\n",
       "                .attr(\"dy\", HEADING_TEXT_SIZE);\n",
       "            for (let i = 0; i < config.numLayers; i++) {\n",
       "                x = HEADING_TEXT_SIZE + HEADING_PADDING + TEXT_SIZE; // HACK\n",
       "                y = axisSize + (i + .5) * config.thumbnailHeight;\n",
       "                config.svg.append(\"text\")\n",
       "                    .text(config.layers[i])\n",
       "                    .attr(\"fill\", \"black\")\n",
       "                    .attr(\"font-size\", TEXT_SIZE + \"px\")\n",
       "                    .attr(\"x\", x)\n",
       "                    .attr(\"text-anchor\", \"end\")\n",
       "                    .attr(\"y\", y)\n",
       "                    .attr(\"dy\", TEXT_SIZE / 2);\n",
       "            }\n",
       "        }\n",
       "\n",
       "\n",
       "        function renderThumbnail(layerIndex, headIndex) {\n",
       "            const axisSize = HEADING_TEXT_SIZE + HEADING_PADDING + TEXT_SIZE + TEXT_PADDING\n",
       "            const x = headIndex * config.thumbnailWidth + axisSize;\n",
       "            const y = layerIndex * config.thumbnailHeight + axisSize;\n",
       "            renderThumbnailAttn(x, y, config.attn[layerIndex][headIndex], layerIndex, headIndex);\n",
       "        }\n",
       "\n",
       "        function renderDetail(att, layerIndex, headIndex) {\n",
       "            const axisSize = TEXT_SIZE + HEADING_PADDING + TEXT_SIZE + TEXT_PADDING;\n",
       "            var xOffset = .8 * config.thumbnailWidth;\n",
       "            var maxX = DIV_WIDTH;\n",
       "            var maxY = config.divHeight - 3;\n",
       "            var leftPos = axisSize + headIndex * config.thumbnailWidth;\n",
       "            var x = leftPos + THUMBNAIL_PADDING + xOffset;\n",
       "            if (x < MIN_X) {\n",
       "                x = MIN_X;\n",
       "            } else if (x + DETAIL_WIDTH > maxX) {\n",
       "                x = leftPos + THUMBNAIL_PADDING - DETAIL_WIDTH + 8;\n",
       "            }\n",
       "            var posLeftText = x;\n",
       "            var posAttention = posLeftText + DETAIL_BOX_WIDTH;\n",
       "            var posRightText = posAttention + DETAIL_ATTENTION_WIDTH;\n",
       "            var thumbnailHeight = Math.max(config.leftText.length, config.rightText.length) * config.thumbnailBoxHeight + 2 * THUMBNAIL_PADDING;\n",
       "            var yOffset = 20;\n",
       "            var y = layerIndex * thumbnailHeight + THUMBNAIL_PADDING + yOffset;\n",
       "            if (y < MIN_Y) {\n",
       "                y = MIN_Y;\n",
       "            } else if (y + config.detailHeight > maxY) {\n",
       "                y = maxY - config.detailHeight;\n",
       "            }\n",
       "            renderDetailFrame(x, y, layerIndex);\n",
       "            y = y + DETAIL_PADDING;\n",
       "            renderDetailHeading(x, y, layerIndex, headIndex);\n",
       "            y = y + DETAIL_HEADING_HEIGHT;\n",
       "            renderDetailText(config.leftText, \"leftText\", posLeftText, y , layerIndex);\n",
       "            renderDetailAttn(posAttention, y, att, layerIndex, headIndex);\n",
       "            renderDetailText(config.rightText, \"rightText\", posRightText, y, layerIndex);\n",
       "        }\n",
       "\n",
       "        function renderDetailHeading(x, y, layerIndex, headIndex) {\n",
       "            var fillColor = getTextColor();\n",
       "            config.svg.append(\"text\")\n",
       "                .classed(\"detail\", true)\n",
       "                .text('Layer ' + config.layers[layerIndex] + \", Head \" + config.heads[headIndex])\n",
       "                .attr(\"font-size\", TEXT_SIZE + \"px\")\n",
       "                .attr(\"font-weight\", \"bold\")\n",
       "                .style(\"cursor\", \"default\")\n",
       "                .style(\"-webkit-user-select\", \"none\")\n",
       "                .attr(\"fill\", fillColor)\n",
       "                .attr(\"x\", x + DETAIL_WIDTH / 2)\n",
       "                .attr(\"text-anchor\", \"middle\")\n",
       "                .attr(\"y\", y)\n",
       "                .attr(\"height\", DETAIL_HEADING_HEIGHT)\n",
       "                .attr(\"width\", DETAIL_WIDTH)\n",
       "                .attr(\"dy\", HEADING_TEXT_SIZE);\n",
       "        }\n",
       "\n",
       "        function renderDetailText(text, id, x, y, layerIndex) {\n",
       "            var tokenContainer = config.svg.append(\"svg:g\")\n",
       "                .classed(\"detail\", true)\n",
       "                .selectAll(\"g\")\n",
       "                .data(text)\n",
       "                .enter()\n",
       "                .append(\"g\");\n",
       "\n",
       "            var fillColor = getTextColor();\n",
       "\n",
       "            tokenContainer.append(\"rect\")\n",
       "                .classed(\"highlight\", true)\n",
       "                .attr(\"fill\", fillColor)\n",
       "                .style(\"opacity\", 0.0)\n",
       "                .attr(\"height\", DETAIL_BOX_HEIGHT)\n",
       "                .attr(\"width\", DETAIL_BOX_WIDTH)\n",
       "                .attr(\"x\", x)\n",
       "                .attr(\"y\", function (d, i) {\n",
       "                    return y + i * DETAIL_BOX_HEIGHT;\n",
       "                });\n",
       "\n",
       "            var textContainer = tokenContainer.append(\"text\")\n",
       "                .classed(\"token\", true)\n",
       "                .text(function (d) {\n",
       "                    return d;\n",
       "                })\n",
       "                .attr(\"font-size\", TEXT_SIZE + \"px\")\n",
       "                .style(\"cursor\", \"default\")\n",
       "                .style(\"-webkit-user-select\", \"none\")\n",
       "                .attr(\"fill\", fillColor)\n",
       "                .attr(\"x\", x)\n",
       "                .attr(\"y\", function (d, i) {\n",
       "                    return i * DETAIL_BOX_HEIGHT + y;\n",
       "                })\n",
       "                .attr(\"height\", DETAIL_BOX_HEIGHT)\n",
       "                .attr(\"width\", DETAIL_BOX_WIDTH)\n",
       "                .attr(\"dy\", TEXT_SIZE);\n",
       "\n",
       "            if (id == \"leftText\") {\n",
       "                textContainer.style(\"text-anchor\", \"end\")\n",
       "                    .attr(\"dx\", DETAIL_BOX_WIDTH - 2);\n",
       "                tokenContainer.on(\"mouseover\", function (d, index) {\n",
       "                    highlightSelection(index);\n",
       "                });\n",
       "                tokenContainer.on(\"mouseleave\", function () {\n",
       "                    unhighlightSelection();\n",
       "                });\n",
       "            }\n",
       "        }\n",
       "\n",
       "        function highlightSelection(index) {\n",
       "            config.svg.select(\"#leftText\")\n",
       "                .selectAll(\".highlight\")\n",
       "                .style(\"opacity\", function (d, i) {\n",
       "                    return i == index ? 1.0 : 0.0;\n",
       "                });\n",
       "            config.svg.selectAll(\".attn-line-group\")\n",
       "                .style(\"opacity\", function (d, i) {\n",
       "                    return i == index ? 1.0 : 0.0;\n",
       "                });\n",
       "        }\n",
       "\n",
       "        function unhighlightSelection() {\n",
       "            config.svg.select(\"#leftText\")\n",
       "                .selectAll(\".highlight\")\n",
       "                .style(\"opacity\", 0.0);\n",
       "            config.svg.selectAll(\".attn-line-group\")\n",
       "                .style(\"opacity\", 1);\n",
       "        }\n",
       "\n",
       "        function renderThumbnailAttn(x, y, att, layerIndex, headIndex) {\n",
       "\n",
       "            var attnContainer = config.svg.append(\"svg:g\");\n",
       "\n",
       "            var attnBackground = attnContainer.append(\"rect\")\n",
       "                .attr(\"id\", 'attn_background_' + layerIndex + \"_\" + headIndex)\n",
       "                .classed(\"attn_background\", true)\n",
       "                .attr(\"x\", x)\n",
       "                .attr(\"y\", y)\n",
       "                .attr(\"height\", config.thumbnailHeight)\n",
       "                .attr(\"width\", config.thumbnailWidth)\n",
       "                .attr(\"stroke-width\", 2)\n",
       "                .attr(\"stroke\", getLayerColor(layerIndex))\n",
       "                .attr(\"stroke-opacity\", 0)\n",
       "                .attr(\"fill\", getBackgroundColor());\n",
       "            var x1 = x + THUMBNAIL_PADDING;\n",
       "            var x2 = x1 + config.thumbnailWidth - 14;\n",
       "            var y1 = y + THUMBNAIL_PADDING;\n",
       "\n",
       "            attnContainer.selectAll(\"g\")\n",
       "                .data(att)\n",
       "                .enter()\n",
       "                .append(\"g\") // Add group for each source token\n",
       "                .attr(\"source-index\", function (d, i) { // Save index of source token\n",
       "                    return i;\n",
       "                })\n",
       "                .selectAll(\"line\")\n",
       "                .data(function (d) { // Loop over all target tokens\n",
       "                    return d;\n",
       "                })\n",
       "                .enter() // When entering\n",
       "                .append(\"line\")\n",
       "                .attr(\"x1\", x1)\n",
       "                .attr(\"y1\", function (d) {\n",
       "                    var sourceIndex = +this.parentNode.getAttribute(\"source-index\");\n",
       "                    return y1 + (sourceIndex + .5) * config.thumbnailBoxHeight;\n",
       "                })\n",
       "                .attr(\"x2\", x2)\n",
       "                .attr(\"y2\", function (d, targetIndex) {\n",
       "                    return y1 + (targetIndex + .5) * config.thumbnailBoxHeight;\n",
       "                })\n",
       "                .attr(\"stroke-width\", 2.2)\n",
       "                .attr(\"stroke\", getLayerColor(layerIndex))\n",
       "                .attr(\"stroke-opacity\", function (d) {\n",
       "                    return d;\n",
       "                });\n",
       "\n",
       "            var clickRegion = attnContainer.append(\"rect\")\n",
       "                .attr(\"x\", x)\n",
       "                .attr(\"y\", y)\n",
       "                .attr(\"height\", config.thumbnailHeight)\n",
       "                .attr(\"width\", config.thumbnailWidth)\n",
       "                .style(\"opacity\", 0);\n",
       "\n",
       "            clickRegion.on(\"click\", function (d, index) {\n",
       "                var attnBackgroundOther = config.svg.selectAll(\".attn_background\");\n",
       "                attnBackgroundOther.attr(\"fill\", getBackgroundColor());\n",
       "                attnBackgroundOther.attr(\"stroke-opacity\", 0);\n",
       "\n",
       "                config.svg.selectAll(\".detail\").remove();\n",
       "                if (config.detail_layer != layerIndex || config.detail_head != headIndex) {\n",
       "                    renderDetail(att, layerIndex, headIndex);\n",
       "                    config.detail_layer = layerIndex;\n",
       "                    config.detail_head = headIndex;\n",
       "                    attnBackground.attr(\"fill\", getHighlightColor());\n",
       "                    attnBackground.attr(\"stroke-opacity\", .8);\n",
       "                } else {\n",
       "                    config.detail_layer = null;\n",
       "                    config.detail_head = null;\n",
       "                    attnBackground.attr(\"fill\", getBackgroundColor());\n",
       "                    attnBackground.attr(\"stroke-opacity\", 0);\n",
       "                }\n",
       "            });\n",
       "\n",
       "            clickRegion.on(\"mouseover\", function (d) {\n",
       "                d3.select(this).style(\"cursor\", \"pointer\");\n",
       "            });\n",
       "        }\n",
       "\n",
       "        function renderDetailFrame(x, y, layerIndex) {\n",
       "            var detailFrame = config.svg.append(\"rect\")\n",
       "                .classed(\"detail\", true)\n",
       "                .attr(\"x\", x)\n",
       "                .attr(\"y\", y)\n",
       "                .attr(\"height\", config.detailHeight)\n",
       "                .attr(\"width\", DETAIL_WIDTH)\n",
       "                .style(\"opacity\", 1)\n",
       "                .attr(\"stroke-width\", 1.5)\n",
       "                .attr(\"stroke-opacity\", 0.7)\n",
       "                .attr(\"stroke\", getLayerColor(layerIndex));\n",
       "        }\n",
       "\n",
       "        function renderDetailAttn(x, y, att, layerIndex) {\n",
       "            var attnContainer = config.svg.append(\"svg:g\")\n",
       "                .classed(\"detail\", true)\n",
       "                .attr(\"pointer-events\", \"none\");\n",
       "            attnContainer.selectAll(\"g\")\n",
       "                .data(att)\n",
       "                .enter()\n",
       "                .append(\"g\") // Add group for each source token\n",
       "                .classed('attn-line-group', true)\n",
       "                .attr(\"source-index\", function (d, i) { // Save index of source token\n",
       "                    return i;\n",
       "                })\n",
       "                .selectAll(\"line\")\n",
       "                .data(function (d) { // Loop over all target tokens\n",
       "                    return d;\n",
       "                })\n",
       "                .enter()\n",
       "                .append(\"line\")\n",
       "                .attr(\"x1\", x + ATTN_PADDING)\n",
       "                .attr(\"y1\", function (d) {\n",
       "                    var sourceIndex = +this.parentNode.getAttribute(\"source-index\");\n",
       "                    return y + (sourceIndex + .5) * DETAIL_BOX_HEIGHT;\n",
       "                })\n",
       "                .attr(\"x2\", x + DETAIL_ATTENTION_WIDTH - ATTN_PADDING)\n",
       "                .attr(\"y2\", function (d, targetIndex) {\n",
       "                    return y + (targetIndex + .5) * DETAIL_BOX_HEIGHT;\n",
       "                })\n",
       "                .attr(\"stroke-width\", 2.2)\n",
       "                .attr(\"stroke\", getLayerColor(layerIndex))\n",
       "                .attr(\"stroke-opacity\", function (d) {\n",
       "                    return d;\n",
       "                });\n",
       "        }\n",
       "\n",
       "        function getLayerColor(layer) {\n",
       "          return LAYER_COLORS[config.layers[layer] % 10];\n",
       "        }\n",
       "\n",
       "        function getTextColor() {\n",
       "            return PALETTE[config.mode]['text']\n",
       "        }\n",
       "\n",
       "        function getBackgroundColor() {\n",
       "           return PALETTE[config.mode]['background']\n",
       "        }\n",
       "\n",
       "        function getHighlightColor() {\n",
       "           return PALETTE[config.mode]['highlight']\n",
       "        }\n",
       "\n",
       "        function initialize() {\n",
       "            config.attention = params['attention'];\n",
       "            config.filter = params['default_filter'];\n",
       "            config.mode = params['display_mode'];\n",
       "            config.layers = params['include_layers']\n",
       "            config.heads = params['include_heads']\n",
       "            config.totalHeads = params['total_heads']\n",
       "            config.rootDivId = params['root_div_id'];\n",
       "            $(`#${config.rootDivId} #filter`).on('change', function (e) {\n",
       "                config.filter = e.currentTarget.value;\n",
       "                render();\n",
       "            });\n",
       "        }\n",
       "\n",
       "        initialize();\n",
       "        render();\n",
       "\n",
       "    });"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel, utils, AutoModelForCausalLM\n",
    "\n",
    "from bertviz import model_view\n",
    "utils.logging.set_verbosity_error()  # Suppress standard warnings\n",
    "\n",
    "model_name = 'openai-community/gpt2'\n",
    "input_text = \"No, I am your father\"  \n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, output_attentions=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "inputs = tokenizer.encode(input_text, return_tensors='pt')  # Tokenize input text\n",
    "outputs = model(inputs)  # Run model\n",
    "attention = outputs[-1]  # Retrieve attention from model outputs\n",
    "tokens = tokenizer.convert_ids_to_tokens(inputs[0])  # Convert input ids to token strings\n",
    "model_view(attention, tokens)  # Display model view"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positional encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IFq78-kjbrWp"
   },
   "source": [
    "Last week, Carlo discussed token embedding, which is when words are encoded into a vocabulary. Now, we just discussed attention mechanisms which account for context between words. Another question we should ask is how do we account for the order of words in an input sentence\n",
    "\n",
    "Consider the following two sentences to see why this is important:\n",
    "\n",
    "``The man ate the sandwich.``\n",
    "\n",
    "``The sandwich ate the man.``\n",
    "\n",
    "Clearly, these are two vastly different situations even though they have the same words. The Transformer can \n",
    "\n",
    "Transformers differentiate between these situations by adding a **Positional encoding** vector to each input embedding. These vectors follow a specific pattern that the model learns, which helps it determine the position of each word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/positional_encoding.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "Image credit: https://medium.com/@xuer.chen.human/llm-study-notes-positional-encoding-0639a1002ec0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set up positional encoding similarly as token embedding using the ``nn.Embedding`` tool. We use a simple embedding here but there are more complex positional encodings used such as sinusoidal. \n",
    "\n",
    "For an explanation of different positional encodings, refer to this post: https://machinelearningmastery.com/a-gentle-introduction-to-positional-encoding-in-transformer-models-part-1/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 65\n",
    "n_embd = 64\n",
    "\n",
    "token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "block_size = 32 # what is the maximum context length for predictions?\n",
    "position_embedding_table = nn.Embedding(block_size, n_embd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will notice the positional encoding size is `(block_size, n_embed)` because it encodes for the postion of a token within the sequence of size `block_size`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, the position embedding used is simply added to the token embedding to apply positional embedding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at token embedding alone:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-9.6630e-05, -1.4031e+00,  6.1625e-01,  3.0717e-02,  1.2290e-01,\n",
      "        -4.0682e-01,  1.9496e+00,  1.1764e+00, -1.5591e+00,  7.2791e-02,\n",
      "        -2.3081e+00, -5.0737e-01, -6.9863e-01, -1.3517e+00, -2.1065e-02,\n",
      "        -9.5309e-01, -1.0516e+00,  7.7541e-02,  4.4402e-01,  8.8709e-01,\n",
      "         1.8823e-01,  7.1672e-02, -3.4917e-01, -5.7223e-01,  3.5027e-01,\n",
      "         7.1300e-01, -4.1757e-01,  1.2332e+00, -1.0018e+00,  6.6873e-01,\n",
      "         9.4601e-03, -1.8759e+00,  3.9894e-01,  6.6391e-01,  6.4071e-02,\n",
      "         1.6804e+00,  6.2182e-01, -1.6898e+00, -3.4645e-01, -3.1754e+00,\n",
      "         9.4335e-01,  1.7508e+00, -7.7534e-01, -8.0301e-01,  2.6676e+00,\n",
      "         3.1534e-01, -5.9224e-01,  4.7193e-01,  6.4641e-01,  4.3199e-01,\n",
      "         1.4329e+00, -1.0546e+00,  1.6986e+00, -1.2204e+00, -1.2765e-02,\n",
      "        -1.3485e+00, -4.3946e-01, -1.3725e-01,  4.2354e-01, -4.0840e-01,\n",
      "        -7.1900e-01, -6.6362e-01, -8.9380e-02,  1.4980e-01],\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1,3,15,4,7,1,4,9])\n",
    "x = token_embedding_table(x)\n",
    "print(x[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And token + positional embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.6103, -1.1454,  0.2832, -0.5627, -0.7867, -0.7475,  1.6190,  0.4168,\n",
      "        -2.4363,  0.1630, -2.2069,  0.1004,  0.3215, -0.9887, -0.2308,  0.1534,\n",
      "        -1.2566, -0.2798, -0.0496, -0.0997,  0.9740,  0.4581,  0.7802, -0.1746,\n",
      "         0.0531, -1.5154,  0.3336,  2.4084, -1.0335,  1.3728, -1.2628, -0.5919,\n",
      "         0.2460, -0.2431,  2.1009,  0.5958, -0.4106, -2.4724, -1.7571, -3.5932,\n",
      "         0.4605,  0.8671, -1.9192, -3.0066,  1.6024, -1.5752,  0.7494,  0.8431,\n",
      "         2.0244,  1.0557,  0.2076,  0.2220,  0.2793, -2.0823,  0.6992, -1.1937,\n",
      "        -0.3509,  0.8347,  1.0244,  0.5620, -0.3641, -1.3770, -0.1733, -1.4676],\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1,3,15,4,7,1,4,9])\n",
    "x= position_embedding_table(x) + token_embedding_table(x)\n",
    "print(x[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see a clear offset between these two embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During the training process, these embeddings will be learned to best encode the token and positional embeddings of the sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iF1HzH9xNJ7S"
   },
   "source": [
    "## Output layers\n",
    "\n",
    "At the end of our Transformer model, we are left with a vector, so how do we turn this into a word?\n",
    "\n",
    "<img src=\"images/transformer-decoder-intro.png\" alt=\"Drawing\" style=\"width: 400px;\"/>\n",
    "\n",
    "Using a final Linear layer and a Softmax Layer.\n",
    "The Linear layer projects the vector produced by the stack of decoders, into a larger vector called a logits vector.\n",
    "\n",
    "If our model knows 10,000 unique English words learned from its training dataset the logits vector is 10,000 cells wide ‚Äì each cell corresponds to the score of a unique word.\n",
    "\n",
    "The softmax layer turns those scores into probabilities. The cell with the highest probability is chosen, and the word associated with it is produced as the output for this time step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/transformer_decoder_output_softmax.png\" alt=\"Drawing\" style=\"width: 450px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HS6r-z8dN_RV"
   },
   "source": [
    "Image credit: https://jalammar.github.io/illustrated-transformer/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XK8q67P03yr4"
   },
   "source": [
    "## Training\n",
    "\n",
    "How does an LLM improve over time?\n",
    "We want to compare the probabilitiy distribution for each token generated by our model to the ground truths. \n",
    "Our model produces a probability distribution for each token. We want to compare these probability distributions to the ground truths. \n",
    "For example, when translating the sentence: ‚Äúje suis √©tudiant‚Äù into ‚Äúi am a student‚Äù as can be seen in the example:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/output_target_probability_distributions.png\" alt=\"Drawing\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HS6r-z8dN_RV"
   },
   "source": [
    "Image credit: https://jalammar.github.io/illustrated-transformer/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model can calculate the loss between the vector it generates and the ground truth vector seen in this example. A commonly used loss function is cross entropy loss:\n",
    "\n",
    "$CE = -\\sum_{x \\in X} p(x) log q(x)$\n",
    "\n",
    "where p(x) represents the true distribution and q(x) represents the predicted distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9119)\n"
     ]
    }
   ],
   "source": [
    "from torch.nn import functional as F\n",
    "logits = torch.tensor([0.5, 0.1, 0.3])\n",
    "targets = torch.tensor([1.0, 0.0, 0.0])\n",
    "loss = F.cross_entropy(logits, targets)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another important metric commonly used in LLMs is **perplexity**.\n",
    "\n",
    "Intuitively, perplexity means to be surprised. We measure how much the model is surprised by seeing new data. The lower the perplexity, the better the training is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mathematically, perplexity is just the exponent of the negative cross entropy loss:\n",
    "\n",
    "$\\text{perplexity} = exp(\\text{CE})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.4891)\n"
     ]
    }
   ],
   "source": [
    "perplexity = torch.exp(loss)\n",
    "print(perplexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we are using cross entropy loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's train a mini-LLM from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "batch_size = 16 # how many independent sequences will we process in parallel?\n",
    "block_size = 32 # what is the maximum context length for predictions?\n",
    "max_iters = 5000\n",
    "eval_interval = 10\n",
    "learning_rate = 1e-3\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = 64\n",
    "n_head = 4 ## so head_size = 16\n",
    "n_layer = 4\n",
    "dropout = 0.0\n",
    "# ------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in data and create train and test datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to be using the tiny Shakespeare dataset. \n",
    "Data is tokenized according to a simple character based tokenizer.\n",
    "Data is split into a train and test set so we have something to test after performing training (9:1 split)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dataset/input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "# Train and test splits\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "# data loading\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up the components of the Decoder block: \n",
    "* MultiHeadAttention\n",
    "* FeedForward Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,C) 16,32,16\n",
    "        q = self.query(x) # (B,T,C)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,C)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd), # Projection layer going back into the residual pathway\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine components into the Decoder block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))    # Communication\n",
    "        x = x + self.ffwd(self.ln2(x))  # Computation\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up the full Transformer model \n",
    "This is a combination of the Token embeddings, Positional embeddings, a stack of Transformer blocks and an output block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# super simple language model\n",
    "class LanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be training a larger LLM on distributed resources in session 6."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework\n",
    "\n",
    "1. In this notebook, we learned the various components of an LLM. \n",
    "    Your homework this week is to take the mini LLM we created from scratch and run your own training loop. Show how the training and validation perplexity change over the steps.\n",
    "      \n",
    "    Hint: this function might be useful for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Run the same training loop but modify one of the hyperparameters from this list: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "n_embd = 64\n",
    "n_head = 4 ## so head_size = 16\n",
    "n_layer = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this at least 4 times with a different value and plot each perplexity over training step. Write a sentence on how the perplexity changed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bonus 1: output some generated text from each model you trained. Did the output make more sense with some hyperparameters than others? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bonus 2: We saw a cool visualization of attention mechanisms with BertViz. Take a more complicated model than GPT2 such as \"meta-llama/Llama-2-7b-chat-hf\" and see how the attention mechanisms are different "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework\n",
    "author: Xiaorui Dong"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Training and validation perplexity change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLanguageModel(LanguageModel):\n",
    "    \n",
    "    def __init__(self, n_embd: int, n_head: int, n_layer: int):  # Make hyperparameters arguments of the model\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: train perplexity: 60.47 | val perplexity: 61.23\n",
      "Iteration 10: train perplexity: 29.62 | val perplexity: 30.37\n",
      "Iteration 20: train perplexity: 25.80 | val perplexity: 26.81\n",
      "Iteration 30: train perplexity: 23.41 | val perplexity: 24.34\n",
      "Iteration 40: train perplexity: 20.75 | val perplexity: 21.32\n",
      "Iteration 50: train perplexity: 18.54 | val perplexity: 19.07\n",
      "Iteration 60: train perplexity: 16.86 | val perplexity: 17.18\n",
      "Iteration 70: train perplexity: 15.65 | val perplexity: 15.91\n",
      "Iteration 80: train perplexity: 14.94 | val perplexity: 15.11\n",
      "Iteration 90: train perplexity: 14.41 | val perplexity: 14.44\n",
      "Iteration 100: train perplexity: 14.17 | val perplexity: 14.12\n",
      "Iteration 110: train perplexity: 13.74 | val perplexity: 13.90\n",
      "Iteration 120: train perplexity: 13.51 | val perplexity: 13.69\n",
      "Iteration 130: train perplexity: 13.30 | val perplexity: 13.36\n",
      "Iteration 140: train perplexity: 13.12 | val perplexity: 13.30\n",
      "Iteration 150: train perplexity: 12.84 | val perplexity: 12.89\n",
      "Iteration 160: train perplexity: 12.81 | val perplexity: 12.94\n",
      "Iteration 170: train perplexity: 12.62 | val perplexity: 12.72\n",
      "Iteration 180: train perplexity: 12.55 | val perplexity: 12.62\n",
      "Iteration 190: train perplexity: 12.50 | val perplexity: 12.41\n",
      "Iteration 200: train perplexity: 12.27 | val perplexity: 12.25\n",
      "Iteration 210: train perplexity: 12.18 | val perplexity: 12.15\n",
      "Iteration 220: train perplexity: 12.05 | val perplexity: 12.09\n",
      "Iteration 230: train perplexity: 11.85 | val perplexity: 12.04\n",
      "Iteration 240: train perplexity: 11.91 | val perplexity: 11.79\n",
      "Iteration 250: train perplexity: 11.81 | val perplexity: 11.92\n",
      "Iteration 260: train perplexity: 11.88 | val perplexity: 11.78\n",
      "Iteration 270: train perplexity: 11.54 | val perplexity: 11.54\n",
      "Iteration 280: train perplexity: 11.43 | val perplexity: 11.49\n",
      "Iteration 290: train perplexity: 11.37 | val perplexity: 11.39\n",
      "Iteration 300: train perplexity: 11.32 | val perplexity: 11.41\n",
      "Iteration 310: train perplexity: 11.27 | val perplexity: 11.26\n",
      "Iteration 320: train perplexity: 11.13 | val perplexity: 11.14\n",
      "Iteration 330: train perplexity: 11.13 | val perplexity: 11.15\n",
      "Iteration 340: train perplexity: 10.98 | val perplexity: 11.01\n",
      "Iteration 350: train perplexity: 10.89 | val perplexity: 11.10\n",
      "Iteration 360: train perplexity: 10.97 | val perplexity: 10.95\n",
      "Iteration 370: train perplexity: 10.73 | val perplexity: 10.82\n",
      "Iteration 380: train perplexity: 10.76 | val perplexity: 10.99\n",
      "Iteration 390: train perplexity: 10.66 | val perplexity: 10.77\n",
      "Iteration 400: train perplexity: 10.62 | val perplexity: 10.68\n",
      "Iteration 410: train perplexity: 10.45 | val perplexity: 10.49\n",
      "Iteration 420: train perplexity: 10.48 | val perplexity: 10.63\n",
      "Iteration 430: train perplexity: 10.42 | val perplexity: 10.61\n",
      "Iteration 440: train perplexity: 10.35 | val perplexity: 10.47\n",
      "Iteration 450: train perplexity: 10.33 | val perplexity: 10.34\n",
      "Iteration 460: train perplexity: 10.28 | val perplexity: 10.36\n",
      "Iteration 470: train perplexity: 10.17 | val perplexity: 10.31\n",
      "Iteration 480: train perplexity: 10.12 | val perplexity: 10.21\n",
      "Iteration 490: train perplexity: 10.00 | val perplexity: 10.17\n",
      "Iteration 500: train perplexity: 10.13 | val perplexity: 10.18\n",
      "Iteration 510: train perplexity: 10.08 | val perplexity: 10.15\n",
      "Iteration 520: train perplexity: 10.03 | val perplexity: 10.09\n",
      "Iteration 530: train perplexity: 9.99 | val perplexity: 10.17\n",
      "Iteration 540: train perplexity: 9.80 | val perplexity: 10.06\n",
      "Iteration 550: train perplexity: 9.84 | val perplexity: 9.92\n",
      "Iteration 560: train perplexity: 9.79 | val perplexity: 9.80\n",
      "Iteration 570: train perplexity: 9.72 | val perplexity: 9.95\n",
      "Iteration 580: train perplexity: 9.76 | val perplexity: 9.92\n",
      "Iteration 590: train perplexity: 9.64 | val perplexity: 9.78\n",
      "Iteration 600: train perplexity: 9.67 | val perplexity: 9.72\n",
      "Iteration 610: train perplexity: 9.54 | val perplexity: 9.70\n",
      "Iteration 620: train perplexity: 9.60 | val perplexity: 9.70\n",
      "Iteration 630: train perplexity: 9.59 | val perplexity: 9.81\n",
      "Iteration 640: train perplexity: 9.45 | val perplexity: 9.56\n",
      "Iteration 650: train perplexity: 9.26 | val perplexity: 9.47\n",
      "Iteration 660: train perplexity: 9.35 | val perplexity: 9.56\n",
      "Iteration 670: train perplexity: 9.28 | val perplexity: 9.50\n",
      "Iteration 680: train perplexity: 9.26 | val perplexity: 9.34\n",
      "Iteration 690: train perplexity: 9.27 | val perplexity: 9.54\n",
      "Iteration 700: train perplexity: 9.23 | val perplexity: 9.40\n",
      "Iteration 710: train perplexity: 9.19 | val perplexity: 9.41\n",
      "Iteration 720: train perplexity: 9.06 | val perplexity: 9.32\n",
      "Iteration 730: train perplexity: 9.10 | val perplexity: 9.17\n",
      "Iteration 740: train perplexity: 8.97 | val perplexity: 9.24\n",
      "Iteration 750: train perplexity: 9.08 | val perplexity: 9.22\n",
      "Iteration 760: train perplexity: 8.96 | val perplexity: 9.16\n",
      "Iteration 770: train perplexity: 9.00 | val perplexity: 9.18\n",
      "Iteration 780: train perplexity: 8.94 | val perplexity: 9.09\n",
      "Iteration 790: train perplexity: 8.91 | val perplexity: 9.08\n",
      "Iteration 800: train perplexity: 8.85 | val perplexity: 9.01\n",
      "Iteration 810: train perplexity: 8.81 | val perplexity: 8.99\n",
      "Iteration 820: train perplexity: 8.74 | val perplexity: 8.97\n",
      "Iteration 830: train perplexity: 8.68 | val perplexity: 8.83\n",
      "Iteration 840: train perplexity: 8.69 | val perplexity: 8.90\n",
      "Iteration 850: train perplexity: 8.71 | val perplexity: 8.93\n",
      "Iteration 860: train perplexity: 8.65 | val perplexity: 8.89\n",
      "Iteration 870: train perplexity: 8.74 | val perplexity: 8.90\n",
      "Iteration 880: train perplexity: 8.63 | val perplexity: 8.75\n",
      "Iteration 890: train perplexity: 8.58 | val perplexity: 8.79\n",
      "Iteration 900: train perplexity: 8.64 | val perplexity: 8.87\n",
      "Iteration 910: train perplexity: 8.50 | val perplexity: 8.74\n",
      "Iteration 920: train perplexity: 8.53 | val perplexity: 8.77\n",
      "Iteration 930: train perplexity: 8.32 | val perplexity: 8.71\n",
      "Iteration 940: train perplexity: 8.50 | val perplexity: 8.74\n",
      "Iteration 950: train perplexity: 8.43 | val perplexity: 8.74\n",
      "Iteration 960: train perplexity: 8.33 | val perplexity: 8.61\n",
      "Iteration 970: train perplexity: 8.28 | val perplexity: 8.67\n",
      "Iteration 980: train perplexity: 8.30 | val perplexity: 8.67\n",
      "Iteration 990: train perplexity: 8.14 | val perplexity: 8.59\n",
      "Iteration 1000: train perplexity: 8.20 | val perplexity: 8.51\n",
      "Iteration 1010: train perplexity: 8.27 | val perplexity: 8.54\n",
      "Iteration 1020: train perplexity: 8.23 | val perplexity: 8.45\n",
      "Iteration 1030: train perplexity: 8.17 | val perplexity: 8.52\n",
      "Iteration 1040: train perplexity: 8.13 | val perplexity: 8.50\n",
      "Iteration 1050: train perplexity: 8.10 | val perplexity: 8.43\n",
      "Iteration 1060: train perplexity: 7.97 | val perplexity: 8.43\n",
      "Iteration 1070: train perplexity: 8.00 | val perplexity: 8.24\n",
      "Iteration 1080: train perplexity: 8.06 | val perplexity: 8.31\n",
      "Iteration 1090: train perplexity: 7.97 | val perplexity: 8.30\n",
      "Iteration 1100: train perplexity: 8.04 | val perplexity: 8.38\n",
      "Iteration 1110: train perplexity: 7.81 | val perplexity: 8.22\n",
      "Iteration 1120: train perplexity: 7.87 | val perplexity: 8.17\n",
      "Iteration 1130: train perplexity: 7.92 | val perplexity: 8.21\n",
      "Iteration 1140: train perplexity: 7.86 | val perplexity: 8.28\n",
      "Iteration 1150: train perplexity: 7.87 | val perplexity: 8.32\n",
      "Iteration 1160: train perplexity: 7.88 | val perplexity: 8.30\n",
      "Iteration 1170: train perplexity: 7.81 | val perplexity: 8.23\n",
      "Iteration 1180: train perplexity: 7.78 | val perplexity: 8.25\n",
      "Iteration 1190: train perplexity: 7.74 | val perplexity: 8.11\n",
      "Iteration 1200: train perplexity: 7.74 | val perplexity: 8.14\n",
      "Iteration 1210: train perplexity: 7.73 | val perplexity: 8.09\n",
      "Iteration 1220: train perplexity: 7.68 | val perplexity: 8.05\n",
      "Iteration 1230: train perplexity: 7.72 | val perplexity: 7.93\n",
      "Iteration 1240: train perplexity: 7.66 | val perplexity: 8.08\n",
      "Iteration 1250: train perplexity: 7.59 | val perplexity: 8.05\n",
      "Iteration 1260: train perplexity: 7.65 | val perplexity: 8.12\n",
      "Iteration 1270: train perplexity: 7.62 | val perplexity: 8.03\n",
      "Iteration 1280: train perplexity: 7.62 | val perplexity: 8.00\n",
      "Iteration 1290: train perplexity: 7.54 | val perplexity: 7.96\n",
      "Iteration 1300: train perplexity: 7.54 | val perplexity: 7.94\n",
      "Iteration 1310: train perplexity: 7.55 | val perplexity: 7.98\n",
      "Iteration 1320: train perplexity: 7.60 | val perplexity: 8.08\n",
      "Iteration 1330: train perplexity: 7.52 | val perplexity: 8.01\n",
      "Iteration 1340: train perplexity: 7.43 | val perplexity: 7.83\n",
      "Iteration 1350: train perplexity: 7.48 | val perplexity: 7.91\n",
      "Iteration 1360: train perplexity: 7.47 | val perplexity: 7.93\n",
      "Iteration 1370: train perplexity: 7.44 | val perplexity: 7.94\n",
      "Iteration 1380: train perplexity: 7.39 | val perplexity: 7.90\n",
      "Iteration 1390: train perplexity: 7.34 | val perplexity: 7.90\n",
      "Iteration 1400: train perplexity: 7.38 | val perplexity: 7.81\n",
      "Iteration 1410: train perplexity: 7.30 | val perplexity: 7.72\n",
      "Iteration 1420: train perplexity: 7.31 | val perplexity: 7.76\n",
      "Iteration 1430: train perplexity: 7.27 | val perplexity: 7.86\n",
      "Iteration 1440: train perplexity: 7.30 | val perplexity: 7.73\n",
      "Iteration 1450: train perplexity: 7.21 | val perplexity: 7.72\n",
      "Iteration 1460: train perplexity: 7.19 | val perplexity: 7.70\n",
      "Iteration 1470: train perplexity: 7.19 | val perplexity: 7.74\n",
      "Iteration 1480: train perplexity: 7.19 | val perplexity: 7.71\n",
      "Iteration 1490: train perplexity: 7.20 | val perplexity: 7.73\n",
      "Iteration 1500: train perplexity: 7.18 | val perplexity: 7.75\n",
      "Iteration 1510: train perplexity: 7.06 | val perplexity: 7.66\n",
      "Iteration 1520: train perplexity: 7.10 | val perplexity: 7.64\n",
      "Iteration 1530: train perplexity: 7.08 | val perplexity: 7.60\n",
      "Iteration 1540: train perplexity: 7.02 | val perplexity: 7.65\n",
      "Iteration 1550: train perplexity: 7.05 | val perplexity: 7.63\n",
      "Iteration 1560: train perplexity: 7.00 | val perplexity: 7.49\n",
      "Iteration 1570: train perplexity: 7.06 | val perplexity: 7.60\n",
      "Iteration 1580: train perplexity: 7.12 | val perplexity: 7.67\n",
      "Iteration 1590: train perplexity: 7.03 | val perplexity: 7.56\n",
      "Iteration 1600: train perplexity: 7.02 | val perplexity: 7.61\n",
      "Iteration 1610: train perplexity: 7.05 | val perplexity: 7.58\n",
      "Iteration 1620: train perplexity: 6.96 | val perplexity: 7.66\n",
      "Iteration 1630: train perplexity: 6.96 | val perplexity: 7.50\n",
      "Iteration 1640: train perplexity: 7.00 | val perplexity: 7.55\n",
      "Iteration 1650: train perplexity: 7.04 | val perplexity: 7.53\n",
      "Iteration 1660: train perplexity: 6.96 | val perplexity: 7.54\n",
      "Iteration 1670: train perplexity: 6.89 | val perplexity: 7.49\n",
      "Iteration 1680: train perplexity: 6.88 | val perplexity: 7.42\n",
      "Iteration 1690: train perplexity: 6.88 | val perplexity: 7.44\n",
      "Iteration 1700: train perplexity: 6.87 | val perplexity: 7.37\n",
      "Iteration 1710: train perplexity: 6.88 | val perplexity: 7.40\n",
      "Iteration 1720: train perplexity: 6.84 | val perplexity: 7.32\n",
      "Iteration 1730: train perplexity: 6.83 | val perplexity: 7.34\n",
      "Iteration 1740: train perplexity: 6.90 | val perplexity: 7.51\n",
      "Iteration 1750: train perplexity: 6.92 | val perplexity: 7.27\n",
      "Iteration 1760: train perplexity: 6.75 | val perplexity: 7.42\n",
      "Iteration 1770: train perplexity: 6.83 | val perplexity: 7.51\n",
      "Iteration 1780: train perplexity: 6.81 | val perplexity: 7.37\n",
      "Iteration 1790: train perplexity: 6.76 | val perplexity: 7.39\n",
      "Iteration 1800: train perplexity: 6.76 | val perplexity: 7.39\n",
      "Iteration 1810: train perplexity: 6.73 | val perplexity: 7.30\n",
      "Iteration 1820: train perplexity: 6.69 | val perplexity: 7.34\n",
      "Iteration 1830: train perplexity: 6.72 | val perplexity: 7.38\n",
      "Iteration 1840: train perplexity: 6.69 | val perplexity: 7.42\n",
      "Iteration 1850: train perplexity: 6.67 | val perplexity: 7.39\n",
      "Iteration 1860: train perplexity: 6.72 | val perplexity: 7.34\n",
      "Iteration 1870: train perplexity: 6.73 | val perplexity: 7.24\n",
      "Iteration 1880: train perplexity: 6.69 | val perplexity: 7.32\n",
      "Iteration 1890: train perplexity: 6.58 | val perplexity: 7.35\n",
      "Iteration 1900: train perplexity: 6.64 | val perplexity: 7.47\n",
      "Iteration 1910: train perplexity: 6.63 | val perplexity: 7.32\n",
      "Iteration 1920: train perplexity: 6.63 | val perplexity: 7.26\n",
      "Iteration 1930: train perplexity: 6.63 | val perplexity: 7.26\n",
      "Iteration 1940: train perplexity: 6.64 | val perplexity: 7.34\n",
      "Iteration 1950: train perplexity: 6.58 | val perplexity: 7.18\n",
      "Iteration 1960: train perplexity: 6.70 | val perplexity: 7.31\n",
      "Iteration 1970: train perplexity: 6.57 | val perplexity: 7.24\n",
      "Iteration 1980: train perplexity: 6.59 | val perplexity: 7.29\n",
      "Iteration 1990: train perplexity: 6.53 | val perplexity: 7.22\n",
      "Iteration 2000: train perplexity: 6.53 | val perplexity: 7.20\n",
      "Iteration 2010: train perplexity: 6.57 | val perplexity: 7.15\n",
      "Iteration 2020: train perplexity: 6.51 | val perplexity: 7.24\n",
      "Iteration 2030: train perplexity: 6.52 | val perplexity: 7.15\n",
      "Iteration 2040: train perplexity: 6.40 | val perplexity: 7.18\n",
      "Iteration 2050: train perplexity: 6.55 | val perplexity: 7.16\n",
      "Iteration 2060: train perplexity: 6.45 | val perplexity: 7.28\n",
      "Iteration 2070: train perplexity: 6.45 | val perplexity: 7.25\n",
      "Iteration 2080: train perplexity: 6.45 | val perplexity: 7.24\n",
      "Iteration 2090: train perplexity: 6.40 | val perplexity: 7.19\n",
      "Iteration 2100: train perplexity: 6.48 | val perplexity: 7.22\n",
      "Iteration 2110: train perplexity: 6.40 | val perplexity: 7.16\n",
      "Iteration 2120: train perplexity: 6.39 | val perplexity: 7.08\n",
      "Iteration 2130: train perplexity: 6.43 | val perplexity: 7.16\n",
      "Iteration 2140: train perplexity: 6.38 | val perplexity: 7.16\n",
      "Iteration 2150: train perplexity: 6.44 | val perplexity: 7.16\n",
      "Iteration 2160: train perplexity: 6.34 | val perplexity: 7.13\n",
      "Iteration 2170: train perplexity: 6.44 | val perplexity: 7.26\n",
      "Iteration 2180: train perplexity: 6.43 | val perplexity: 7.17\n",
      "Iteration 2190: train perplexity: 6.36 | val perplexity: 7.08\n",
      "Iteration 2200: train perplexity: 6.40 | val perplexity: 7.07\n",
      "Iteration 2210: train perplexity: 6.41 | val perplexity: 7.22\n",
      "Iteration 2220: train perplexity: 6.36 | val perplexity: 7.21\n",
      "Iteration 2230: train perplexity: 6.27 | val perplexity: 7.10\n",
      "Iteration 2240: train perplexity: 6.27 | val perplexity: 7.09\n",
      "Iteration 2250: train perplexity: 6.30 | val perplexity: 7.03\n",
      "Iteration 2260: train perplexity: 6.37 | val perplexity: 7.02\n",
      "Iteration 2270: train perplexity: 6.27 | val perplexity: 6.96\n",
      "Iteration 2280: train perplexity: 6.22 | val perplexity: 7.07\n",
      "Iteration 2290: train perplexity: 6.30 | val perplexity: 6.99\n",
      "Iteration 2300: train perplexity: 6.21 | val perplexity: 6.94\n",
      "Iteration 2310: train perplexity: 6.21 | val perplexity: 6.90\n",
      "Iteration 2320: train perplexity: 6.24 | val perplexity: 6.95\n",
      "Iteration 2330: train perplexity: 6.25 | val perplexity: 7.03\n",
      "Iteration 2340: train perplexity: 6.22 | val perplexity: 6.94\n",
      "Iteration 2350: train perplexity: 6.20 | val perplexity: 6.92\n",
      "Iteration 2360: train perplexity: 6.21 | val perplexity: 7.05\n",
      "Iteration 2370: train perplexity: 6.26 | val perplexity: 7.10\n",
      "Iteration 2380: train perplexity: 6.21 | val perplexity: 7.01\n",
      "Iteration 2390: train perplexity: 6.15 | val perplexity: 6.97\n",
      "Iteration 2400: train perplexity: 6.17 | val perplexity: 7.00\n",
      "Iteration 2410: train perplexity: 6.16 | val perplexity: 6.94\n",
      "Iteration 2420: train perplexity: 6.13 | val perplexity: 6.94\n",
      "Iteration 2430: train perplexity: 6.18 | val perplexity: 6.96\n",
      "Iteration 2440: train perplexity: 6.15 | val perplexity: 7.04\n",
      "Iteration 2450: train perplexity: 6.17 | val perplexity: 7.03\n",
      "Iteration 2460: train perplexity: 6.15 | val perplexity: 6.95\n",
      "Iteration 2470: train perplexity: 6.15 | val perplexity: 7.03\n",
      "Iteration 2480: train perplexity: 6.16 | val perplexity: 7.05\n",
      "Iteration 2490: train perplexity: 6.15 | val perplexity: 6.92\n",
      "Iteration 2500: train perplexity: 6.16 | val perplexity: 6.91\n",
      "Iteration 2510: train perplexity: 6.06 | val perplexity: 6.88\n",
      "Iteration 2520: train perplexity: 6.13 | val perplexity: 6.95\n",
      "Iteration 2530: train perplexity: 6.06 | val perplexity: 6.82\n",
      "Iteration 2540: train perplexity: 6.07 | val perplexity: 6.84\n",
      "Iteration 2550: train perplexity: 6.03 | val perplexity: 6.82\n",
      "Iteration 2560: train perplexity: 6.01 | val perplexity: 6.85\n",
      "Iteration 2570: train perplexity: 6.11 | val perplexity: 6.94\n",
      "Iteration 2580: train perplexity: 6.04 | val perplexity: 6.79\n",
      "Iteration 2590: train perplexity: 6.13 | val perplexity: 6.95\n",
      "Iteration 2600: train perplexity: 6.04 | val perplexity: 6.92\n",
      "Iteration 2610: train perplexity: 6.02 | val perplexity: 6.94\n",
      "Iteration 2620: train perplexity: 6.07 | val perplexity: 6.81\n",
      "Iteration 2630: train perplexity: 6.01 | val perplexity: 6.75\n",
      "Iteration 2640: train perplexity: 6.08 | val perplexity: 6.93\n",
      "Iteration 2650: train perplexity: 6.07 | val perplexity: 6.93\n",
      "Iteration 2660: train perplexity: 6.05 | val perplexity: 6.89\n",
      "Iteration 2670: train perplexity: 6.00 | val perplexity: 6.84\n",
      "Iteration 2680: train perplexity: 6.04 | val perplexity: 6.86\n",
      "Iteration 2690: train perplexity: 6.09 | val perplexity: 6.91\n",
      "Iteration 2700: train perplexity: 5.93 | val perplexity: 6.95\n",
      "Iteration 2710: train perplexity: 6.02 | val perplexity: 6.83\n",
      "Iteration 2720: train perplexity: 5.98 | val perplexity: 6.82\n",
      "Iteration 2730: train perplexity: 5.97 | val perplexity: 6.79\n",
      "Iteration 2740: train perplexity: 5.99 | val perplexity: 6.97\n",
      "Iteration 2750: train perplexity: 6.01 | val perplexity: 6.87\n",
      "Iteration 2760: train perplexity: 5.94 | val perplexity: 6.82\n",
      "Iteration 2770: train perplexity: 5.95 | val perplexity: 6.86\n",
      "Iteration 2780: train perplexity: 5.94 | val perplexity: 6.79\n",
      "Iteration 2790: train perplexity: 6.02 | val perplexity: 6.84\n",
      "Iteration 2800: train perplexity: 5.93 | val perplexity: 6.77\n",
      "Iteration 2810: train perplexity: 5.91 | val perplexity: 6.83\n",
      "Iteration 2820: train perplexity: 5.86 | val perplexity: 6.76\n",
      "Iteration 2830: train perplexity: 5.92 | val perplexity: 6.81\n",
      "Iteration 2840: train perplexity: 5.89 | val perplexity: 6.79\n",
      "Iteration 2850: train perplexity: 5.91 | val perplexity: 6.70\n",
      "Iteration 2860: train perplexity: 5.93 | val perplexity: 6.77\n",
      "Iteration 2870: train perplexity: 5.86 | val perplexity: 6.74\n",
      "Iteration 2880: train perplexity: 5.86 | val perplexity: 6.70\n",
      "Iteration 2890: train perplexity: 5.93 | val perplexity: 6.72\n",
      "Iteration 2900: train perplexity: 5.85 | val perplexity: 6.70\n",
      "Iteration 2910: train perplexity: 5.91 | val perplexity: 6.79\n",
      "Iteration 2920: train perplexity: 5.92 | val perplexity: 6.72\n",
      "Iteration 2930: train perplexity: 5.90 | val perplexity: 6.76\n",
      "Iteration 2940: train perplexity: 5.87 | val perplexity: 6.73\n",
      "Iteration 2950: train perplexity: 5.87 | val perplexity: 6.69\n",
      "Iteration 2960: train perplexity: 5.87 | val perplexity: 6.68\n",
      "Iteration 2970: train perplexity: 5.81 | val perplexity: 6.67\n",
      "Iteration 2980: train perplexity: 5.84 | val perplexity: 6.76\n",
      "Iteration 2990: train perplexity: 5.88 | val perplexity: 6.75\n",
      "Iteration 3000: train perplexity: 5.85 | val perplexity: 6.68\n",
      "Iteration 3010: train perplexity: 5.84 | val perplexity: 6.66\n",
      "Iteration 3020: train perplexity: 5.86 | val perplexity: 6.71\n",
      "Iteration 3030: train perplexity: 5.88 | val perplexity: 6.70\n",
      "Iteration 3040: train perplexity: 5.84 | val perplexity: 6.69\n",
      "Iteration 3050: train perplexity: 5.79 | val perplexity: 6.77\n",
      "Iteration 3060: train perplexity: 5.81 | val perplexity: 6.77\n",
      "Iteration 3070: train perplexity: 5.83 | val perplexity: 6.66\n",
      "Iteration 3080: train perplexity: 5.82 | val perplexity: 6.72\n",
      "Iteration 3090: train perplexity: 5.82 | val perplexity: 6.67\n",
      "Iteration 3100: train perplexity: 5.81 | val perplexity: 6.60\n",
      "Iteration 3110: train perplexity: 5.78 | val perplexity: 6.65\n",
      "Iteration 3120: train perplexity: 5.80 | val perplexity: 6.61\n",
      "Iteration 3130: train perplexity: 5.86 | val perplexity: 6.66\n",
      "Iteration 3140: train perplexity: 5.73 | val perplexity: 6.63\n",
      "Iteration 3150: train perplexity: 5.80 | val perplexity: 6.62\n",
      "Iteration 3160: train perplexity: 5.74 | val perplexity: 6.66\n",
      "Iteration 3170: train perplexity: 5.80 | val perplexity: 6.65\n",
      "Iteration 3180: train perplexity: 5.71 | val perplexity: 6.70\n",
      "Iteration 3190: train perplexity: 5.66 | val perplexity: 6.63\n",
      "Iteration 3200: train perplexity: 5.73 | val perplexity: 6.60\n",
      "Iteration 3210: train perplexity: 5.77 | val perplexity: 6.68\n",
      "Iteration 3220: train perplexity: 5.67 | val perplexity: 6.59\n",
      "Iteration 3230: train perplexity: 5.67 | val perplexity: 6.58\n",
      "Iteration 3240: train perplexity: 5.70 | val perplexity: 6.56\n",
      "Iteration 3250: train perplexity: 5.77 | val perplexity: 6.70\n",
      "Iteration 3260: train perplexity: 5.68 | val perplexity: 6.62\n",
      "Iteration 3270: train perplexity: 5.70 | val perplexity: 6.60\n",
      "Iteration 3280: train perplexity: 5.69 | val perplexity: 6.48\n",
      "Iteration 3290: train perplexity: 5.70 | val perplexity: 6.46\n",
      "Iteration 3300: train perplexity: 5.73 | val perplexity: 6.56\n",
      "Iteration 3310: train perplexity: 5.69 | val perplexity: 6.64\n",
      "Iteration 3320: train perplexity: 5.72 | val perplexity: 6.48\n",
      "Iteration 3330: train perplexity: 5.68 | val perplexity: 6.57\n",
      "Iteration 3340: train perplexity: 5.70 | val perplexity: 6.64\n",
      "Iteration 3350: train perplexity: 5.71 | val perplexity: 6.60\n",
      "Iteration 3360: train perplexity: 5.65 | val perplexity: 6.58\n",
      "Iteration 3370: train perplexity: 5.71 | val perplexity: 6.65\n",
      "Iteration 3380: train perplexity: 5.66 | val perplexity: 6.58\n",
      "Iteration 3390: train perplexity: 5.63 | val perplexity: 6.57\n",
      "Iteration 3400: train perplexity: 5.64 | val perplexity: 6.49\n",
      "Iteration 3410: train perplexity: 5.68 | val perplexity: 6.51\n",
      "Iteration 3420: train perplexity: 5.67 | val perplexity: 6.57\n",
      "Iteration 3430: train perplexity: 5.58 | val perplexity: 6.44\n",
      "Iteration 3440: train perplexity: 5.63 | val perplexity: 6.48\n",
      "Iteration 3450: train perplexity: 5.68 | val perplexity: 6.56\n",
      "Iteration 3460: train perplexity: 5.65 | val perplexity: 6.48\n",
      "Iteration 3470: train perplexity: 5.67 | val perplexity: 6.53\n",
      "Iteration 3480: train perplexity: 5.65 | val perplexity: 6.53\n",
      "Iteration 3490: train perplexity: 5.61 | val perplexity: 6.60\n",
      "Iteration 3500: train perplexity: 5.63 | val perplexity: 6.56\n",
      "Iteration 3510: train perplexity: 5.64 | val perplexity: 6.55\n",
      "Iteration 3520: train perplexity: 5.59 | val perplexity: 6.45\n",
      "Iteration 3530: train perplexity: 5.56 | val perplexity: 6.48\n",
      "Iteration 3540: train perplexity: 5.59 | val perplexity: 6.43\n",
      "Iteration 3550: train perplexity: 5.61 | val perplexity: 6.53\n",
      "Iteration 3560: train perplexity: 5.62 | val perplexity: 6.52\n",
      "Iteration 3570: train perplexity: 5.65 | val perplexity: 6.45\n",
      "Iteration 3580: train perplexity: 5.62 | val perplexity: 6.52\n",
      "Iteration 3590: train perplexity: 5.62 | val perplexity: 6.47\n",
      "Iteration 3600: train perplexity: 5.61 | val perplexity: 6.53\n",
      "Iteration 3610: train perplexity: 5.55 | val perplexity: 6.50\n",
      "Iteration 3620: train perplexity: 5.63 | val perplexity: 6.53\n",
      "Iteration 3630: train perplexity: 5.57 | val perplexity: 6.51\n",
      "Iteration 3640: train perplexity: 5.64 | val perplexity: 6.42\n",
      "Iteration 3650: train perplexity: 5.55 | val perplexity: 6.42\n",
      "Iteration 3660: train perplexity: 5.56 | val perplexity: 6.49\n",
      "Iteration 3670: train perplexity: 5.54 | val perplexity: 6.54\n",
      "Iteration 3680: train perplexity: 5.52 | val perplexity: 6.49\n",
      "Iteration 3690: train perplexity: 5.54 | val perplexity: 6.45\n",
      "Iteration 3700: train perplexity: 5.60 | val perplexity: 6.58\n",
      "Iteration 3710: train perplexity: 5.53 | val perplexity: 6.41\n",
      "Iteration 3720: train perplexity: 5.54 | val perplexity: 6.46\n",
      "Iteration 3730: train perplexity: 5.59 | val perplexity: 6.45\n",
      "Iteration 3740: train perplexity: 5.53 | val perplexity: 6.45\n",
      "Iteration 3750: train perplexity: 5.56 | val perplexity: 6.55\n",
      "Iteration 3760: train perplexity: 5.58 | val perplexity: 6.44\n",
      "Iteration 3770: train perplexity: 5.52 | val perplexity: 6.44\n",
      "Iteration 3780: train perplexity: 5.51 | val perplexity: 6.43\n",
      "Iteration 3790: train perplexity: 5.58 | val perplexity: 6.47\n",
      "Iteration 3800: train perplexity: 5.51 | val perplexity: 6.39\n",
      "Iteration 3810: train perplexity: 5.53 | val perplexity: 6.43\n",
      "Iteration 3820: train perplexity: 5.51 | val perplexity: 6.41\n",
      "Iteration 3830: train perplexity: 5.52 | val perplexity: 6.40\n",
      "Iteration 3840: train perplexity: 5.55 | val perplexity: 6.36\n",
      "Iteration 3850: train perplexity: 5.48 | val perplexity: 6.43\n",
      "Iteration 3860: train perplexity: 5.53 | val perplexity: 6.39\n",
      "Iteration 3870: train perplexity: 5.53 | val perplexity: 6.41\n",
      "Iteration 3880: train perplexity: 5.49 | val perplexity: 6.45\n",
      "Iteration 3890: train perplexity: 5.51 | val perplexity: 6.42\n",
      "Iteration 3900: train perplexity: 5.48 | val perplexity: 6.48\n",
      "Iteration 3910: train perplexity: 5.54 | val perplexity: 6.38\n",
      "Iteration 3920: train perplexity: 5.53 | val perplexity: 6.45\n",
      "Iteration 3930: train perplexity: 5.55 | val perplexity: 6.38\n",
      "Iteration 3940: train perplexity: 5.51 | val perplexity: 6.35\n",
      "Iteration 3950: train perplexity: 5.53 | val perplexity: 6.34\n",
      "Iteration 3960: train perplexity: 5.49 | val perplexity: 6.33\n",
      "Iteration 3970: train perplexity: 5.46 | val perplexity: 6.36\n",
      "Iteration 3980: train perplexity: 5.41 | val perplexity: 6.35\n",
      "Iteration 3990: train perplexity: 5.49 | val perplexity: 6.35\n",
      "Iteration 4000: train perplexity: 5.46 | val perplexity: 6.43\n",
      "Iteration 4010: train perplexity: 5.43 | val perplexity: 6.39\n",
      "Iteration 4020: train perplexity: 5.43 | val perplexity: 6.36\n",
      "Iteration 4030: train perplexity: 5.45 | val perplexity: 6.38\n",
      "Iteration 4040: train perplexity: 5.45 | val perplexity: 6.43\n",
      "Iteration 4050: train perplexity: 5.44 | val perplexity: 6.38\n",
      "Iteration 4060: train perplexity: 5.43 | val perplexity: 6.31\n",
      "Iteration 4070: train perplexity: 5.43 | val perplexity: 6.39\n",
      "Iteration 4080: train perplexity: 5.46 | val perplexity: 6.38\n",
      "Iteration 4090: train perplexity: 5.45 | val perplexity: 6.42\n",
      "Iteration 4100: train perplexity: 5.37 | val perplexity: 6.31\n",
      "Iteration 4110: train perplexity: 5.41 | val perplexity: 6.31\n",
      "Iteration 4120: train perplexity: 5.43 | val perplexity: 6.35\n",
      "Iteration 4130: train perplexity: 5.42 | val perplexity: 6.30\n",
      "Iteration 4140: train perplexity: 5.36 | val perplexity: 6.34\n",
      "Iteration 4150: train perplexity: 5.42 | val perplexity: 6.43\n",
      "Iteration 4160: train perplexity: 5.39 | val perplexity: 6.30\n",
      "Iteration 4170: train perplexity: 5.40 | val perplexity: 6.31\n",
      "Iteration 4180: train perplexity: 5.42 | val perplexity: 6.28\n",
      "Iteration 4190: train perplexity: 5.39 | val perplexity: 6.33\n",
      "Iteration 4200: train perplexity: 5.43 | val perplexity: 6.41\n",
      "Iteration 4210: train perplexity: 5.48 | val perplexity: 6.26\n",
      "Iteration 4220: train perplexity: 5.35 | val perplexity: 6.29\n",
      "Iteration 4230: train perplexity: 5.38 | val perplexity: 6.33\n",
      "Iteration 4240: train perplexity: 5.34 | val perplexity: 6.27\n",
      "Iteration 4250: train perplexity: 5.37 | val perplexity: 6.23\n",
      "Iteration 4260: train perplexity: 5.41 | val perplexity: 6.25\n",
      "Iteration 4270: train perplexity: 5.38 | val perplexity: 6.34\n",
      "Iteration 4280: train perplexity: 5.41 | val perplexity: 6.33\n",
      "Iteration 4290: train perplexity: 5.38 | val perplexity: 6.25\n",
      "Iteration 4300: train perplexity: 5.40 | val perplexity: 6.27\n",
      "Iteration 4310: train perplexity: 5.35 | val perplexity: 6.27\n",
      "Iteration 4320: train perplexity: 5.41 | val perplexity: 6.30\n",
      "Iteration 4330: train perplexity: 5.36 | val perplexity: 6.24\n",
      "Iteration 4340: train perplexity: 5.39 | val perplexity: 6.25\n",
      "Iteration 4350: train perplexity: 5.35 | val perplexity: 6.29\n",
      "Iteration 4360: train perplexity: 5.43 | val perplexity: 6.33\n",
      "Iteration 4370: train perplexity: 5.35 | val perplexity: 6.24\n",
      "Iteration 4380: train perplexity: 5.37 | val perplexity: 6.22\n",
      "Iteration 4390: train perplexity: 5.36 | val perplexity: 6.36\n",
      "Iteration 4400: train perplexity: 5.36 | val perplexity: 6.36\n",
      "Iteration 4410: train perplexity: 5.38 | val perplexity: 6.39\n",
      "Iteration 4420: train perplexity: 5.39 | val perplexity: 6.29\n",
      "Iteration 4430: train perplexity: 5.28 | val perplexity: 6.32\n",
      "Iteration 4440: train perplexity: 5.29 | val perplexity: 6.26\n",
      "Iteration 4450: train perplexity: 5.36 | val perplexity: 6.28\n",
      "Iteration 4460: train perplexity: 5.31 | val perplexity: 6.27\n",
      "Iteration 4470: train perplexity: 5.35 | val perplexity: 6.25\n",
      "Iteration 4480: train perplexity: 5.33 | val perplexity: 6.27\n",
      "Iteration 4490: train perplexity: 5.30 | val perplexity: 6.24\n",
      "Iteration 4500: train perplexity: 5.29 | val perplexity: 6.22\n",
      "Iteration 4510: train perplexity: 5.35 | val perplexity: 6.22\n",
      "Iteration 4520: train perplexity: 5.34 | val perplexity: 6.37\n",
      "Iteration 4530: train perplexity: 5.34 | val perplexity: 6.26\n",
      "Iteration 4540: train perplexity: 5.33 | val perplexity: 6.26\n",
      "Iteration 4550: train perplexity: 5.35 | val perplexity: 6.32\n",
      "Iteration 4560: train perplexity: 5.35 | val perplexity: 6.19\n",
      "Iteration 4570: train perplexity: 5.34 | val perplexity: 6.32\n",
      "Iteration 4580: train perplexity: 5.33 | val perplexity: 6.27\n",
      "Iteration 4590: train perplexity: 5.32 | val perplexity: 6.31\n",
      "Iteration 4600: train perplexity: 5.32 | val perplexity: 6.27\n",
      "Iteration 4610: train perplexity: 5.28 | val perplexity: 6.25\n",
      "Iteration 4620: train perplexity: 5.30 | val perplexity: 6.18\n",
      "Iteration 4630: train perplexity: 5.31 | val perplexity: 6.14\n",
      "Iteration 4640: train perplexity: 5.35 | val perplexity: 6.14\n",
      "Iteration 4650: train perplexity: 5.26 | val perplexity: 6.16\n",
      "Iteration 4660: train perplexity: 5.29 | val perplexity: 6.23\n",
      "Iteration 4670: train perplexity: 5.31 | val perplexity: 6.24\n",
      "Iteration 4680: train perplexity: 5.27 | val perplexity: 6.24\n",
      "Iteration 4690: train perplexity: 5.30 | val perplexity: 6.20\n",
      "Iteration 4700: train perplexity: 5.26 | val perplexity: 6.28\n",
      "Iteration 4710: train perplexity: 5.31 | val perplexity: 6.22\n",
      "Iteration 4720: train perplexity: 5.27 | val perplexity: 6.15\n",
      "Iteration 4730: train perplexity: 5.26 | val perplexity: 6.22\n",
      "Iteration 4740: train perplexity: 5.27 | val perplexity: 6.20\n",
      "Iteration 4750: train perplexity: 5.22 | val perplexity: 6.24\n",
      "Iteration 4760: train perplexity: 5.27 | val perplexity: 6.18\n",
      "Iteration 4770: train perplexity: 5.28 | val perplexity: 6.16\n",
      "Iteration 4780: train perplexity: 5.23 | val perplexity: 6.34\n",
      "Iteration 4790: train perplexity: 5.28 | val perplexity: 6.20\n",
      "Iteration 4800: train perplexity: 5.28 | val perplexity: 6.22\n",
      "Iteration 4810: train perplexity: 5.26 | val perplexity: 6.22\n",
      "Iteration 4820: train perplexity: 5.27 | val perplexity: 6.21\n",
      "Iteration 4830: train perplexity: 5.29 | val perplexity: 6.23\n",
      "Iteration 4840: train perplexity: 5.24 | val perplexity: 6.22\n",
      "Iteration 4850: train perplexity: 5.31 | val perplexity: 6.18\n",
      "Iteration 4860: train perplexity: 5.20 | val perplexity: 6.16\n",
      "Iteration 4870: train perplexity: 5.22 | val perplexity: 6.18\n",
      "Iteration 4880: train perplexity: 5.26 | val perplexity: 6.19\n",
      "Iteration 4890: train perplexity: 5.24 | val perplexity: 6.24\n",
      "Iteration 4900: train perplexity: 5.30 | val perplexity: 6.33\n",
      "Iteration 4910: train perplexity: 5.26 | val perplexity: 6.30\n",
      "Iteration 4920: train perplexity: 5.23 | val perplexity: 6.20\n",
      "Iteration 4930: train perplexity: 5.27 | val perplexity: 6.22\n",
      "Iteration 4940: train perplexity: 5.23 | val perplexity: 6.24\n",
      "Iteration 4950: train perplexity: 5.22 | val perplexity: 6.14\n",
      "Iteration 4960: train perplexity: 5.28 | val perplexity: 6.26\n",
      "Iteration 4970: train perplexity: 5.28 | val perplexity: 6.14\n",
      "Iteration 4980: train perplexity: 5.23 | val perplexity: 6.12\n",
      "Iteration 4990: train perplexity: 5.20 | val perplexity: 6.19\n"
     ]
    }
   ],
   "source": [
    "model = MyLanguageModel(64, 4, 4)\n",
    "model.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "results[(64, 4, 4)] = {}\n",
    "\n",
    "for i in range(max_iters):\n",
    "    \n",
    "    x, y = get_batch('train')\n",
    "    \n",
    "    logits, loss = model(x, y)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if i % eval_interval == 0:\n",
    "        out = estimate_loss()\n",
    "        print(f\"Iteration {i}: train perplexity: {torch.exp(out['train']):.2f} | val perplexity: {torch.exp(out['val']):.2f}\")\n",
    "        results[(64, 4, 4)][i] = out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "hps = (64, 4, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"./{hps[0]}_{hps[1]}_{hps[2]}.pkl\", \"wb\") as f:\n",
    "    pickle.dump(results[hps], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 700x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "with open(f\"./{hps[0]}_{hps[1]}_{hps[2]}.pkl\", \"rb\") as f:\n",
    "    results[hps] = pickle.load(f)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(7, 5))\n",
    "\n",
    "iters = list(results[hps].keys())\n",
    "train_perplexity = np.exp(np.array([val['train'].item() for val in results[hps].values()]))\n",
    "val_perplexity = np.exp(np.array([val['val'].item() for val in results[hps].values()]))\n",
    "\n",
    "ax.plot(iters, train_perplexity, 'k-', label='train')\n",
    "ax.plot(iters, val_perplexity, 'r-', label='val')\n",
    "ax.set(xlabel='steps', ylabel='perplexity', yscale='log')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Different hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = [\n",
    "    (64, 4, 2),\n",
    "    (64, 4, 6),\n",
    "    (64, 2, 4),\n",
    "    (64, 8, 4),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have to restart the kernel to finish all training, so the training logs are not complete. But all results are available in the pickled files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameter: (64, 4, 6)\n",
      "Iteration 0: train perplexity: 56.70 | val perplexity: 57.15\n",
      "Iteration 100: train perplexity: 13.74 | val perplexity: 13.77\n",
      "Iteration 200: train perplexity: 12.06 | val perplexity: 12.02\n",
      "Iteration 300: train perplexity: 10.94 | val perplexity: 10.91\n",
      "Iteration 400: train perplexity: 10.21 | val perplexity: 10.38\n",
      "Iteration 500: train perplexity: 9.85 | val perplexity: 9.94\n",
      "Iteration 600: train perplexity: 9.35 | val perplexity: 9.42\n",
      "Iteration 700: train perplexity: 8.83 | val perplexity: 9.06\n",
      "Iteration 800: train perplexity: 8.67 | val perplexity: 8.87\n",
      "Iteration 900: train perplexity: 8.06 | val perplexity: 8.32\n",
      "Iteration 1000: train perplexity: 7.78 | val perplexity: 8.18\n",
      "Iteration 1100: train perplexity: 7.64 | val perplexity: 8.04\n",
      "Iteration 1200: train perplexity: 7.41 | val perplexity: 7.90\n",
      "Iteration 1300: train perplexity: 7.28 | val perplexity: 7.81\n",
      "Iteration 1400: train perplexity: 7.10 | val perplexity: 7.66\n",
      "Iteration 1500: train perplexity: 6.98 | val perplexity: 7.63\n",
      "Iteration 1600: train perplexity: 6.76 | val perplexity: 7.50\n",
      "Iteration 1700: train perplexity: 6.59 | val perplexity: 7.41\n",
      "Iteration 1800: train perplexity: 6.54 | val perplexity: 7.29\n",
      "Iteration 1900: train perplexity: 6.49 | val perplexity: 7.07\n",
      "Iteration 2000: train perplexity: 6.34 | val perplexity: 7.15\n",
      "Iteration 2100: train perplexity: 6.25 | val perplexity: 7.01\n",
      "Iteration 2200: train perplexity: 6.12 | val perplexity: 6.91\n",
      "Iteration 2300: train perplexity: 6.10 | val perplexity: 6.89\n",
      "Iteration 2400: train perplexity: 6.00 | val perplexity: 6.87\n",
      "Iteration 2500: train perplexity: 5.94 | val perplexity: 6.76\n",
      "Iteration 2600: train perplexity: 5.84 | val perplexity: 6.75\n",
      "Iteration 2700: train perplexity: 5.82 | val perplexity: 6.76\n",
      "Iteration 2800: train perplexity: 5.77 | val perplexity: 6.66\n",
      "Iteration 2900: train perplexity: 5.70 | val perplexity: 6.58\n",
      "Iteration 3000: train perplexity: 5.66 | val perplexity: 6.51\n",
      "Iteration 3100: train perplexity: 5.70 | val perplexity: 6.40\n",
      "Iteration 3200: train perplexity: 5.59 | val perplexity: 6.40\n",
      "Iteration 3300: train perplexity: 5.54 | val perplexity: 6.43\n",
      "Iteration 3400: train perplexity: 5.52 | val perplexity: 6.31\n",
      "Iteration 3500: train perplexity: 5.47 | val perplexity: 6.37\n",
      "Iteration 3600: train perplexity: 5.36 | val perplexity: 6.23\n",
      "Iteration 3700: train perplexity: 5.35 | val perplexity: 6.30\n",
      "Iteration 3800: train perplexity: 5.29 | val perplexity: 6.26\n",
      "Iteration 3900: train perplexity: 5.26 | val perplexity: 6.24\n",
      "Iteration 4000: train perplexity: 5.35 | val perplexity: 6.23\n",
      "Iteration 4100: train perplexity: 5.31 | val perplexity: 6.22\n",
      "Iteration 4200: train perplexity: 5.20 | val perplexity: 6.08\n",
      "Iteration 4300: train perplexity: 5.21 | val perplexity: 6.12\n",
      "Iteration 4400: train perplexity: 5.23 | val perplexity: 6.13\n",
      "Iteration 4500: train perplexity: 5.18 | val perplexity: 6.07\n",
      "Iteration 4600: train perplexity: 5.14 | val perplexity: 6.03\n",
      "Iteration 4700: train perplexity: 5.13 | val perplexity: 6.08\n",
      "Iteration 4800: train perplexity: 5.08 | val perplexity: 5.93\n",
      "Iteration 4900: train perplexity: 5.09 | val perplexity: 5.99\n",
      "Hyperparameter: (64, 2, 4)\n",
      "Iteration 0: train perplexity: 59.17 | val perplexity: 59.35\n",
      "Iteration 100: train perplexity: 14.36 | val perplexity: 14.46\n",
      "Iteration 200: train perplexity: 12.28 | val perplexity: 12.33\n",
      "Iteration 300: train perplexity: 11.05 | val perplexity: 11.18\n",
      "Iteration 400: train perplexity: 10.17 | val perplexity: 10.30\n",
      "Iteration 500: train perplexity: 9.67 | val perplexity: 9.82\n",
      "Iteration 600: train perplexity: 9.18 | val perplexity: 9.41\n",
      "Iteration 700: train perplexity: 8.73 | val perplexity: 8.96\n",
      "Iteration 800: train perplexity: 8.37 | val perplexity: 8.70\n",
      "Iteration 900: train perplexity: 8.06 | val perplexity: 8.37\n",
      "Iteration 1000: train perplexity: 7.80 | val perplexity: 8.20\n",
      "Iteration 1100: train perplexity: 7.60 | val perplexity: 8.03\n",
      "Iteration 1200: train perplexity: 7.42 | val perplexity: 7.86\n",
      "Iteration 1300: train perplexity: 7.29 | val perplexity: 7.81\n",
      "Iteration 1400: train perplexity: 7.04 | val perplexity: 7.51\n",
      "Iteration 1500: train perplexity: 6.92 | val perplexity: 7.56\n",
      "Iteration 1600: train perplexity: 6.73 | val perplexity: 7.44\n",
      "Iteration 1700: train perplexity: 6.70 | val perplexity: 7.32\n",
      "Iteration 1800: train perplexity: 6.60 | val perplexity: 7.34\n",
      "Iteration 1900: train perplexity: 6.43 | val perplexity: 7.22\n",
      "Iteration 2000: train perplexity: 6.34 | val perplexity: 7.11\n",
      "Iteration 2100: train perplexity: 6.22 | val perplexity: 7.02\n",
      "Iteration 2200: train perplexity: 6.24 | val perplexity: 6.99\n",
      "Iteration 2300: train perplexity: 6.07 | val perplexity: 6.93\n",
      "Iteration 2400: train perplexity: 6.11 | val perplexity: 7.02\n",
      "Iteration 2500: train perplexity: 6.07 | val perplexity: 6.94\n",
      "Iteration 2600: train perplexity: 5.93 | val perplexity: 6.94\n",
      "Iteration 2700: train perplexity: 5.93 | val perplexity: 6.76\n",
      "Iteration 2800: train perplexity: 5.82 | val perplexity: 6.69\n",
      "Iteration 2900: train perplexity: 5.78 | val perplexity: 6.64\n",
      "Iteration 3000: train perplexity: 5.72 | val perplexity: 6.70\n",
      "Iteration 3100: train perplexity: 5.64 | val perplexity: 6.62\n",
      "Iteration 3200: train perplexity: 5.70 | val perplexity: 6.61\n",
      "Iteration 3300: train perplexity: 5.57 | val perplexity: 6.56\n",
      "Iteration 3400: train perplexity: 5.64 | val perplexity: 6.50\n",
      "Iteration 3500: train perplexity: 5.54 | val perplexity: 6.59\n",
      "Iteration 3600: train perplexity: 5.53 | val perplexity: 6.48\n",
      "Iteration 3700: train perplexity: 5.47 | val perplexity: 6.48\n",
      "Iteration 3800: train perplexity: 5.49 | val perplexity: 6.43\n",
      "Iteration 3900: train perplexity: 5.42 | val perplexity: 6.36\n",
      "Iteration 4000: train perplexity: 5.40 | val perplexity: 6.29\n",
      "Iteration 4100: train perplexity: 5.34 | val perplexity: 6.38\n",
      "Iteration 4200: train perplexity: 5.38 | val perplexity: 6.37\n",
      "Iteration 4300: train perplexity: 5.34 | val perplexity: 6.27\n",
      "Iteration 4400: train perplexity: 5.27 | val perplexity: 6.25\n",
      "Iteration 4500: train perplexity: 5.26 | val perplexity: 6.27\n",
      "Iteration 4600: train perplexity: 5.27 | val perplexity: 6.25\n",
      "Iteration 4700: train perplexity: 5.21 | val perplexity: 6.19\n",
      "Iteration 4800: train perplexity: 5.25 | val perplexity: 6.24\n",
      "Iteration 4900: train perplexity: 5.19 | val perplexity: 6.17\n",
      "Hyperparameter: (64, 8, 4)\n",
      "Iteration 0: train perplexity: 66.94 | val perplexity: 67.16\n",
      "Iteration 100: train perplexity: 14.70 | val perplexity: 14.79\n",
      "Iteration 200: train perplexity: 12.55 | val perplexity: 12.64\n",
      "Iteration 300: train perplexity: 11.62 | val perplexity: 11.59\n",
      "Iteration 400: train perplexity: 10.94 | val perplexity: 10.87\n",
      "Iteration 500: train perplexity: 10.39 | val perplexity: 10.50\n",
      "Iteration 600: train perplexity: 9.92 | val perplexity: 10.03\n",
      "Iteration 700: train perplexity: 9.65 | val perplexity: 9.81\n",
      "Iteration 800: train perplexity: 9.16 | val perplexity: 9.34\n",
      "Iteration 900: train perplexity: 8.86 | val perplexity: 9.03\n",
      "Iteration 1000: train perplexity: 8.66 | val perplexity: 8.84\n",
      "Iteration 1100: train perplexity: 8.35 | val perplexity: 8.64\n",
      "Iteration 1200: train perplexity: 8.01 | val perplexity: 8.43\n",
      "Iteration 1300: train perplexity: 7.87 | val perplexity: 8.32\n",
      "Iteration 1400: train perplexity: 7.59 | val perplexity: 8.11\n",
      "Iteration 1500: train perplexity: 7.47 | val perplexity: 7.95\n",
      "Iteration 1600: train perplexity: 7.26 | val perplexity: 7.91\n",
      "Iteration 1700: train perplexity: 7.24 | val perplexity: 7.90\n",
      "Iteration 1800: train perplexity: 7.05 | val perplexity: 7.56\n",
      "Iteration 1900: train perplexity: 6.84 | val perplexity: 7.56\n",
      "Iteration 2000: train perplexity: 6.77 | val perplexity: 7.59\n",
      "Iteration 2100: train perplexity: 6.73 | val perplexity: 7.44\n",
      "Iteration 2200: train perplexity: 6.59 | val perplexity: 7.33\n",
      "Iteration 2300: train perplexity: 6.50 | val perplexity: 7.31\n",
      "Iteration 2400: train perplexity: 6.40 | val perplexity: 7.29\n",
      "Iteration 2500: train perplexity: 6.32 | val perplexity: 7.24\n",
      "Iteration 2600: train perplexity: 6.25 | val perplexity: 7.17\n",
      "Iteration 2700: train perplexity: 6.16 | val perplexity: 6.90\n",
      "Iteration 2800: train perplexity: 6.12 | val perplexity: 7.06\n",
      "Iteration 2900: train perplexity: 6.09 | val perplexity: 6.90\n",
      "Iteration 3000: train perplexity: 6.01 | val perplexity: 6.90\n",
      "Iteration 3100: train perplexity: 5.92 | val perplexity: 6.90\n",
      "Iteration 3200: train perplexity: 5.95 | val perplexity: 6.86\n",
      "Iteration 3300: train perplexity: 5.90 | val perplexity: 6.72\n",
      "Iteration 3400: train perplexity: 5.73 | val perplexity: 6.72\n",
      "Iteration 3500: train perplexity: 5.71 | val perplexity: 6.69\n",
      "Iteration 3600: train perplexity: 5.72 | val perplexity: 6.87\n",
      "Iteration 3700: train perplexity: 5.69 | val perplexity: 6.63\n",
      "Iteration 3800: train perplexity: 5.64 | val perplexity: 6.62\n",
      "Iteration 3900: train perplexity: 5.58 | val perplexity: 6.62\n",
      "Iteration 4000: train perplexity: 5.57 | val perplexity: 6.54\n",
      "Iteration 4100: train perplexity: 5.52 | val perplexity: 6.45\n",
      "Iteration 4200: train perplexity: 5.49 | val perplexity: 6.45\n",
      "Iteration 4300: train perplexity: 5.45 | val perplexity: 6.48\n",
      "Iteration 4400: train perplexity: 5.44 | val perplexity: 6.38\n",
      "Iteration 4500: train perplexity: 5.38 | val perplexity: 6.41\n",
      "Iteration 4600: train perplexity: 5.42 | val perplexity: 6.33\n",
      "Iteration 4700: train perplexity: 5.33 | val perplexity: 6.25\n",
      "Iteration 4800: train perplexity: 5.31 | val perplexity: 6.32\n",
      "Iteration 4900: train perplexity: 5.31 | val perplexity: 6.23\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "for hps in hyperparameters:\n",
    "    \n",
    "    print(f\"Hyperparameter: {hps}\")\n",
    "    model = MyLanguageModel(*hps)\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    results[hps] = {}\n",
    "\n",
    "    for i in range(max_iters):\n",
    "\n",
    "        x, y = get_batch('train')\n",
    "\n",
    "        logits, loss = model(x, y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % eval_interval == 0:\n",
    "            out = estimate_loss()\n",
    "            results[hps][i] = out\n",
    "            if i % 100 == 0:\n",
    "                print(f\"Iteration {i}: train perplexity: {torch.exp(out['train']):.2f} | val perplexity: {torch.exp(out['val']):.2f}\")\n",
    "            \n",
    "    with open(f\"./{hps[0]}_{hps[1]}_{hps[2]}.pkl\", \"wb\") as f:\n",
    "        pickle.dump(results[hps], f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 700x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 700x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "results = {}\n",
    "\n",
    "for hps in [\n",
    "    (64, 4, 2),\n",
    "    (64, 4, 4),\n",
    "    (64, 4, 6),\n",
    "    (64, 2, 4),\n",
    "    (64, 8, 4),\n",
    "]:\n",
    "    with open(f\"./{hps[0]}_{hps[1]}_{hps[2]}.pkl\", \"rb\") as f:\n",
    "        results[hps] = pickle.load(f)\n",
    "\n",
    "        \n",
    "fig, ax = plt.subplots(1, 1, figsize=(7,5))\n",
    "        \n",
    "for hps in [\n",
    "    (64, 4, 2),\n",
    "    (64, 4, 4),\n",
    "    (64, 4, 6),\n",
    "]:\n",
    "    iters = list(results[hps].keys())\n",
    "    perplexity = np.exp(np.array([val['train'].item() for val in results[hps].values()]))\n",
    "    ax.plot(iters, perplexity, label=str(hps[2]))\n",
    "    ax.set(xlabel='steps', ylabel='perplexity', yscale='log', title='Perplexity vs Number of layers')\n",
    "\n",
    "plt.legend(title='n_layer')\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(7,5))\n",
    "        \n",
    "for hps in [\n",
    "    (64, 2, 4),\n",
    "    (64, 4, 4),\n",
    "    (64, 8, 4),\n",
    "]:\n",
    "    iters = list(results[hps].keys())\n",
    "    perplexity = np.exp(np.array([val['train'].item() for val in results[hps].values()]))\n",
    "    ax.plot(iters, perplexity, label=str(hps[1]))\n",
    "    ax.set(xlabel='steps', ylabel='perplexity', yscale='log', title='Perplexity vs Number of heads')\n",
    "\n",
    "plt.legend(title='n_head')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In summary, for this example\n",
    "- a larger number of layers results in lower perplexity\n",
    "- a larger number of heads results in higher perplexity, but the perplexity values seem to converge with more training steps despite the number of heads used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nDXLTusqxXHf"
   },
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some recommendations for further reading and additional code for review.\n",
    "\n",
    "* \"The Illustrated Transformer\" by Jay Alammar\n",
    "* \"Visualizing A Neural Machine Translation Model (Mechanics of Seq2seq Models With Attention)\"\n",
    "* \"The Illustrated GPT-2 (Visualizing Transformer Language Models)\"\n",
    "* \"A gentle introduction to positional encoding\"\n",
    "* \"LLM Tutorial Workshop (Argonne National Laboratory)\"\n",
    "* \"LLM Tutorial Workshop Part 2 (Argonne National Laboratory)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "datascience/conda-2023-01-10",
   "language": "python",
   "name": "conda-2023-01-10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
